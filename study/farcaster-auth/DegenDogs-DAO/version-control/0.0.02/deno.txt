URL: https://docs.deno.com/deploy/
TITLE: About Deno Deploy

About Deno Deploy
Go to the Deno Deploy dashboard
Deno Deploy comes with an easy to use dashboard at console.deno.com . In this dashboard, you can create new Deno Deploy organizations that contain Deno Deploy apps.
Within a single organization, you cannot mix Deno Deploy apps with Deploy Classic projects. You can switch between different organizations using the organization picker in the top left of the dashboard.
What is Deno Deploy? Jump to heading #
Deno Deploy is a serverless platform for running JavaScript and TypeScript applications in the cloud (or self-hosted on your own infrastructure). It provides a management plane for deploying and running applications with the built-in CI or through integrations such as GitHub actions.
Comparison to Deploy Classic Jump to heading #
Deno Deploy is a complete rework of Deploy Classic. It has a new dashboard, and a new execution environment that uses Deno 2.0 and is much more powerful than Deploy Classic. The below table compares the two versions of Deno Deploy.
Feature Deno Deploy Deploy Classic
Web interface console.deno.com dash.deno.com
Dark mode  Supported  Not supported
Builds  Fully integrated  Runs in GitHub Actions, no live streamed logs in the dashboard, caching requires manual setup, changing config requires editing YAML
Can run Deno apps  Full support  Limited (no FFI, subprocesses, write permission)
Can run Node apps  Full support  Limited (no FFI, native addons, subprocesses, write permission, and degraded NPM compatibility)
Can run Next.js/Astro/SvelteKit  First-class support  Framework dependent, requires manual setup
First class static sites  Supported  Not supported
Environment Variables  Different dev/prod env vars  One set of env vars for all deployments
CDN caching  Supported  Not supported
Web Cache API  Supported  Supported
Databases  Supported  Deno KV
Queues  Not supported  Supported
Cron  Not supported  Supported
Deploy from GitHub  Supported  Supported
Deploy from CLI  Supported  Supported
Instant Rollback  Supported  Supported
Logs  Supported  Supported
Tracing  Supported  Not supported
Metrics  Supported  Not supported
OpenTelemetry export  Work in progress  Not supported
Regions 2 6
Self hostable regions  Supported  Not supported
How to access Deno Deploy Jump to heading #
To begin using Deno Deploy:
- Visit console.deno.com to access the new dashboard
- Create a new Deno Deploy organization
- Create your first application within this organization
- Deploy from your GitHub repository or directly from the dashboard
For detailed configuration instructions and framework-specific guides, please refer to our reference documentation.


---

URL: https://docs.deno.com/deploy/changelog/
TITLE: Deno Deploy changelog

Deno Deploy changelog
December 18th, 2025 Jump to heading #
Features Jump to heading #
- Deno Deploy can now detect Deno and NPM workspace / monorepo configurations, allowing you to deploy applications located in subdirectories of a larger repository.
- During app creation, we'll now scan the repository for workspace configurations, and allow you to select which workspace member to deploy.
- During builds, the working directory is set to the workspace member's directory.
- The app directory can be customized after app creation in the app config settings.
- The build logs now have a dedicated "Deploy" section that replaces the previous "Warmup" and "Routing" steps, providing more clarity on what is happening during deployment.
- Inside of the "Deploy" section, you'll find sub-sections for each timeline that the revision is being deployed to, including production, git branches, and preview deployments.
- The "Warmup" sub-section shows logs related to prewarming the application.
- The "Pre-deploy" sub-section shows logs related to running the user-defined pre-deploy command, for example to run migrations.
- The "Database creation" sub-section shows logs related to creating any linked databases for the timeline.
- The top navigation bar has been redesigned to include a breadcrumb dropdown for the current section of the dashboard. This allows you to quickly navigate between, for example, apps and domains.
- You can now skip deploying a specific commit using the GitHub integration by including the string [skip ci] or [skip deploy] in the commit message.
- Revisions that have not received traffic for more than 30 days are now automatically disabled, and deleting after a further 7 days of inactivity.
- Disabled revisions can be re-enabled before deletion from the revisions page.
- The deno deploy CLI and --tunnel flag for deno run and deno task now support using organization tokens for authentication, in addition to user tokens.
- To use an organization token, pass it as usual in the DENO_DEPLOY_TOKEN environment variable.
- We have rolled out several runtime security patches to address recently disclosed vulnerabilities in React and Next.js: CVE-2025-55182 (Remote Code Execution) and CVE-2025-55184/CVE-2025-67779 (Denial of Service).
- And one more thing at the end: we've quietly enabled our new sandboxes infrastructure for all Deno Deploy users to try.
- Sandboxes provide fully isolated Linux microVMs for you to run untrusted code in.
- This is particularly useful for running third-party code, such as plugins, extensions, or user-generated or LLM-generated code, without risking the security of your application.
- We'll announce more details about sandboxes in the new year, so stay tuned!
- Try it out from the "Sandboxes" tab in the organization overview.
- Learn more about sandboxes.
Bug fixes Jump to heading #
- Fixed an issue where some Next.js and Astro builds would fail when installing dependencies with a frozen lockfile.
- Fixed an issue the _acme-challenge CNAME DNS records was displayed without a trailing dot, causing confusion when copying the record value for DNS verification.
November 25th, 2025 Jump to heading #
Features Jump to heading #
- Deno Deploy can now securely expose locally running applications on a public domain using the deno run --tunnel / deno task --tunnel .
- This is particularly useful for testing webhooks from third-party services, or sharing work-in-progress applications with colleagues or clients.
- The tunnel creates a secure connection from your local machine to Deno Deploy's infrastructure, and provisions a temporary public domain that routes traffic to your local application.
- In addition, --tunnel automatically pulls down "Local" environment variables from the Deno Deploy dashboard, making configuration and secrets management much easier.
- Open Telemetry metrics, logs, and traces are also collected from local applications and can be viewed in the Deno Deploy dashboard, the same way as deployed applications.
- Learn more in the documentation.
- Postgres databases can now be directly provisioned from the Deno Deploy dashboard, hosted by our friends at Prisma.
- Like other externally linked databases, each timeline (production, git branches, and previews) in every app, gets its own isolated database schema and data.
- You can manage your database directly from the Deno Deploy dashboard, with options to export your database to your own Prisma account for further management.
- Learn more in the documentation.
- Builds have been improved further with:
- Customizable build timeouts (defaulting to 5 minutes, up to 15 minutes on Pro plan)
- Customizable memory allocation for the builder (defaulting to 3GB, up to 4GB on Pro plan)
- The directory that builds run in can now be customized, enabling deployment of applications inside of a monorepo
- Applications can now set the runtime working directory that is used when booting up the application after a successful build
- Users can now sign in to Deno Deploy using Google, in addition to GitHub.
- From the account settings page, you can link both Google and GitHub to your account, allowing you to sign in with either provider.
- The playgrounds list on the organization overview page has been merged into the apps list, allowing you to see and manage all your deployed code from a single place.
- Playgrounds now have an overview page, similar to apps, showing metrics, builds, logs, and traces.
- Playgrounds can now be assigned custom domains through the settings.
- The domain and database dropdowns in the assignment drawers now support searching, making it easier to find the domain or database you want to assign when you have many.
- Billing and metering has moved to a new dedicated page per organization, showing detailed usage breakdowns, invoice history, and payment methods, plan details, and more.
- Applications now have a dedicated databases page, showing all linked databases, their status, and options to manage them.
- The .env import field in the environment variable drawer is now more visible, and now supports drag-and-drop of .env files
Bug fixes Jump to heading #
- Fixed a bug where the percentage in usage alert emails was off by 100x (e.g. showing 100% instead of 1%). This was caused by a decimal vs percentage mixup.
- The package managers npm , yarn , and pnpm now more reliably install dependencies during the build step.
- The environment variable value input field now handles multiline values correctly, and does not strip out newlines anymore.
- Fix some organizations not being unsuspended immediately after verifying with a credit card.
- Fix some builds hanging when a user provided install or build command does not terminate quickly on SIGTERM.
- Changing the slug of a database instance now correctly updates the slug in the URL bar, ensuring that the page can be refreshed without error.
- Build timeouts are now displayed as timeouts, rather than generic cancellations, in the build logs, and build history.
- A timeline does not have to be unlocked anymore before being able to be locked to a new revision, if already locked to a revision.
September 26th, 2025 Jump to heading #
Features Jump to heading #
- Metering and billing is now enabled for all organizations on Deno Deploy.
- After creation, all organizations default to the Free plan, which includes generous free usage limits each month. To learn more about the Free and Pro plans, see the pricing page .
- Free organizations that exceed their usage on requests, bandwidth, or CPU or memory time will have their applications paused until the next billing cycle, while Pro organizations will be billed for the overage at the end of the month.
- Organizations can only make use of restricted Free plan limits until they verify their organization by linking a credit card.
- The Pro plan enables features such as wildcard custom domains, priority support, and increased included limits.
- Spend limits are available to Pro organizations, allowing you to cap your monthly spend to avoid unexpected charges.
- Deno Deploy now supports issuing OIDC tokens for all applications at runtime, allowing you to securely authenticate to third-party services and APIs without needing to manage long-lived static credentials.
- OIDC tokens can be retrieved with the @deno/oidc module on JSR .
- When authenticating to AWS or GCP, you can make use of the Cloud Connections feature instead, which will guide you through set up and automatically handle token retrieval and rotation for you. Learn more about Cloud Connections .
- Learn more about OIDC on Deno Deploy in the documentation .
- In addition to TLS certificates provisioned automatically through Let's Encrypt by Deno Deploy, you can now upload and manage custom TLS certificates for your domains. This is useful for organizations that use EV or OV certificates, or have specific compliance requirements.
- If a certificate nears expiration, we'll send email reminders to the organization owners to renew the certificate.
- This feature is only available to organizations on the Pro plan.
- Applications that are linked to GitHub repositories now dispatch GitHub repository_dispatch events every time a build is started, or completed (successfully or failed). These events can be picked up by GitHub Actions workflows to trigger additional actions, such as notifying a Slack channel, or running additional tests. See the documentation for more details.
- Domains can now be unassigned from an application through the organization domains page, without needing to go to the application settings.
Bug fixes Jump to heading #
- When bulk importing environment variables, the heuristic to detect whether a variable is a secret or plain text has been improved. Now, variables with keys containing PUBLIC_ are always treated as plain text.
- Some metrics on the organization and app metrics pages were displaying second values as milliseconds, causing them to appear 1000x too low. This has been fixed.
August 27th, 2025 Jump to heading #
Features Jump to heading #
- Deno KV can now be used with the database integration:
- Provision a Deno KV database through the "Databases" tab, and link it to an app or playground.
- Access the Deno KV database from your code by using Deno.openKv() .
- KV queues, read-replication, manual backups, and choosing a primary region are not available at this time.
- Playgrounds now support dragging in individual files and folders.
- The playground file explorer now supports inline rename and delete of files.
- New built-in environment variables have been added to enable detection of Deno Deploy EA, and the app that is running, and the organization it is running in: DENO_DEPLOY=1 , DENO_DEPLOY_ORG_ID , DENO_DEPLOY_ORG_SLUG , DENO_DEPLOY_APP_ID , DENO_DEPLOY_APP_SLUG , DENO_DEPLOY_REVISION_ID .
- Users can now create personal access tokens from their account page.
- The Deno Deploy EA dashboard has migrated from https://app.deno.com to https://console.deno.com . All existing URLs will automatically redirect to the new URL.
Bug fixes Jump to heading #
- Check that Postgres database instances support dynamic provisioning of databases before allowing them to be linked to an organization.
- Ensure that deleted Deno Deploy apps will never trigger GitHub status checks on push to the previously linked repo.
- The playground HTTP explorer now correctly sends the set headers when making requests.
- Playgrounds do not error on top level await anymore.
- You can now add environment variables named GOOGLE_APPLICATION_CREDENTIALS to your Deno Deploy app.
- When bulk importing environment variables in the app settings, we now correctly import them into that app, rather than mistakenly importing them into the organization environment variables.
- Some versions of Next.js, that do not support using declarations, now correctly build again.
- npm install in the build step now works more reliably, and does not fail with certificate related issues anymore.
July 23rd, 2025 Jump to heading #
Features Jump to heading #
- New: Database support for Deno Deploy apps, allowing you to easily connect to and use Postgres databases in your applications.
- Provision a Postgres database instance on AWS RDS, Neon, Supabase, or any other provider and then link it to your Deno Deploy organization.
- Assign the database instance to an application, making it available in the application's environment.
- Every timeline (production, each git branch, and previews) has their own isolated database with a separate schema and data, allowing you to test migrations and changes without affecting production data.
- Use any Postgres client library to connect, including npm:pg , npm:drizzle , or npm:kysely .
- Applications and playgrounds can now be renamed. Note, old deno.net URLs will no longer work after renaming, but custom domains will continue to function.
- Applications and playgrounds can now be deleted.
- Playgrounds now have an HTTP Explorer tab that allows you to make arbitrary HTTP requests to any URL served by the playground. This is useful for testing APIs or other services that do not serve a web page.
- You can now delete entire folders in the playground file explorer by pressing the delete button next to the folder name.
- You can now drag a zip file onto the playground file explorer to upload all files in the zip file to the playground.
- You can now enable auto-format on save in the playground, which will automatically format your code when you save a file.
Bug fixes Jump to heading #
- DENO_ prefixed environment variables such as DENO_CONDITIONS , DENO_COMPAT , and DENO_AUTH_TOKENS can now be set without error.
- The DENO_REVISION_ID environment variable is now correctly exposed to applications and playgrounds.
- The custom domain assignment drawer now shows custom domains that are already assigned to another application or playground as disabled.
- The network usage graph on the metrics page now correctly shows incoming and outgoing traffic. Previously, the data shown was incorrect.
- For newly created organizations the first build now waits until the <org>.deno.net domain is provisioned before the routing step.
- Pressing Ctrl-S / Cmd-S in the playground now saves the current file and triggers a build, instead of opening the browser's save dialog.
- Viewing some specific traces previously hung the trace viewer. These now show correctly.
July 9th, 2025 Jump to heading #
Features Jump to heading #
- New: Cloud Connect allows you to securely connect your Deno Deploy apps to AWS and GCP, enabling you to use services like AWS S3, Google Cloud Storage, without needing to manage credentials.
- This is done without storing any long-lived static credentials, but rather using short-lived tokens and OIDC (OpenID Connect) to establish a trust relationship between Deno Deploy and your cloud provider.
- A setup flow in the app settings page, or a drawer in playgrounds, guides you through the process of connecting your app to AWS or GCP.
- You can use the standard AWS and GCP SDKs to access the services - no need to re-write any code to use a different API.
- Learn more in the documentation.
- The application metrics page now shows more metrics, including V8 memory metrics such as heap size and garbage collection stats, as well as process level metrics such as CPU usage and overall memory usage.
- There is now a new "Metrics" tab in the organization overview that shows overall metrics for all applications in the organization, including the number of requests, CPU usage, and memory usage.
- You can now edit the URL you are viewing in the playground preview iframe by editing the "address bar" that is displayed above the preview.
- Environment variables now default to being a secret when the key contains SECRET , KEY , TOKEN , PRIVATE , or PASSWORD . You can still manually switch them to plain text if needed.
- The maximum length limit for environment variable values has been increased to 4096 characters, up from 1024 characters.
Bug fixes Jump to heading #
- Playgrounds do not get stuck when attempting to deploy an empty file anymore.
- Playground drawer resizing now works more reliably, especially when some drawers are collapsed.
- Builds now take significantly less time to complete, especially for larger projects. The "Warmup" and "Routing" steps, which previously took more than 10 seconds respectively, now usually take less than 1 second each.
- Builds can now be cancelled while they are in the "Queueing" and "Routing" steps.
- The organization creation page now correctly displays whether an organization slug is taken or not, prior to submitting the form.
- npm install can now install esbuild again - previously it would fail with a generic error.
June 24th, 2025 Jump to heading #
Features Jump to heading #
- The playground now has live-streaming logs and traces panels
- Logs and traces for the current revision are displayed for the past hour
- Logs and traces can be filtered, just like in the dedicated observability pages
- Framework auto-detection now works for more projects out of the box, including many Vite-based projects
- The organization dropdown now highlights the currently selected organization more clearly
Bug fixes Jump to heading #
- The sparklines in the metrics overview are now working correctly
- The error rate metric now functions properly
- GitHub-triggered builds no longer run multiple times
- Next.js builds now work more reliably on older Next.js versions
June 12th, 2025 Jump to heading #
Features Jump to heading #
- Deno Deploy now supports playgrounds!
- Playgrounds can be created and accessed from the playgrounds tab in the organizations overview
- Playgrounds can contain multiple files and include build steps
- The playground UI features an iframe to preview your deployed app
- Three templates are currently available: hello world, Next.js, and Hono
- On mobile devices, there is now a floating navbar that doesn't intrude into page content
June 9th, 2025 Jump to heading #
Features Jump to heading #
- Deno Deploy has a new logo!
- Anyone can now join by signing up at console.deno.com
- Builds
- Builds can now use up to 8 GB of storage, up from 2 GB
- Builds can now use environment variables and secrets configured in the organization or app settings (in the new "Build" context)
- Builds now have a maximum runtime of 5 minutes
- The metrics page has had a complete overhaul by rewriting the chart rendering:
- Dragging on a graph now zooms in on the selected area
- Much more data can now be shown without the page becoming slow to load
- The tooltip now follows the mouse cursor, together with a new crosshair that allows for precise analysis
- Font sizes and colors have been improved for better readability
Bug fixes Jump to heading #
- Builds should not get stuck in a pending state anymore
- Dashboard pages now load significantly faster
- Correctly shows spans in traces that have parents that are not exported (yet)
- The metrics page correctly refreshes now when switching time ranges
- The "Clear search" button in the telemetry search bar now works correctly
- Older Next.js versions (such as Next.js 13) build correctly now
- The environment variable drawer is now used everywhere, fixing a bug where multiple env vars with the same name but different contexts would conflict
- Running node <path> in the builder does not fail anymore when the path is absolute
- npx is now available in the builder
- Astro builds will not sporadically fail with --unstable-vsock errors anymore
- Svelte projects now deploy correctly when a project explicitly specifies @deno/svelte-adapter
May 26th, 2025 Jump to heading #
Features Jump to heading #
- When triggering a manual build you can now choose which branch to deploy
- You can now deploy Astro static sites without having to manually install the Deno adapter
- There are now reference docs for you to peruse .
Bug fixes Jump to heading #
- SvelteKit auto-detection now works when using npm as the package manager
- Prewarming does not trigger random POST requests to your app anymore
- Visiting a page with a trailing slash will not 404 anymore
- Drawers will no longer close if you click inside, hold and drag over the backdrop, and release
May 22nd, 2025 Jump to heading #
Features Jump to heading #
- You can now bulk import env vars during app creation by pasting a .env file into the env var drawer
- SvelteKit now works out of the box without manually installing the Deno adapter
- A preset for the Lume static site generator is now available
Bug fixes Jump to heading #
- Environment variables now show up correctly on the timelines page
- The production timeline page now correctly shows all builds
- console.deno.com works on older versions of Firefox now
- Page titles across console.deno.com now reflect the page you are on
- The "Provision certificate" button does not lock up after DNS verification failures anymore
- Domains that had a provisioned certificate or attached application can now be deleted


---

URL: https://docs.deno.com/deploy/kv/key_space/
TITLE: Key Space

Key Space
Deno KV is a key value store. The key space is a flat namespace of key+value+versionstamp pairs. Keys are sequences of key parts, which allow modeling of hierarchical data. Values are arbitrary JavaScript objects. Versionstamps represent when a value was inserted / modified.
Keys Jump to heading #
Keys in Deno KV are sequences of key parts, which can be string s, number s, boolean s, Uint8Array s, or bigint s.
Using a sequence of parts, rather than a single string eliminates the possibility of delimiter injection attacks, because there is no visible delimiter.
A key injection attack occurs when an attacker manipulates the structure of a key-value store by injecting delimiters used in the key encoding scheme into a user controlled variable, leading to unintended behavior or unauthorized access. For example, consider a key-value store using a slash (/) as a delimiter, with keys like "users/alice/settings" and "users/bob/settings". An attacker could create a new user with the name "alice/settings/hacked" to form the key "users/alice/settings/hacked/settings", injecting the delimiter and manipulating the key structure. In Deno KV, the injection would result in the key ["users", "alice/settings/hacked", "settings"] , which is not harmful.
Between key parts, invisible delimiters are used to separate the parts. These delimiters are never visible, but ensure that one part can not be confused with another part. For example, the key parts ["abc", "def"] , ["ab", "cdef"] , ["abc", "", "def"] are all different keys.
Keys are case sensitive and are ordered lexicographically by their parts. The first part is the most significant, and the last part is the least significant. The order of the parts is determined by both the type and the value of the part.
Key Part Ordering Jump to heading #
Key parts are ordered lexicographically by their type, and within a given type, they are ordered by their value. The ordering of types is as follows:
- Uint8Array
- string
- number
- bigint
- boolean
Within a given type, the ordering is:
- Uint8Array : byte ordering of the array
- string : byte ordering of the UTF-8 encoding of the string
- number : -Infinity < -1.0 < -0.5 < -0.0 < 0.0 < 0.5 < 1.0 < Infinity < NaN
- bigint : mathematical ordering, largest negative number first, largest positive number last
- boolean : false < true
This means that the part 1.0 (a number) is ordered before the part 2.0 (also a number), but is greater than the part 0n (a bigint), because 1.0 is a number and 0n is a bigint, and type ordering has precedence over the ordering of values within a type.
Key Examples Jump to heading #
[ "users" , 42 , "profile" ] ; // User with ID 42's profile [ "posts" , "2023-04-23" , "comments" ] ; // Comments for all posts on 2023-04-23 [ "products" , "electronics" , "smartphones" , "apple" ] ; // Apple smartphones in the electronics category [ "orders" , 1001 , "shipping" , "tracking" ] ; // Tracking information for order ID 1001 [ "files" , new Uint8Array ( [ 1 , 2 , 3 ] ) , "metadata" ] ; // Metadata for a file with Uint8Array identifier [ "projects" , "openai" , "tasks" , 5 ] ; // Task with ID 5 in the OpenAI project [ "events" , "2023-03-31" , "location" , "san_francisco" ] ; // Events in San Francisco on 2023-03-31 [ "invoices" , 2023 , "Q1" , "summary" ] ; // Summary of Q1 invoices for 2023 [ "teams" , "engineering" , "members" , 1n ] ; // Member with ID 1n in the engineering team
Universally Unique Lexicographically Sortable Identifiers (ULIDs) Jump to heading #
Key part ordering allows keys consisting of timestamps and ID parts to be listed chronologically. Typically, you can generate a key using the following: Date.now() and crypto.randomUUID() :
async function setUser ( user ) { await kv . set ( [ "users" , Date . now ( ) , crypto . randomUUID ( ) ] , user ) ; }
Run multiple times sequentially, this produces the following keys:
[ "users" , 1691377037923 , "8c72fa25-40ad-42ce-80b0-44f79bc7a09e" ] ; // First user [ "users" , 1691377037924 , "8063f20c-8c2e-425e-a5ab-d61e7a717765" ] ; // Second user [ "users" , 1691377037925 , "35310cea-58ba-4101-b09a-86232bf230b2" ] ; // Third user
However, having the timestamp and ID represented within a single key part may be more straightforward in some cases. You can use a Universally Unique Lexicographically Sortable Identifier (ULID) to do this. This type of identifier encodes a UTC timestamp, is lexicographically sortable and is cryptographically random by default:
import { ulid } from "jsr:@std/ulid" ; const kv = await Deno . openKv ( ) ; async function setUser ( user ) { await kv . set ( [ "users" , ulid ( ) ] , user ) ; }
[ "users" , "01H76YTWK3YBV020S6MP69TBEQ" ] ; // First user [ "users" , "01H76YTWK4V82VFET9YTYDQ0NY" ] ; // Second user [ "users" , "01H76YTWK5DM1G9TFR0Y5SCZQV" ] ; // Third user
Furthermore, you can generate ULIDs monotonically increasingly using monotonicUlid function:
import { monotonicUlid } from "jsr:@std/ulid" ; async function setUser ( user ) { await kv . set ( [ "users" , monotonicUlid ( ) ] , user ) ; }
// Strict ordering for the same timestamp by incrementing the least-significant random bit by 1 [ "users" , "01H76YTWK3YBV020S6MP69TBEQ" ] ; // First user [ "users" , "01H76YTWK3YBV020S6MP69TBER" ] ; // Second user [ "users" , "01H76YTWK3YBV020S6MP69TBES" ] ; // Third user
Values Jump to heading #
Values in Deno KV can be arbitrary JavaScript values that are compatible with the structured clone algorithm . This includes:
- undefined
- null
- boolean
- number
- string
- bigint
- Uint8Array
- Array
- Object
- Map
- Set
- Date
- RegExp
Objects and arrays can contain any of the above types, including other objects and arrays. Map s and Set s can contain any of the above types, including other Map s and Set s.
Circular references within values are supported.
Objects with a non-primitive prototype are not supported (such as class instances or Web API objects). Functions and symbols can also not be serialized.
Deno.KvU64 type Jump to heading #
In addition to structured serializable values, the special value Deno.KvU64 is also supported as a value. This object represents a 64-bit unsigned integer, represented as a bigint. It can be used with the sum , min , and max KV operations. It can not be stored within an object or array. It must be stored as a top-level value.
It can be created with the Deno.KvU64 constructor:
const u64 = new Deno . KvU64 ( 42n ) ;
Value Examples Jump to heading #
undefined; null; true; false; 42; -42.5; 42n; "hello"; new Uint8Array([1, 2, 3]); [1, 2, 3]; { a: 1, b: 2, c: 3 }; new Map([["a", 1], ["b", 2], ["c", 3]]); new Set([1, 2, 3]); new Date("2023-04-23"); /abc/; // Circular references are supported const a = {}; const b = { a }; a.b = b; // Deno.KvU64 is supported new Deno.KvU64(42n);
Versionstamp Jump to heading #
All data in the Deno KV key-space is versioned. Every time a value is inserted or modified, a versionstamp is assigned to it. Versionstamps are monotonically increasing, non-sequential, 12 byte values that represent the time that the value was modified. Versionstamps do not represent real time, but rather the order in which the values were modified.
Because versionstamps are monotonically increasing, they can be used to determine whether a given value is newer or older than another value. This can be done by comparing the versionstamps of the two values. If versionstamp A is greater than versionstamp B, then value A was modified more recently than value B.
versionstampA > versionstampB ; "000002fa526aaccb0000" > "000002fa526aacc90000" ; // true
All data modified by a single transaction are assigned the same versionstamp. This means that if two set operations are performed in the same atomic operation, then the versionstamp of the new values will be the same.
Versionstamps are used to implement optimistic concurrency control. Atomic operations can contain checks that ensure that the versionstamp of the data they are operating on matches a versionstamp passed to the operation. If the versionstamp of the data is not the same as the versionstamp passed to the operation, then the transaction will fail and the operation will not be applied.


---

URL: https://docs.deno.com/deploy/getting_started/
TITLE: Getting started

Getting started
Create an organization Jump to heading #
Deno Deploy will eventually replace Deno Deploy Classic. Until that time both systems will be available simultaneously. By creating an organisation for use with Deploy it is possible to explore Deploy without disrupting projects already using Deploy Classic.
To get started with Deno Deploy:
- Visit console.deno.com
- Create an organization:
Note that you cannot create an organization with the same slug as any existing project in Deploy Classic. Organization names and slugs cannot be changed after creation.
Create an app Jump to heading #
After creating an organization, you'll be directed to the organization apps page, which shows all your applications and provides access to organization settings and custom domains.
To create an app, press the + New App button:
An application is a single deployed web service with one build configuration, build history, environment variables, attached custom domains, a linked GitHub repository, etc.
Select a repo Jump to heading #
- Choose the GitHub repository for your application:
If your repository doesn't appear, use the Add another GitHub account or Configure GitHub App permissions buttons to grant the Deno Deploy GitHub app access to your repositories.
 Mono-repos (repositories where the application lives in a subdirectory) are not yet supported.
Configure your app Jump to heading #
Deno Deploy automatically attempts to detect your application type and configure an appropriate build setup. You can see the detected configuration in the App Config box:
To modify this configuration, click Edit build config .
Configure your build Jump to heading #
In the build config drawer, you can customize:
Framework preset Jump to heading #
Select your framework or choose No Preset if using a custom setup.
Install command Jump to heading #
Command for installing dependencies (e.g., npm install , deno install ). This can be empty for Deno applications without a package.json .
Build command Jump to heading #
Command to compile/bundle your application (e.g., next build , deno task build ). Leave empty if your application doesn't require building.
Runtime configuration Jump to heading #
For most frameworks there are no options to configure here, as Deno Deploy will figure out the ideal runtime configuration for the app based on the framework preset. When a framework is not configured, you can choose here whether the app is a Dynamic app that needs to execute code server side for every request, such as an API server, server-side rendered application, etc., or a Static app that consists only of a set of static files that need to be hosted.
Dynamic Entrypoint Jump to heading #
The JavaScript or TypeScript file that should be executed to start the application. This is the file path that you would pass locally to deno run or node to start the app. The path has to be relative to the working directory.
Dynamic arguments Jump to heading #
Additional command line arguments to pass to the app on startup, after the entrypoint. These are arguments that are passed to the application not to Deno itself.
Static Directory Jump to heading #
The directory in the working directory that contains the static files to be served. For example, dist , _site , or .output .
Single Page App mode Jump to heading #
Whether the application is a single page app that should have the root index.html served for any paths that do not exist as files in the static directory, instead of a 404 page.
Closing the drawer saves the settings.
Environment variables Jump to heading #
To add environment variables:
- Click Add/Edit environment variables
- Click + Add variable in the drawer
- Enter the name and value
- Choose whether it's a plain text variable or secret
- Select the contexts where it should be available:
- Production : For requests to production domains
- Development : For requests to preview/branch domains
- Click Save to apply your changes
Build and deploy your app Jump to heading #
- Click Create App to create the application and start the first build
- Watch the build progress through the live logs:
The build logs show these stages:
- Prepare : Cloning the repository and restoring caches
- Install : Running the install command and framework-specific setup
- Build : Executing the build command and preparing the deployment artifact
- Warm up : Testing the deployment with a request
- Route : Deploying the build to global regions
You can cancel a build with the button in the top-left corner, or restart failed builds from the same location.
After completion, the top-right shows the preview URL, and below that, all timelines where the build is deployed.
Monitor your application Jump to heading #
After deploying, use the observability tools to monitor your application:
Logs Jump to heading #
View application logs with filtering options for context, revision, and text content:
Use the search bar to filter logs (e.g., context:production , revision:<id> ). The time picker adjusts the displayed time range.
If a log is associated with a trace, you can click "View trace" to see the corresponding trace information.
Traces Jump to heading #
View request traces with detailed timing information:
Click any trace to open the trace view showing all spans in a waterfall visualization:
The trace view shows:
- Timeline of spans with duration
- Span details including attributes
- Logs emitted during the span To save the environment variables, press the save button. You can re-open the drawer to edit / remove environment variables you have added.
You can also edit the app name on this page, and select which region(s) the application should be served from.
Build and deploy your app Jump to heading #
Finally, you can press the Create App button to create the app. This will create the app and immediately trigger the first build:
On the build page you can see live streaming build logs split into multiple sections:
- Prepare: cloning the GitHub repository and restoring build cache
- Install: executing the install command, and any framework specific pre-install setup
- Build: executing the build command, any framework specific pre- and post-build setup, and preparing the build artifact for deployment
- Warm up: sending a request to the preview URL of the deployment to ensure it starts up correctly. The logs shown in the Warm up section are Runtime logs, not build logs.
- Route: Deno Deploy is rolling out the new version of this build into all global regions.
In the top left of this build is a button to cancel the build. For failed builds, there is also a button to restart the build.
For completed builds, the top right shows the preview URL of the build. Further down all timelines that this build is deployed to are shown, such as Production , or Git Branch timelines.
You can also see how the build was triggered on this page. This can either be manual action , for builds triggered through the UI, or GitHub repo for builds triggered through the GitHub integration.
You can view the application through either the preview URL, or any of the other URLs shown in the timelines list.
Monitor your application Jump to heading #
After visiting your application, you can view telemetry about your application in the form of the logs and traces available in our observability panels. You can visit these pages by clicking the respective buttons in the left sidebar.
Logs Jump to heading #
The logs page shows all recent logs in the project. By default logs from all contexts (production and development) are shown, but using the filter button and search bar at the top, the shown logs can be restricted. For example, to filter to only production logs, add context:production to the search bar. To only show logs from a certain revision, use revision:<id> etc.
You can also use full text search in the search bar. The full text search fill filter down the log entries to only those containing the text written, case-insensitively.
By default logs from the last hour are shown. The time picker in the top right can be used to adjust the time frame that logs are shown for. The time zone of the timestamps shown is the time zone set in the time picker.
The "view trace" button on the right of a log line shows up if a log line is correlated with a trace. This happens when a log line occurs within an active trace. Clicking this button will open the respective trace as an overlay.
Traces Jump to heading #
The traces page shows all recent traces in the project. By default traces from all contexts (production and development) are shown, but using the filter button and search bar at the top, the shown traces can be restricted. For example, to filter to only production traces, add context:production to the search bar. To only show traces from a certain revision, use revision:<id> etc.
All traces that contain an incoming HTTP request are shown in the list. The text shown for each trace is the path of the request, and the duration of the trace in milliseconds.
Clicking on a trace will open the trace view, which shows the full trace including all spans and logs that are part of the trace.
For each span in the trace you can see the duration of the span, the name of the span, the start and end time, and the recorded attributes. By clicking on a span in the timeline, the details of that span will be shown in the summary panel at the bottom.
The logs that are emitted as part of a given span are shown in the logs tab at the bottom. Changing the selected span will update which logs are shown in this panel.


---

URL: https://docs.deno.com/deploy/reference/accounts/
TITLE: Accounts

Accounts
Deno Deploy supports logging in with GitHub.
Your primary contact email address and name are synced from GitHub. Both your username and email address update on every sign in. After changing your email, login, or name on GitHub, sign in again to see these changes reflected in the Deno Deploy dashboard.
Deno also supports logging in with Google accounts.
Users authenticating with Deno Deploy using a Google account will also need to provide GitHub credentials when creating new applications in order to access GitHub repositories for deployments.


---

URL: https://docs.deno.com/deploy/fulfillment_policy/
TITLE: Fulfillment Policy

Fulfillment Policy
Refund Policy Jump to heading #
At Deno Deploy, we strive to provide exceptional service. If you are not satisfied with our service, you may request a refund under the following conditions:
A refund must be requested within 14 days of the initial purchase or upgrade of any subscription plan. Refunds may be considered if the service fails to function correctly and if the issue cannot be resolved by our support team within a reasonable time frame. No refunds will be issued for services used in violation of our terms of service or for problems clearly attributable to user error or external platform changes. Recurring subscriptions may be canceled but are only eligible for a refund for the initial billing cycle if requested within the 14-day period.
Cancellation Policy Jump to heading #
You can cancel your Deno Deploy or Deno Deploy Classic subscription at any time under the following terms:
Subscription cancellations are effective immediately, and the service will continue to run until the end of the current billing period. To cancel your subscription, please navigate to your account settings on the Deno Deploy dashboard and select 'Cancel Subscription'. Once the subscription is canceled, no further charges will be incurred, but you are responsible for any charges accrued before the effective date of cancellation. Contact Us For more information about our fulfillment policies, or if you require assistance, please contact our support team at deploy@deno.com .


---

URL: https://docs.deno.com/deploy/usage/
TITLE: Deno Deploy Usage Guidelines

Deno Deploy Usage Guidelines
Deno Deploy offers a generous free tier that allows you to run applications at the edge with minimal cost. However, to ensure a fair and reliable service for all users, there are certain usage guidelines and limitations in place. To review the pricing details, please visit the Deno Deploy pricing page .
If your application experiences a surprise traffic spike, we want to protect you from an unexpectedly large bill. At the same time, we don't want to dampen your success by automatically suspending traffic when limits are reached if that is your preference.
Before October 1st 2025, you'll be able to configure alert thresholds and hard spending limits directly from your organization dashboard. These controls, along with timely notifications about your quota usage, will help protect you from unexpected billing charges.
Youll be able to review and control these in the Billing section of your console.
The Deno company is now using Deno Deploy to host our own websites and is putting significant efforts into ensuring service reliability.
- Deno Deploy Acceptable Use Policy
- Deno Deploy Terms and Conditions
Deno reserves the right to terminate any user, organization, or app that we find to be in violation of the terms and conditions.


---

URL: https://docs.deno.com/deploy/reference/observability/
TITLE: Observability

Observability
Deno Deploy provides comprehensive observability features to help you understand application performance, debug errors, and monitor usage. These features leverage OpenTelemetry and the built-in OpenTelemetry integration in Deno .
The three main observability features in Deno Deploy are:
- Logs : Unstructured debug information emitted by your application code
- Traces : Structured information about request handling, including execution time for each step and automatic capture of outbound I/O operations
- Metrics : Structured, high-level data about application performance and usage, such as request count, error count, and latency
Logs Jump to heading #
Logs in Deno Deploy are captured using the standard console API and can be queried from the logs page in the dashboard.
Logs are organized by application. You can use the search bar to filter logs based on various attributes and message content.
When logs are emitted inside the context of a trace, they become associated with that specific trace and span. For such logs, a "View trace" button appears in the logs interface, allowing you to open the relevant trace in an overlay drawer for detailed inspection.
Traces Jump to heading #
Traces in Deno Deploy are captured in three ways:
- Automatically for built-in operations : Incoming HTTP requests, outbound fetch calls, and other system operations are traced automatically. This cannot be disabled.
- Automatically for supported frameworks : Frameworks like Next.js, Fresh, and Astro include built-in instrumentation. The specific frameworks and operations covered may change over time.
- Manually through custom instrumentation : Your application code can create new traces or spans using the OpenTelemetry API.
Traces are organized by application. The search bar lets you filter based on various attributes and span names.
Clicking a trace opens the trace overlay drawer, showing all spans within that trace in a waterfall view. This visualization displays the start time, end time, and duration of each span, grouped by parent span with the root span at the top.
Clicking any span shows its details at the bottom of the drawer, including all captured attributes. For example, outbound HTTP requests include the method, URL, and status code.
The span details section also includes a "Logs" tab showing all logs emitted within the selected span's context.
You can click "View logs" on any trace to open the logs page with the trace ID pre-filled in the search bar, showing all logs related to that trace.
Metrics Jump to heading #
Metrics in Deno Deploy are automatically captured for various operations such as incoming HTTP requests and outbound fetch calls. This automatic capture cannot be disabled.
Metrics are organized by application and displayed in time-series graphs showing values over time. You can use the search bar to filter metrics based on various attributes.
Filtering Jump to heading #
Logs, traces, and metrics can be filtered using these general attributes:
- Revision : The ID of the application revision that emitted the data
- Context : The context in which the data was emitted ("Production" or "Development")
For logs and traces, this additional filter is available:
- Trace : The ID of the trace containing the log or spans
For traces only, these additional filters are available:
- HTTP Method : The HTTP method of the request that triggered the trace
- HTTP Path : The path of the request that triggered the trace
- HTTP Status : The HTTP status code of the response
Time range filter Jump to heading #
By default, the observability pages show data for the last hour. You can change this using the time range filter in the top right corner of each page.
You can select predefined time ranges like "Last 1 hour," "Last 24 hours," or "Last 7 days," or set a custom time range by clicking the "Custom" button.
Custom time ranges can be either absolute (specific start and end times) or relative (e.g., 3 days ago, 1 hour from now). Relative time ranges use the same syntax as Grafana:
- now - the current time
- now-1h - 1 hour ago
- now/h - the start of the current hour
- now-1h/h - the start of the previous hour
- now/d+3h - 3 hours from the start of the current day
- now-1d/d - the start of the previous day page. The time range filter can be set to a predefined time range, like "Last 1 hour", "Last 24 hours", or "Last 7 days", or a custom time range.
The custom time range can be set by clicking on the "Custom" button. A custom time range can either be absolute (a specific start and end time) or relative (3 days ago, 1 hour from now, etc.). The time range filter is shown in the top right corner of the page.
Relative time ranges use the same syntax as Grafana, where now is the current time, and now-1h is 1 hour ago. Furthermore syntax such as now-1h/h can be used to round the time to the nearest hour. Some examples:
- now-1h - 1 hour ago
- now/h - the start of the current hour
- now-1h/h - the start of the previous hour
- now/d+3h - 3 hours from the start of the current day
- now-1d/d - the start of the previous day


---

URL: https://docs.deno.com/deploy/terms_and_conditions/
TITLE: Terms and Conditions

Terms and Conditions
DENO TERMS AND CONDITIONS 09 September 2024
These Terms and Conditions (these Terms) are a legal agreement between you and Deno Land Inc. (Deno, we, us, or our). They specify the terms under which you may access and use (i) our website at https://deno.com (the Site); (ii) any websites, applications or other digital properties that link to these Terms; and (iii) the products and services (the Deno Offerings) we offer to you on our proprietary platform (the Platform) via the following websites:
- Deno Deploy ( https://deno.com/deploy )
- Deno Deploy Classic ( https://deno.com/deploy/classic/ )
- Deno Subhosting ( https://deno.com/subhosting )
By accessing or using the Site or any other digital property that links to these Terms, you may learn about Deno and our technology platform, and registered customers may also access the Deno Offerings (collectively, the Services).
PLEASE READ THESE TERMS CAREFULLY. BY ACCESSING AND/OR USING THE SERVICES, YOU ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTOOD, AND AGREE TO BE LEGALLY BOUND BY THESE TERMS, THE DATA PROCESSING ADDENDUM (THE DPA), AND THE TERMS AND CONDITIONS OF OUR PRIVACY POLICY (THE PRIVACY POLICY), WHICH ARE HEREBY INCORPORATED INTO THESE TERMS AND MADE A PART HEREOF BY REFERENCE (COLLECTIVELY, THE AGREEMENT). IF YOU DO NOT AGREE TO ANY OF THE TERMS IN THIS AGREEMENT, THEN PLEASE DO NOT USE THE SERVICES.
If you accept or agree to the Agreement on behalf of a company or other legal entity, you represent and warrant that you have the authority to bind that company or other legal entity to the Agreement and, in such event, you and your will refer and apply to that company or other legal entity.
We reserve the right, at our sole discretion, to modify, discontinue, or terminate the availability of any Services, or modify this Agreement, at any time and without prior notice. We encourage you to check these Terms and the Last Update date above whenever you access or use the Services. By continuing to access or use the Services after we have posted a modification to these Terms, you are indicating that you agree to be bound by the modified Agreement. If the modified Agreement is not acceptable to you, your only recourse is to cease accessing or using the Services.
Deno also offers fee-based products and services (including, from time to time, as free trials), which may offer access to certain data products and/or services (Paid Products). We provide access to and use of our Paid Products pursuant to commercial agreements, associated with the applicable Paid Products made available to you at the time of purchase (each, a Commercial Agreement). If there is a conflict between these Terms and terms and conditions of the applicable Commercial Agreement associated with the Paid Products you are purchasing, the terms and conditions of the Commercial Agreement will take precedence with respect to the use of or access to such Paid Products.
Capitalized terms not defined in these Terms shall have the meaning set forth in our Privacy Policy.
THE SECTIONS BELOW TITLED BINDING ARBITRATION AND CLASS ACTION WAIVER CONTAIN A BINDING ARBITRATION AGREEMENT AND CLASS ACTION WAIVER. THEY AFFECT YOUR LEGAL RIGHTS. PLEASE READ THEM CAREFULLY.
- DESCRIPTION OF THE SERVICES; RIGHT TO ACCESS AND USE THE SERVICES
Deno Deploy and Deno Subhosting are globally distributed platforms for serverless JavaScript applications. Your JavaScript, TypeScript, and WebAssembly code runs on managed servers geographically close to your users, enabling low latency and faster response times. Deploy and Subhosting applications run on fast, light-weight V8 isolates rather than virtual machines, powered by the Deno runtime.
Subject to the terms and conditions of this Agreement, Deno hereby grants you during the term of this Agreement a limited, non-exclusive, non-transferable, non-sublicensable, revocable right, to access and use the Services solely for your internal business purposes.
Deno reserves the right to, at any time, and without notice or liability to you:
- Block and disable any deployments that, for any reason, make the Platform unstable;
- Change the regions in which the Services run,
- Change which features are supported by the Services; and
- Modify or discontinue the availability of any other feature, function, or content relating to the Services.
You agree that we will not be liable to you or to any third party for any modification, suspension, or discontinuance of the Services or any part thereof. You are free to stop using the Services at any time.
- 
ACCOUNT CREDENTIALS
In order to use the Deno Offerings, you must be an Authorized User. To become an Authorized User, you need to create an account on the Platform, and authenticate via GitHub (collectively, the Account Credentials). When creating the account, each Authorized User must provide true, accurate, current, and complete information. Each Account Credential can be used by only one Authorized User. Each Authorized User is responsible for the confidentiality and use of his/her Account Credentials, including all activities that are associated with his/her Account Credentials. Authorized Users must promptly inform us of any need to deactivate any Account Credentials. Deno is under no obligation to accept any individual as Authorized User, and may accept or reject any registration in its sole and complete discretion. We have the right to disable any Account Credentials at any time for any reason, including if in our sole discretion if we believe that you have failed to comply with these Terms.
- 
USE OF PERSONAL INFORMATION
Your use of the Services may involve the transmission to us of certain personal information. Our policies with respect to the collection and use of such personal information are governed according to our Privacy Policy, which is hereby incorporated by reference in its entirety.
- INTELLECTUAL PROPERTY
The Services may contain material, such as software, text, graphics, images, sound recordings, audiovisual works, and other material provided by or on behalf of Deno (collectively referred to as the Content). The Content may be owned by us or by third parties. The Content is protected under both United States and foreign laws. Unauthorized use of the Content may violate copyright, trademark, and other laws. You have no rights in or to the Content, and you will not use the Content except as permitted under this Agreement. No other use is permitted without prior written consent from us. You must retain all copyright and other proprietary notices contained in the original Content on any copy you make of the Content. You may not sell, transfer, assign, license, sublicense, or modify the Content or reproduce, display, publicly perform, make a derivative version of, distribute, or otherwise use the Content in any way for any public or commercial purpose. The use or posting of the Content on any other website or in a networked computer environment for any purpose is expressly prohibited.
If you violate any part of this Agreement, your permission to access and/or use the Content, and the Services automatically terminates and you must immediately destroy any copies you have made of the Content.
The trademarks, service marks, and logos of Deno (the Deno Trademarks) used and displayed on the Services are registered and unregistered trademarks or service marks of Deno. Other company, product, and service names located on the Services may be trademarks or service marks owned by others (the Third-Party Trademarks, and, collectively with Deno Trademarks, the Trademarks). Nothing on the Services should be construed as granting, by implication, estoppel, or otherwise, any license or right to use the Trademarks, without our prior written permission specific for each such use. Use of the Trademarks as part of a link to or from any website is prohibited unless establishment of such a link is approved in advance by us in writing. All goodwill generated from the use of Deno Trademarks inures to our benefit.
Elements of the Services are protected by trade dress, trademark, unfair competition, and other state and federal laws and may not be copied or imitated in whole or in part, by any means, including, but not limited to, the use of framing or mirrors. None of the Content may be retransmitted without our express, written consent for each and every instance.
- USER DATA; USAGE DATA; AGGREGATE DATA
For purposes of this Agreement, User Data means (i) any data and information that we ingest by connecting to Authorized Users business systems, including but not limited to event logs; and (ii) any data and information that Authorized Users submit through the Services; and Usage Data means anonymous, analytical data that Deno collects concerning the performance and your use of the Services, including, without limitation, date and time that you access the Services, the portions of the Services visited, the frequency and number of times such pages are accessed, the number of times the Services is used in a given time period and other usage and performance data.
As between the parties, Authorized Users own all right, title, and interest in and to User Data, including all modifications, improvements, adaptations, enhancements, or translations made thereto, and all intellectual rights therein. Authorized Users hereby grant Deno a non-exclusive, worldwide, fully paid-up, royalty-free right and license, with the right to grant sublicenses, to reproduce, execute, use, store, archive, modify, perform, display and distribute User Data: (i) during the term of this Agreement, in furtherance of Deno obligations hereunder; and (ii) for Denos internal business purposes, including using such data to analyze, update, and improve the Services and Denos analytics capabilities and for benchmarking purposes.
Notwithstanding anything to the contrary herein, we may use, and may permit our third-party service providers to access and use, User Data, as well as any Usage Data that we may collect, in an anonymous and aggregated form (Aggregate Data) for the purposes of operating, maintaining, managing, and improving our products and services including the Services. Aggregate Data does not identify Authorized Users or any individual. You hereby agree that we may collect, use, publish, disseminate, transfer, and otherwise exploit such Aggregate Data.
- FEES
Deno offers and Authorized Users can purchase a monthly or annual subscription for the Services (Subscription) for a fee set forth on our website (the Subscription Fee). Deno may add new fees and charges, or amend fees and charges, at any time in its sole discretion. Payment for a Subscription is due immediately upon making a purchase for a subscription. By making a purchase, you agree to pay Deno, through our third-party payment processor (Third-Party Payment Processor), all charges at the fees then in effect for Subscriptions. Any information you provide to the Third-Party Payment Processor will be processed by such Third-Party Payment Processor in accordance with its privacy policy and terms of use. YOU MUST PROVIDE CURRENT, COMPLETE, AND ACCURATE INFORMATION FOR YOUR ACCOUNT, AND PROMPTLY UPDATE ALL INFORMATION TO KEEP SUCH ACCOUNT INFORMATION CURRENT, COMPLETE, AND ACCURATE (SUCH AS A CHANGE IN BILLING ADDRESS, CREDIT CARD NUMBER, OR CREDIT CARD EXPIRATION DATE). FURTHER, YOU MUST PROMPTLY NOTIFY US IF A PAYMENT METHOD IS CANCELED (E.G., FOR LOSS OR THEFT) OR IF YOU BECOME AWARE OF A POTENTIAL BREACH OF SECURITY, SUCH AS THE UNAUTHORIZED DISCLOSURE OR USE OF YOUR USERNAME OR PASSWORD. CHANGES TO SUCH INFORMATION CAN BE MADE THROUGH YOUR ACCOUNT.
By purchasing a Subscription, you acknowledge that your Subscription has an initial and recurring payment charge at the then-current Subscription rate, and you agree that Deno may submit monthly charges, in advance to your chosen payment method without further authorization from you, until you provide notice that you wish to cancel your Subscription or to change your payment method. You further accept responsibility for all recurring charges prior to cancellation, including, where applicable, any charges processed by Deno after the expiration date of your payment card.
You may change or terminate your Subscription by emailing us at support@deno.com . If you terminate your Subscription, you may use your Subscription until the end of the then-current billing cycle, and the Subscription will not be renewed after that period expires. Deno does not refund any pre-paid portion of the Subscription fee. Deno may immediately terminate or suspend your Subscription for any reason or no reason in accordance with these Terms, including for failure to pay the applicable fees when due. If we terminate or suspend your Subscription, your right to use any software or content provided in connection with the Subscription is also terminated or suspended (as applicable).
From time to time, Deno may offer free trial of the Services. Deno reserves the right in its sole discretion to stop offering free trial of the Services at any time without any liability to you.
- COMMUNITY GUIDELINES
By accessing and/or using the Services, you hereby agree to comply with the following guidelines:
- 
You will not use the Services for any unlawful purpose;
- 
You will not access or use the Services to collect any market research for a competing businesses;
- 
You will not upload, post, e-mail, transmit, or otherwise make available any content that infringes any copyright, trademark, right of publicity, or other proprietary rights of any person or entity;
- 
You will not impersonate any person or entity or falsely state or otherwise misrepresent your affiliation with a person or entity;
- 
You will not decompile, reverse engineer, disassemble, or otherwise attempt to discern the source code or interface protocols of any software or other products or processes accessible through the Services;
- 
You will not remove or modify any proprietary markings or restrictive legends placed on the Services;
- 
You will not use the Services, or any portion or component thereof in violation of any applicable law, in order to build a competitive product or service, or for any purpose not specifically permitted in these Terms;
- 
You will not cover, obscure, block, or in any way interfere with any advertisements and/or safety features on the Services;
- 
You will not circumvent, remove, alter, deactivate, degrade, or thwart any of the protections in the Services;
- 
You will not introduce, post, or upload to the Services any Harmful Code. As used herein, Harmful Code means computer code, programs, or programming devices that are intentionally designed to disrupt, modify, access, delete, damage, deactivate, disable, harm, or otherwise impede in any manner, including aesthetic disruptions or distortions, the operation of the Services, or any other associated software, firmware, hardware, computer system, or network (including, without limitation, Trojan horses, viruses, worms, time bombs, time locks, devices, traps, access codes, or drop dead or trap door devices) or any other harmful, malicious, or hidden procedures, routines or mechanisms that would cause the Services to cease functioning or to damage or corrupt data, storage media, programs, equipment, or communications, or otherwise interfere with the operations of the Services;
- 
You will not take any action that imposes or may impose (in our sole discretion) an unreasonable or disproportionately large load on our technical infrastructure; and
- 
You will not interfere with or attempt to interrupt the proper operation of the Services through the use of any virus, device, information collection or transmission mechanism, software or routine, or access or attempt to gain access to any data, files, or passwords related to the Services through hacking, password or data mining, or any other means.
Although we are not obligated to monitor access to or use of the Services, we have the right to do so for the purpose of operating them, to ensure compliance with these Terms, and to comply with applicable law or other legal requirements. We have the right to investigate violations of these Terms or conduct that affects the Services. We may also consult and cooperate with law enforcement authorities to prosecute Users who violate the law.
If you find something that violates our User Guidelines, please let us know, and we will review it.
- LINKING AND CITATION OF CONTENT
Deno does not object to links on third-party Services to our homepage in an appropriate context. However, framing or mirroring the Services or the Content is prohibited without the prior express written consent of Deno.
- RESTRICTIONS
The Services are available only for individuals aged 18 years or older. If you are under 18 years of age, then please do not access and/or use the Services. By entering into this Agreement, you represent and warrant that you are 18 years or older.
- FEEDBACK
We welcome and encourage you to provide feedback, comments, and suggestions for improvements to the Services and our services (Feedback). Although we encourage you to e-mail us, we do not want you to, and you should not, e-mail us any content that contains confidential information. With respect to any Feedback you provide, we shall be free to use and disclose any ideas, concepts, know-how, techniques, or other materials contained in your Feedback for any purpose whatsoever, including, but not limited to, the development, production and marketing of products and services that incorporate such information, without compensation or attribution to you.
- NO WARRANTIES; LIMITATION OF LIABILITY
THE SERVICES AND THE CONTENT ARE PROVIDED ON AN AS IS AND AS AVAILABLE BASIS, AND NEITHER DENO NOR DENOS SUPPLIERS MAKE ANY WARRANTIES WITH RESPECT TO THE SAME OR OTHERWISE IN CONNECTION WITH THIS AGREEMENT, AND DENO HEREBY DISCLAIMS ANY AND ALL EXPRESS, IMPLIED, OR STATUTORY WARRANTIES, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AVAILABILITY, ERROR-FREE OR UNINTERRUPTED OPERATION, AND ANY WARRANTIES ARISING FROM A COURSE OF DEALING, COURSE OF PERFORMANCE, OR USAGE OF TRADE. TO THE EXTENT THAT DENO AND DENOS SUPPLIERS MAY NOT AS A MATTER OF APPLICABLE LAW DISCLAIM ANY IMPLIED WARRANTY, THE SCOPE AND DURATION OF SUCH WARRANTY WILL BE THE MINIMUM PERMITTED UNDER SUCH LAW.
WITHOUT LIMITING THE FOREGOING, WE DO NOT WARRANT, GUARANTEE OR MAKE ANY REPRESENTATION, NOR SHALL WE BE RESPONSIBLE FOR (A) THE CORRECTNESS, ACCURACY, RELIABILITY, COMPLETENESS OR CURRENCY OF THE SERVICES; OR (B) ANY RESULTS ACHIEVED OR ACTION TAKEN BY YOU IN RELIANCE ON THE SERVICES OR THE CONTENT OR ALERTS PROVIDED THROUGH THE SERVICES. ANY DECISION, ACT OR OMISSION OF YOURS THAT IS BASED ON THE SERVICES OR THE CONTENT OR ALERTS PROVIDED THROUGH THE SERVICES IS AT YOUR OWN AND SOLE RISK. THE SERVICES AND THE CONTENT AND ALERTS PROVIDED THROUGH THE SERVICES IS PROVIDED AS A CONVENIENCE ONLY AND DOES NOT REPLACE THE NEED TO REVIEW ITS ACCURACY, COMPLETENESS AND CORRECTNESS.
IN CONNECTION WITH ANY WARRANTY, CONTRACT, OR COMMON LAW TORT CLAIMS: (I) WE SHALL NOT BE LIABLE FOR ANY INCIDENTAL OR CONSEQUENTIAL DAMAGES, LOST PROFITS, OR DAMAGES RESULTING FROM LOST DATA OR BUSINESS INTERRUPTION RESULTING FROM THE USE OR INABILITY TO ACCESS AND USE THE SERVICES, EVEN IF WE HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES; AND (II) ANY DIRECT DAMAGES THAT YOU MAY SUFFER AS A RESULT OF YOUR USE OF THE SERVICES, SHALL BE LIMITED TO THE GREATER OF (I) MONIES YOU HAVE PAID US IN CONNECTION WITH YOUR USE OF THE SERVICES DURING THE TWELVE (12) MONTHS IMMEDIATELY PRECEDING THE DATE THAT GAVE RISE TO THE CLAIM OR (II) ONE HUNDRED DOLLARS ($100).
- EXTERNAL SITES
The Services may contain links to third-party websites (External Sites). These links are provided solely as a convenience to you and not as an endorsement by us of the content on such External Sites. The content of such External Sites is developed and provided by others. You should contact the website administrator or webmaster for those External Sites if you have any concerns regarding such links or any content located on such External Sites. We are not responsible for the content of any linked External Sites and do not make any representations regarding the content or accuracy of materials on such External Sites. You should take precautions when downloading files from all websites to protect your computer from viruses and other destructive programs. If you decide to access linked External Sites, you do so at your own risk.
- REPRESENTATIONS AND WARRANTIES
You represent and warrant that you have: (i) all rights and permissions necessary to provide us with or grant us access to and use of User Data, and (ii) obtained all necessary and appropriate consents, permissions, and authorizations in accordance with all applicable laws and regulations with respect to User Data provided hereunder.
- INDEMNIFICATION
You will indemnify, defend, and hold Deno, its affiliates, and our and their respective shareholders, members, officers, directors, employees, agents, and representatives (collectively, Deno Indemnitees) harmless from and against any and all damages, liabilities, losses, costs, and expenses, including reasonable attorneys fees (collectively, Losses) incurred by any Deno Indemnitee in connection with a third-party claim, action, or proceeding (each, a Claim) arising from your (i) breach of this Agreement, including but not limited to, any breach of your representations and warranties; (ii) misuse of the Services, and/or the Content; (iii) negligence, gross negligence, willful misconduct, fraud, misrepresentation or violation of law; or (iv) violation of any third-party right, including without limitation any copyright, trademark, property, or privacy right; provided , however , that the foregoing obligations shall be subject to our: (i) promptly notifying you of the Claim; (ii) providing you, at your expense, with reasonable cooperation in the defense of the Claim; and (iii) providing you with sole control over the defense and negotiations for a settlement or compromise.
- COMPLIANCE WITH APPLICABLE LAWS
The Services are based in the United States. We make no claims concerning whether the Services may be viewed or be appropriate for use outside of the United States. If you access the Services from outside of the United States, you do so at your own risk. Whether inside or outside of the United States, you are solely responsible for ensuring compliance with the laws of your specific jurisdiction.
- TERM; TERMINATION
These Terms, and your right to access and use the Services, will commence upon your acceptance of these Terms and will continue for the period of your Subscription and/or use of the Services.
We reserve the right, in our sole discretion, to restrict, suspend, or terminate these Terms and your access to all or any part of the Services, at any time and for any reason without prior notice or liability. We reserve the right to change, suspend, or discontinue all or any part of the Services at any time without prior notice or liability. The Sections Description of the Services; Right to Use and Access the Service; Use of Personal Information, Intellectual Property, Feedback, No Warranties; Limitation of Liability, Indemnification, Compliance with Applicable Laws, Term; Termination, Binding Arbitration, Class Action Waiver, Equitable Relief, Controlling Law; Exclusive Forum, and Miscellaneous shall survive the termination of these Terms.
- BINDING ARBITRATION
In the event of a dispute arising under or relating to this Agreement, and/or the Services (each, a Dispute), such dispute will be finally and exclusively resolved by binding arbitration governed by the Federal Arbitration Act (FAA). NEITHER PARTY SHALL HAVE THE RIGHT TO LITIGATE SUCH CLAIM IN COURT OR TO HAVE A JURY TRIAL, EXCEPT EITHER PARTY MAY BRING ITS CLAIM IN ITS LOCAL SMALL CLAIMS COURT, IF PERMITTED BY THAT SMALL CLAIMS COURT RULES AND IF WITHIN SUCH COURTS JURISDICTION. ARBITRATION IS DIFFERENT FROM COURT, AND DISCOVERY AND APPEAL RIGHTS MAY ALSO BE LIMITED IN ARBITRATION. All disputes will be resolved before a neutral arbitrator selected jointly by the parties, whose decision will be final, except for a limited right of appeal under the FAA. The arbitration shall be commenced and conducted by JAMS pursuant to its then current Comprehensive Arbitration Rules and Procedures and in accordance with the Expedited Procedures in those rules, or, where appropriate, pursuant to JAMS Streamlined Arbitration Rules and Procedures. All applicable JAMS rules and procedures are available at the JAMS website www.jamsadr.com . Each party will be responsible for paying any JAMS filing, administrative, and arbitrator fees in accordance with JAMS rules. Judgment on the arbitrators award may be entered in any court having jurisdiction. This clause shall not preclude parties from seeking provisional remedies in aid of arbitration from a court of appropriate jurisdiction. The arbitration may be conducted in person, through the submission of documents, by phone, or online. If conducted in person, the arbitration shall take place in the United States county where you reside. The parties may litigate in court to compel arbitration, to stay a proceeding pending arbitration, or to confirm, modify, vacate, or enter judgment on the award entered by the arbitrator. The parties shall cooperate in good faith in the voluntary and informal exchange of all non-privileged documents and other information (including electronically stored information) relevant to the Dispute immediately after commencement of the arbitration. As set forth in Section 18 below, nothing in this Agreement will prevent us from seeking injunctive relief in any court of competent jurisdiction as necessary to protect our proprietary interests.
- CLASS ACTION WAIVER
You agree that any arbitration or proceeding shall be limited to the Dispute between us and you individually. To the full extent permitted by law, (i) no arbitration or proceeding shall be joined with any other; (ii) there is no right or authority for any Dispute to be arbitrated or resolved on a class action-basis or to utilize class action procedures; and (iii) there is no right or authority for any Dispute to be brought in a purported representative capacity on behalf of the general public or any other persons. YOU AGREE THAT YOU MAY BRING CLAIMS AGAINST US ONLY IN YOUR INDIVIDUAL CAPACITY AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS OR REPRESENTATIVE PROCEEDING.
- EQUITABLE RELIEF
You acknowledge and agree that in the event of a breach or threatened violation of our intellectual property rights and confidential and proprietary information by you, we will suffer irreparable harm and will therefore be entitled to injunctive relief to enforce this Agreement. We may, without waiving any other remedies under this Agreement, seek from any court having jurisdiction any interim, equitable, provisional, or injunctive relief that is necessary to protect our rights and property pending the outcome of the arbitration referenced above. You hereby irrevocably and unconditionally consent to the personal and subject matter jurisdiction of the federal and state courts in the State of New York for purposes of any such action by us.
- CONTROLLING LAW; EXCLUSIVE FORUM
The Agreement and any action related thereto will be governed by the laws of the State of New York without regard to its conflict of laws provisions. The parties hereby consent and agree to the exclusive jurisdiction of the state and federal courts located in the State of New York for all suits, actions, or proceedings directly or indirectly arising out of or relating to this Agreement, and waive any and all objections to such courts, including but not limited to, objections based on improper venue or inconvenient forum, and each party hereby irrevocably submits to the exclusive jurisdiction of such courts in any suits, actions, or proceedings arising out of or relating to this Agreement
- 
MISCELLANEOUS
Notwithstanding anything to the contrary set forth in these Terms, each party may during the term of this Agreement, use the other partys name and/or logo for marketing and promotional purposes, including, without limitation, identifying Authorized Users as a customer of Deno on Denos website or elsewhere. You may not assign any of your rights, duties, or obligations under these Terms to any person or entity, in whole or in part, without written consent from Deno. Our failure to act on or enforce any provision of the Agreement shall not be construed as a waiver of that provision or any other provision in this Agreement. No waiver shall be effective against us unless made in writing, and no such waiver shall be construed as a waiver in any other or subsequent instance. Except as expressly agreed by us and you in writing, the Agreement constitutes the entire agreement between you and us with respect to the subject matter, and supersedes all previous or contemporaneous agreements, whether written or oral, between the parties with respect to the subject matter. The section headings are provided merely for convenience and shall not be given any legal import. This Agreement will inure to the benefit of our successors, assigns, licensees, and sublicensees.
Copyright 2025 Deno Land Inc. All rights reserved.


---

URL: https://docs.deno.com/deploy/reference/cloud_connections/
TITLE: Cloud Connections

Cloud Connections
Deno Deploy allows you to connect to cloud providers like AWS and Google Cloud Platform (GCP) without needing to manually manage static credentials. This is done through the use of OpenID Connect (OIDC) and identity federation.
How it works Jump to heading #
Deno Deploy is an OIDC provider. Every running application of Deno Deploy can be issued short-lived JWT tokens that are signed by Deno Deploy. These tokens contain information about the application, such as the organization and application ids and slugs, the context in which an application is executing, and the running revision ID. Learn more about OIDC in Deno Deploy .
By sending these tokens to AWS or GCP, one can exchange them for short-lived AWS or GCP credentials that can be used to access cloud resources such as AWS S3 buckets or Google Cloud Spanner instances. When sending the token to AWS or GCP, the token is verified by the cloud provider, which checks that it was issued by Deno Deploy and that it is valid for the application and context that should be allowed to access the cloud resources.
To enable AWS or GCP to exchange OIDC tokens for credentials, the cloud provider needs to be configured to trust Deno Deploy as an OIDC identity provider, and an AWS IAM role or GCP service account needs to be created that allows the exchange of tokens for credentials, for a specific Deno Deploy application.
Setting up AWS Jump to heading #
This guide contains three guides for setting up these AWS resources. You can use any of these to set up the AWS resources.
- Using the deno deploy setup-aws command from your local machine (recommended)
- Using the aws CLI
- Using the AWS Console
- Using Terraform
To set up AWS with Deno Deploy, the following resources need to be created inside of your AWS account:
- An AWS IAM OIDC Identity Provider that trusts Deno Deploy as an OIDC provider.
- The OIDC provider URL is https://oidc.deno.com .
- The audience (client ID) is sts.amazonaws.com .
- An AWS IAM Role that can be "assumed" (signed into) using a Deno Deploy OIDC token.
- The trust policy of the role should allow the OIDC provider to assume the role, such as:
{ "Version" : "2012-10-17" , "Statement" : [ { "Effect" : "Allow" , "Principal" : { "Federated" : "arn:aws:iam::<account-id>:oidc-provider/oidc.deno.com" } , "Action" : "sts:AssumeRoleWithWebIdentity" , "Condition" : { "StringEquals" : { "oidc.deno.com:aud" : "sts.amazonaws.com" , "oidc.deno.com:sub" : "deployment:<organization-slug>/<application-slug>/<context-name>" } } } ] }
- The role should have permissions to access the AWS resources you want to use, such as S3 buckets or DynamoDB tables.
After setting up the AWS resources, navigate to the AWS cloud integration setup page from the app settings. There you must select the context(s) in which the cloud connection should be available.
Then you must enter the ARN (Amazon Resource Name) for the AWS IAM Role created earlier. After entering the ARN you can start a connection test by pressing the "Test connection" button. The connection test will check that the AWS IAM Role and OIDC provider are configured correctly, and does not allow access from apps, orgs, or contexts that should not have access.
After testing the connection, you can save the cloud connection.
Usage Jump to heading #
After setting up a cloud connection between AWS and Deno Deploy you can access AWS resources such as S3 directly from your application code, without having to configure any credentials.
The AWS SDK v3 automatically picks up on the cloud connection configuration. Here is an example of accessing an S3 bucket from a Deno Deploy application with a configured AWS account.
import { ListBucketsCommand , S3Client } from "@aws-sdk/client-s3" ; const s3 = new S3Client ( { region : "us-west-2" } ) ; Deno . serve ( ( ) => { const { Buckets } = await s3 . send ( new ListBucketsCommand ( { } ) ) ; return Response . json ( Buckets ) ; } ) ;
Setting up GCP Jump to heading #
To set up GCP with Deno Deploy, the following resources need to be created inside of your GCP account:
- A Workload Identity Pool and Workload Identity Provider that trusts Deno Deploy as an OIDC provider.
- The OIDC provider URL is https://oidc.deno.com .
- The audience should be the default (starts with https://iam.googleapis.com ).
- At least the following attribute mappings must be set:
- google.subject = assertion.sub
- attribute.full_slug = assertion.org_slug + "/" + assertion.app_slug
- A Service account that can be "impersonated" (signed into) using the OIDC token.
- A principal or principal set from the workload identity pool should have access to the service account using the Workload Identity User role ( roles/iam.workloadIdentityUser ). Examples:
- A specific context in an app: principal://iam.googleapis.com/projects/<PROJECT_NUMBER>/locations/global/workloadIdentityPools/oidc-deno-com/subject/deployment:<ORG_SLUG>/<APP_SLUG>/<CONTEXT_NAME>
- All contexts in an app: principalSet://iam.googleapis.com/projects/<PROJECT_NUMBER>/locations/global/workloadIdentityPools/oidc-deno-com/attribute.full_slug/<ORG_SLUG>/<APP_SLUG>
- The service account should have access to the GCP resources you want to use, such as a Google Cloud Storage bucket.
This guide contains three guides for setting up these GCP resources. You can use any of these to set up the GCP resources.
- Using the deno deploy setup-gcp command from your local machine (recommended)
- Using the gcloud CLI
- Using the GCP Console
- Using Terraform
After setting up the GCP resources, navigate to the GCP cloud integration setup page from the app settings. There you must select the context(s) in which the cloud connection should be available.
Then you must enter the workload identity provider ID, in the form projects/<PROJECT_NUMBER>/locations/global/workloadIdentityPools/oidc-deno-com/providers/oidc-deno-com , and the email address of the GCP Service Account created earlier. After entering the email address you can start a connection test by pressing the "Test connection" button. The connection test will check that the GCP Service Account and OIDC provider are configured correctly, and does not allow access from apps, orgs, or contexts that should not have access.
After testing the connection, you can save the cloud connection.
Usage Jump to heading #
After setting up a cloud connection between GCP and Deno Deploy you can access GCP resources such as Cloud Storage directly from your application code, without having to configure any credentials.
The Google Cloud SDK automatically picks up on the cloud connection configuration. Here is an example of accessing a Cloud Storage bucket from a Deno Deploy application with a configured GCP account.
import { Storage } from "@google-cloud/storage" ; const storage = new Storage ( ) ; Deno . serve ( ( ) => { const [ buckets ] = await storage . getBuckets ( ) ; return Response . json ( buckets ) ; } ) ;
Removing a cloud integration Jump to heading #
You can remove a cloud connection by pressing the "Delete" button in the cloud integration section, next to a specific cloud connection.
Setup Guides Jump to heading #
AWS: Easy setup with deno deploy setup-aws Jump to heading #
For instructions on how to set up AWS with Deno Deploy using the deno deploy setup-aws command, please see the instructions on the AWS cloud integration setup page in your app settings.
AWS: Using the aws CLI Jump to heading #
You can manually set up AWS resources using the AWS CLI. This requires having the AWS CLI installed and configured with appropriate permissions to create IAM roles, OIDC providers, and attach policies.
Prerequisites Jump to heading #
- AWS CLI installed and configured
- Permissions to create IAM roles, OIDC providers, and attach policies
Step 1: Create OIDC Provider Jump to heading #
First, create the OIDC provider if it doesn't already exist:
aws iam create-open-id-connect-provider \ --url https://oidc.deno.com \ --client-id-list sts.amazonaws.com
Step 2: Create IAM Role with Trust Policy Jump to heading #
Create a trust policy file that allows your Deno Deploy application to assume the role. You can choose between allowing access to all contexts or specific contexts only.
For all contexts in your app:
# Create trust policy file for entire app cat > trust-policy-all-contexts.json << EOF { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "Federated": "arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/oidc.deno.com" }, "Action": "sts:AssumeRoleWithWebIdentity", "Condition": { "StringLike": { "oidc.deno.com:sub": "deployment:YOUR_ORG/YOUR_APP/*" } } } ] } EOF
For specific contexts only:
# Create trust policy file for specific contexts cat > trust-policy-specific-contexts.json << EOF { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "Federated": "arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/oidc.deno.com" }, "Action": "sts:AssumeRoleWithWebIdentity", "Condition": { "StringEquals": { "oidc.deno.com:sub": [ "deployment:YOUR_ORG/YOUR_APP/production", "deployment:YOUR_ORG/YOUR_APP/staging" ] } } } ] } EOF
Step 3: Create the IAM Role Jump to heading #
Create the role using the appropriate trust policy:
# For entire app aws iam create-role \ --role-name DenoDeploy-YourOrg-YourApp \ --assume-role-policy-document file://trust-policy-all-contexts.json # OR for specific contexts aws iam create-role \ --role-name DenoDeploy-YourOrg-YourApp \ --assume-role-policy-document file://trust-policy-specific-contexts.json
Step 4: Attach Policies Jump to heading #
Attach the necessary policies to grant permissions for the AWS resources your application needs:
aws iam attach-role-policy \ --role-name DenoDeploy-YourOrg-YourApp \ --policy-arn arn:aws:iam::aws:policy/POLICY_NAME
Replace POLICY_NAME with the appropriate AWS policies (e.g., AmazonS3ReadOnlyAccess , AmazonDynamoDBReadOnlyAccess , etc.) based on your requirements.
After completing these steps, use the Role ARN in your Deno Deploy cloud connection configuration.
AWS: Using the AWS Console Jump to heading #
You can set up AWS resources using the AWS Management Console web interface. This method provides a visual way to configure the necessary IAM resources.
Step 1: Create OIDC Identity Provider Jump to heading #
- Navigate to IAM Console  Identity providers
- Create OIDC Provider :
- Click "Add provider"
- Select "OpenID Connect"
- Provider URL: https://oidc.deno.com
- Audience: sts.amazonaws.com
- Click "Add provider"
Step 2: Create IAM Role Jump to heading #
- Navigate to IAM Console  Roles
- Create role :
- Click "Create role"
- Trusted entity type: Web identity
- Identity provider: Select the created OIDC provider ( oidc.deno.com )
- Audience: sts.amazonaws.com
Step 3: Configure Trust Policy Conditions Jump to heading #
Add a condition to restrict which Deno Deploy applications can assume this role. Choose one approach:
For all contexts in your app:
- Condition key: oidc.deno.com:sub
- Operator: StringLike
- Value: deployment:YOUR_ORG/YOUR_APP/*
For specific contexts only:
- Condition key: oidc.deno.com:sub
- Operator: StringEquals
- Value: deployment:YOUR_ORG/YOUR_APP/production
- Add additional conditions for each context (e.g., staging, development)
Click "Next" to continue.
Step 4: Attach Permissions Policies Jump to heading #
- Search and select appropriate policies based on your needs:
- For S3 access: AmazonS3ReadOnlyAccess or AmazonS3FullAccess
- For DynamoDB access: AmazonDynamoDBReadOnlyAccess or AmazonDynamoDBFullAccess
- For other services: Select relevant policies
- Click "Next"
Step 5: Name and Create Role Jump to heading #
- Role name : DenoDeploy-YourOrg-YourApp (replace with your actual organization and app names)
- Description : Optional description of the role's purpose
- Review the trust policy and permissions
- Click "Create role"
Step 6: Copy Role ARN Jump to heading #
After creating the role:
- Go to the role details page
- Copy the Role ARN (it looks like arn:aws:iam::123456789012:role/DenoDeploy-YourOrg-YourApp )
- Use this ARN in your Deno Deploy cloud connection configuration
AWS: Using Terraform Jump to heading #
You can use Terraform to programmatically create the AWS resources needed for cloud connections. This approach is ideal for infrastructure-as-code workflows.
Terraform Configuration Jump to heading #
Create a Terraform configuration file with the following content:
# Variables variable "org" { description = "Deno Deploy organization name" type = string } variable "app" { description = "Deno Deploy app name" type = string } variable "contexts" { description = "List of specific contexts to allow (leave empty for all contexts)" type = list(string) default = [] } # OIDC Provider resource "aws_iam_openid_connect_provider" "deno_deploy" { url = "https://oidc.deno.com" client_id_list = ["sts.amazonaws.com"] } # IAM Role with dynamic trust policy based on contexts resource "aws_iam_role" "deno_deploy_role" { name = "DenoDeploy-${var.org}-${var.app}" assume_role_policy = jsonencode({ Version = "2012-10-17" Statement = [ { Effect = "Allow" Principal = { Federated = aws_iam_openid_connect_provider.deno_deploy.arn } Action = "sts:AssumeRoleWithWebIdentity" Condition = length(var.contexts) > 0 ? { # Specific contexts only StringEquals = { "oidc.deno.com:sub" = [ for context in var.contexts : "deployment:${var.org}/${var.app}/${context}" ] } } : { # All contexts (wildcard) StringLike = { "oidc.deno.com:sub" = "deployment:${var.org}/${var.app}/*" } } } ] }) } # Attach policies resource "aws_iam_role_policy_attachment" "example" { role = aws_iam_role.deno_deploy_role.name policy_arn = "arn:aws:iam::aws:policy/POLICY_NAME" } # Output the role ARN output "role_arn" { value = aws_iam_role.deno_deploy_role.arn }
Usage Examples Jump to heading #
For entire app access (all contexts):
module "deno_deploy_aws" { source = "./path-to-terraform-module" org = "your-org" app = "your-app" contexts = [] # Empty list allows all contexts }
For specific contexts only:
module "deno_deploy_aws" { source = "./path-to-terraform-module" org = "your-org" app = "your-app" contexts = ["production", "staging"] }
Applying the Configuration Jump to heading #
- 
Initialize Terraform:
terraform init
- 
Plan the deployment:
terraform plan
- 
Apply the configuration:
terraform apply
After applying, Terraform will output the Role ARN that you can use in your Deno Deploy cloud connection configuration.
Customizing Policies Jump to heading #
Replace POLICY_NAME in the aws_iam_role_policy_attachment resource with the appropriate AWS managed policies or create custom policies based on your requirements. You can add multiple policy attachments by creating additional aws_iam_role_policy_attachment resources.
GCP: Easy setup with deno deploy setup-gcp Jump to heading #
For instructions on how to set up GCP with Deno Deploy using the deno deploy setup-gcp command, please see the instructions on the Google cloud integration setup page in your app settings.
GCP: Using the gcloud CLI Jump to heading #
You can manually set up GCP resources using the gcloud CLI. This requires having the gcloud CLI installed and authenticated with appropriate permissions to create workload identity pools, service accounts, and grant IAM roles.
Prerequisites Jump to heading #
- gcloud CLI installed and authenticated
- Access to create workload identity pools, service accounts, and grant IAM roles
- Required APIs enabled:
- iam.googleapis.com
- iamcredentials.googleapis.com
- sts.googleapis.com
Step 1: Enable Required APIs Jump to heading #
First, enable the required APIs for your project:
gcloud services enable iam.googleapis.com gcloud services enable iamcredentials.googleapis.com gcloud services enable sts.googleapis.com
Step 2: Create Workload Identity Pool Jump to heading #
Create a workload identity pool to manage external identities:
gcloud iam workload-identity-pools create oidc-deno-com \ --location = global \ --display-name = "Deno Deploy Workload Identity Pool"
Step 3: Create Workload Identity Provider Jump to heading #
Configure the OIDC provider within the workload identity pool:
gcloud iam workload-identity-pools providers create-oidc oidc-deno-com \ --workload-identity-pool = oidc-deno-com \ --location = global \ --issuer-uri = https://oidc.deno.com \ --attribute-mapping = "google.subject=assertion.sub,attribute.org_slug=assertion.org_slug,attribute.app_slug=assertion.app_slug,attribute.full_slug=assertion.org_slug+ \" / \" +assertion.app_slug"
Step 4: Create Service Account Jump to heading #
Create a service account that will be used by your Deno Deploy application:
gcloud iam service-accounts create deno-your-org-your-app \ --display-name = "Deno Deploy YourOrg/YourApp"
Step 5: Configure Workload Identity Binding Jump to heading #
Get your project number and configure the workload identity binding. Choose between allowing access to all contexts or specific contexts only.
# Get project number PROJECT_NUMBER = $( gcloud projects describe PROJECT_ID --format = "value(projectNumber)" )
For all contexts in your app:
gcloud iam service-accounts add-iam-policy-binding \ deno-your-org-your-app@PROJECT_ID.iam.gserviceaccount.com \ --role = roles/iam.workloadIdentityUser \ --member = "principalSet://iam.googleapis.com/projects/ $PROJECT_NUMBER /locations/global/workloadIdentityPools/oidc-deno-com/attribute.full_slug/YOUR_ORG/YOUR_APP"
For specific contexts only:
# Bind for production context gcloud iam service-accounts add-iam-policy-binding \ deno-your-org-your-app@PROJECT_ID.iam.gserviceaccount.com \ --role = roles/iam.workloadIdentityUser \ --member = "principal://iam.googleapis.com/projects/ $PROJECT_NUMBER /locations/global/workloadIdentityPools/oidc-deno-com/subject/deployment:YOUR_ORG/YOUR_APP/production" # Bind for staging context gcloud iam service-accounts add-iam-policy-binding \ deno-your-org-your-app@PROJECT_ID.iam.gserviceaccount.com \ --role = roles/iam.workloadIdentityUser \ --member = "principal://iam.googleapis.com/projects/ $PROJECT_NUMBER /locations/global/workloadIdentityPools/oidc-deno-com/subject/deployment:YOUR_ORG/YOUR_APP/staging" # Add more bindings for each specific context as needed
Step 6: Grant Roles to Service Account Jump to heading #
Grant the necessary roles to the service account for accessing GCP resources:
gcloud projects add-iam-policy-binding PROJECT_ID \ --member = "serviceAccount:deno-your-org-your-app@PROJECT_ID.iam.gserviceaccount.com" \ --role = "roles/ROLE_NAME"
Replace ROLE_NAME with appropriate roles such as:
- roles/storage.objectViewer for Cloud Storage read access
- roles/storage.objectAdmin for Cloud Storage full access
- roles/cloudsql.client for Cloud SQL access
- Other roles based on your requirements
Step 7: Get Required Values Jump to heading #
After completing the setup, you'll need two values for your Deno Deploy configuration:
- Workload Provider ID : projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/oidc-deno-com/providers/oidc-deno-com
- Service Account Email : deno-your-org-your-app@PROJECT_ID.iam.gserviceaccount.com
Use these values in your Deno Deploy cloud connection configuration.
GCP: Using the GCP Console Jump to heading #
You can set up GCP resources using the Google Cloud Console web interface. This method provides a visual way to configure workload identity federation and service accounts.
Step 1: Enable Required APIs Jump to heading #
- Navigate to APIs & Services  Library
- Search for and enable the following APIs:
- "Identity and Access Management (IAM) API"
- "IAM Service Account Credentials API"
- "Security Token Service API"
Step 2: Create Workload Identity Pool Jump to heading #
- Navigate to IAM & Admin  Workload Identity Federation
- Create Pool :
- Click "Create Pool"
- Pool name: Deno Deploy Workload Id Pool
- Pool ID: oidc-deno-com
- Click "Continue"
Step 3: Add Provider to Pool Jump to heading #
- 
Add a provider :
- Click "Add a provider"
- Provider type: OpenID Connect (OIDC)
- Provider name: Deno Deploy OIDC Provider
- Provider ID: oidc-deno-com
- Issuer URL: https://oidc.deno.com
- 
Configure attribute mappings :
- google.subject  assertion.sub
- attribute.org_slug  assertion.org_slug
- attribute.app_slug  assertion.app_slug
- attribute.full_slug  assertion.org_slug + "/" + assertion.app_slug
- 
Click "Save"
Step 4: Create Service Account Jump to heading #
- Navigate to IAM & Admin  Service Accounts
- Create Service Account :
- Click "Create Service Account"
- Service account name: deno-your-org-your-app
- Service account ID: deno-your-org-your-app
- Description: Service account for Deno Deploy project your-org/your-app
- Click "Create and Continue"
Step 5: Grant Roles to Service Account Jump to heading #
- Select appropriate roles based on your needs:
- For Cloud Storage: Storage Object Viewer or Storage Admin
- For Cloud SQL: Cloud SQL Client
- For other services: Select relevant roles
- Click "Continue" then "Done"
Step 6: Configure Workload Identity Binding Jump to heading #
- 
Go back to the created service account
- 
Click on the "Principals with access" tab
- 
Click "Grant Access"
- 
Configure principals - choose one approach:
For all contexts in your app:
- New principals: principalSet://iam.googleapis.com/projects/YOUR_PROJECT_NUMBER/locations/global/workloadIdentityPools/oidc-deno-com/attribute.full_slug/YOUR_ORG/YOUR_APP
For specific contexts only:
- New principals: principal://iam.googleapis.com/projects/YOUR_PROJECT_NUMBER/locations/global/workloadIdentityPools/oidc-deno-com/subject/deployment:YOUR_ORG/YOUR_APP/production
- Repeat for each context (staging, etc.)
- 
Role: Workload Identity User
- 
Click "Save"
Step 7: Get Required Values Jump to heading #
You'll need two values for your Deno Deploy configuration:
- Workload Provider ID :
- Navigate back to Workload Identity Federation
- Click on your pool, then your provider
- Copy the provider resource name (full path starting with projects/ )
- Service Account Email : Copy from the service account details page
Step 8: Verify Configuration Jump to heading #
The final workload identity pool overview should show:
- Your pool with the OIDC provider
- The connected service account
- Proper bindings configured
Use the Service Account Email and Workload Provider ID in your Deno Deploy cloud connection configuration.
GCP: Using Terraform Jump to heading #
You can use Terraform to programmatically create the GCP resources needed for cloud connections. This approach is ideal for infrastructure-as-code workflows.
Terraform Configuration Jump to heading #
Create a Terraform configuration file with the following content:
# Variables variable "org" { description = "Deno Deploy organization name" type = string } variable "app" { description = "Deno Deploy app name" type = string } variable "contexts" { description = "List of specific contexts to allow (leave empty for all contexts)" type = list(string) default = [] } variable "project_id" { description = "GCP Project ID" type = string } variable "roles" { description = "List of IAM roles to grant to the service account" type = list(string) default = [] } # Data source for project information data "google_project" "project" { project_id = var.project_id } # Workload Identity Pool resource "google_iam_workload_identity_pool" "deno_deploy" { workload_identity_pool_id = "oidc-deno-com" display_name = "Deno Deploy Workload Id Pool" } # Workload Identity Provider resource "google_iam_workload_identity_pool_provider" "deno_deploy" { workload_identity_pool_id = google_iam_workload_identity_pool.deno_deploy.workload_identity_pool_id workload_identity_pool_provider_id = "oidc-deno-com" display_name = "Deno Deploy OIDC Provider" attribute_mapping = { "google.subject" = "assertion.sub" "attribute.org_slug" = "assertion.org_slug" "attribute.app_slug" = "assertion.app_slug" "attribute.full_slug" = "assertion.org_slug + \"/\" + assertion.app_slug" } oidc { issuer_uri = "https://oidc.deno.com" } } # Service Account resource "google_service_account" "deno_deploy" { account_id = "deno-${var.org}-${var.app}" display_name = "Deno Deploy ${var.org}/${var.app}" } # Workload Identity Binding - dynamic based on contexts resource "google_service_account_iam_binding" "workload_identity" { service_account_id = google_service_account.deno_deploy.name role = "roles/iam.workloadIdentityUser" members = length(var.contexts) > 0 ? [ # Specific contexts only for context in var.contexts : "principal://iam.googleapis.com/projects/${data.google_project.project.number}/locations/global/workloadIdentityPools/${google_iam_workload_identity_pool.deno_deploy.workload_identity_pool_id}/subject/deployment:${var.org}/${var.app}/${context}" ] : [ # All contexts (using attribute mapping) "principalSet://iam.googleapis.com/projects/${data.google_project.project.number}/locations/global/workloadIdentityPools/${google_iam_workload_identity_pool.deno_deploy.workload_identity_pool_id}/attribute.full_slug/${var.org}/${var.app}" ] } # Grant roles to service account resource "google_project_iam_member" "service_account_roles" { for_each = toset(var.roles) project = var.project_id role = each.value member = "serviceAccount:${google_service_account.deno_deploy.email}" } # Outputs output "workload_provider_id" { value = "projects/${data.google_project.project.number}/locations/global/workloadIdentityPools/${google_iam_workload_identity_pool.deno_deploy.workload_identity_pool_id}/providers/${google_iam_workload_identity_pool_provider.deno_deploy.workload_identity_pool_provider_id}" } output "service_account_email" { value = google_service_account.deno_deploy.email }
Usage Examples Jump to heading #
For entire app access (all contexts):
module "deno_deploy_gcp" { source = "./path-to-terraform-module" org = "your-org" app = "your-app" project_id = "your-gcp-project-id" contexts = [] # Empty list allows all contexts roles = [ "roles/storage.objectViewer", "roles/cloudsql.client" ] }
For specific contexts only:
module "deno_deploy_gcp" { source = "./path-to-terraform-module" org = "your-org" app = "your-app" project_id = "your-gcp-project-id" contexts = ["production", "staging"] roles = [ "roles/storage.objectAdmin", "roles/cloudsql.client" ] }
Applying the Configuration Jump to heading #
- 
Initialize Terraform:
terraform init
- 
Plan the deployment:
terraform plan
- 
Apply the configuration:
terraform apply
After applying, Terraform will output the Workload Provider ID and Service Account Email that you can use in your Deno Deploy cloud connection configuration.
Customizing Roles Jump to heading #
The roles variable accepts a list of GCP IAM roles. Common roles include:
- roles/storage.objectViewer - Read access to Cloud Storage
- roles/storage.objectAdmin - Full access to Cloud Storage objects
- roles/cloudsql.client - Access to Cloud SQL instances
- roles/secretmanager.secretAccessor - Access to Secret Manager secrets
- Custom roles can also be specified


---

URL: https://docs.deno.com/deploy/reference/domains/
TITLE: Domains

Domains
Every organization has a default domain used for all applications deployed within that organization. For example, an organization with the slug acme-inc would have a default domain of acme-inc.deno.net . An application named my-app would automatically receive the production domain my-app.acme-inc.deno.net .
In addition to these default domains, you can add custom domains to your applications. Custom domains are domains that you own and control. To use a custom domain, you must:
- Own the domain (purchased from a domain registrar)
- Have access to edit its DNS records
Custom domains belong to an organization and can be attached to any application within that organization.
A custom domain can be added as:
- A base domain (e.g., example.com or a specific subdomain)
- A wildcard domain (e.g., *.example.com )
A base domain works with a single application, while a wildcard domain offers more flexibility. You can either:
- Assign the entire wildcard to one application (all subdomains point to the same app)
- Partially assign it to multiple applications (different subdomains point to different apps)
All custom domains require valid TLS certificates. Deno Deploy can automatically provision these certificates using Let's Encrypt . Alternatively, you can bring your own TLS certificates, which you will then need to renew manually.
Adding a custom domain Jump to heading #
- Go to the organization domains page (click your organization name in the top left corner, then the "Domains" tab)
- Click "Add Domain"
- Enter your domain (e.g., example.com )
- Select whether to add just this domain or also include the wildcard subdomain
- Click "Add Domain"
This will open the domain configuration drawer.
DNS configuration Jump to heading #
The domain configuration drawer shows the DNS records needed to:
- Verify domain ownership
- Optionally provision TLS certificates
- Route traffic to Deno Deploy
There are three possible configuration methods, depending on your domain registrar's capabilities:
ANAME/ALIAS method (preferred) Jump to heading #
If your registrar supports ANAME or ALIAS records, this is the best option:
- Add one ANAME / ALIAS record
- Add one CNAME record for verification
CNAME method Jump to heading #
Works well for subdomains but not for apex domains:
- Add two CNAME records
- Note: This method doesn't allow other DNS records (like MX records) on the same domain
A record method Jump to heading #
Most compatible but requires more configuration:
- Add one A record
- Add one CNAME record for verification
Note: Deno Deploy does not currently support IPv6. When using the ANAME/ALIAS or CNAME methods, your domain will automatically use IPv6 when supported. With the A method, you'll receive an email when it's time to add an AAAA record.
Caution
When using Cloudflare as your DNS provider, you MUST disable the proxying feature (orange cloud) for the _acme-challenge CNAME record, or verification and certificate provisioning will fail.
Verification Jump to heading #
After adding the DNS records, Deno Deploy will verify your domain ownership. This process may take a few minutes depending on your DNS provider. You can leave the domain configuration drawer open during verification  it will refresh automatically when complete.
You can manually trigger verification by clicking the "Provision Certificate" button. Successful verification also initiates TLS certificate provisioning.
TLS certificates Jump to heading #
After domain verification, you need a valid TLS certificate to use the domain with Deno Deploy. You can either have Deno Deploy provision a certificate for you using Let's Encrypt, or you can bring your own certificate.
Automatic provisioning (Let's Encrypt) Jump to heading #
After domain verification, click "Provision Certificate" to generate a TLS certificate through Let's Encrypt. This process can take up to 90 seconds.
Once provisioned, you'll see certificate details including expiration date and issue time.
Certificates are automatically renewed near expiration. You can check the current certificate status in the domain configuration drawer.
If automatic renewal fails (for example, because DNS records changed), you will receive an email notification 14 days before the certificate expires. You then have a chance to fix the issue and contact support to retry the renewal. If the certificate is not renewed before expiration, the domain will stop working.
Bring your own certificate Jump to heading #
If you prefer to use your own TLS certificate, you can upload it in the domain configuration drawer. You'll need to provide the following:
- The certificate file (PEM format)
- The private key file (PEM format)
Once uploaded, the certificate will be used for the domain. You are responsible for renewing and updating the certificate before it expires.
You will receive email notifications 14 days before the certificate expires reminding you to update it. If the certificate expires, the domain will stop working.
The TLS certificate must be valid at the time of upload. It must cover the base domain (and, if you have a wildcard domain, the wildcard subdomain as well) through either the common name or the subject alternative names in the certificate. The private key and certificate must match, and must be either RSA (2048, 3072, or 4096 bits) or ECDSA (P-256, P-384, or P-521).
Assigning a custom domain to an application Jump to heading #
After adding a custom domain to your organization:
- Go to the organization domains page
- Click "Assign" next to the custom domain
- Select the target application
- If using a wildcard domain, choose whether to attach the base domain, the wildcard, or a specific subdomain
- Click "Assign Domain"
Unassigning a custom domain from an application Jump to heading #
- Go to the application settings page
- Find the "Custom Domains" section
- Click "Remove" next to the domain you want to unassign
This removes the domain from the application but keeps it available in your organization for use with other applications.
Removing a custom domain Jump to heading #
- Go to the organization domains page
- Open the domain configuration drawer
- Click "Delete" and confirm
This removes the custom domain from your organization and deletes all domain assignments across all applications.
Migrating a custom domain from Deploy Classic to Deno Deploy Jump to heading #
If you have previously set up a custom domain on Deploy Classic and want to migrate it to Deno Deploy, we've created a step-by-step tutorial to guide you through the process.


---

URL: https://docs.deno.com/deploy/kv/secondary_indexes/
TITLE: Secondary Indexes

Secondary Indexes
Key-value stores like Deno KV organize data as collections of key-value pairs, where each unique key is associated with a single value. This structure enables easy retrieval of values based on their keys but does not allow for querying based on the values themselves. To overcome this constraint, you can create secondary indexes, which store an additional key that lets you look up related data by an alternate attribute (for example, email  user). A best practice is to store a pointer to the primary key in the secondary index, rather than duplicating the full value.
Recommended approach for Pointer indexes
Prefer storing the primary key (or a compact reference to it) as the value in a secondary index. This reduces storage usage and avoids keeping multiple copies of the same data in sync. The tradeoff is a double read when querying through the index (index  primary).
Pros
- Lower storage and write amplification
- Fewer updates when nonindexed fields change
- Clearer transactional updates: update primary + index together
Cons
- Requires a second read to resolve the primary value
- You must maintain referential integrity atomically (create/update/delete in a single transaction)
Maintaining consistency between primary and secondary keys is crucial when using secondary indexes. If a value is updated at the primary key without updating the secondary key, the data returned from a query targeting the secondary key will be incorrect. To ensure that primary and secondary keys always represent the same data, use atomic operations when inserting, updating, or deleting data. This approach ensures that the group of mutation actions are executed as a single unit, and either all succeed or all fail, preventing inconsistencies.
Unique indexes (one-to-one) Jump to heading #
Unique indexes have each key in the index associated with exactly one primary key. For example, when storing user data and looking up users by both their unique IDs and email addresses, store user data under two separate keys: one for the primary key (user ID) and another for the secondary index (email  user ID). This setup allows querying users based on either their ID or their email. The secondary index can also enforce uniqueness constraints on values in the store. In the case of user data, use the index to ensure that each email address is associated with only one user.
To implement a unique secondary index for this example, follow these steps:
- 
Create a User interface representing the data:
interface User { id : string ; name : string ; email : string ; }
- 
Define an insertUser function that stores user data at the primary key and stores a pointer (the primary key) at the secondary key:
async function insertUser ( user : User ) { const primaryKey = [ "users" , user . id ] as const ; const byEmailKey = [ "users_by_email" , user . email . toLowerCase ( ) ] as const ; const res = await kv . atomic ( ) . check ( { key : primaryKey , versionstamp : null } ) . check ( { key : byEmailKey , versionstamp : null } ) . set ( primaryKey , user ) // store pointer, not full user . set ( byEmailKey , user . id ) . commit ( ) ; if ( ! res . ok ) { throw new TypeError ( "User with ID or email already exists" ) ; } }
This function performs the insert using an atomic operation that checks that no user with the same ID or email already exists. If either of these constraints is violated, the insert fails and no data is modified.
- 
Define a getUser function to retrieve a user by their ID:
async function getUser ( id : string ) : Promise < User | null > { const res = await kv . get < User > ( [ "users" , id ] ) ; return res . value ; }
- 
Define a getUserByEmail function to retrieve a user by their email address using a double lookup (email  user ID  user):
async function getUserByEmail ( email : string ) : Promise < User | null > { const idRes = await kv . get < string > ( [ "users_by_email" , email . toLowerCase ( ) , ] ) ; if ( ! idRes . value ) return null ; const res = await kv . get < User > ( [ "users" , idRes . value ] ) ; return res . value ; }
This function queries the store using the secondary key ( ["users_by_email", email] ).
- 
Define a deleteUser function to delete users by their ID, removing the index entry too:
async function deleteUser ( id : string ) { let res = { ok : false } as { ok : boolean } ; while ( ! res . ok ) { const cur = await kv . get < User > ( [ "users" , id ] ) ; if ( cur . value === null ) return ; res = await kv . atomic ( ) . check ( cur ) . delete ( [ "users" , id ] ) . delete ( [ "users_by_email" , cur . value . email . toLowerCase ( ) ] ) . commit ( ) ; } }
This function first retrieves the user by their ID to get the users email address. This is needed to retrieve the email that is needed to construct the key for the secondary index for this user address. It then performs an atomic operation that checks that the user in the database has not changed, and then deletes both the primary and secondary key pointing to the user value. If this fails (the user has been modified between query and delete), the atomic operation aborts. The entire procedure is retried until the delete succeeds. The check is required to prevent race conditions where value may have been modified between the retrieve and delete. This race can occur if an update changes the user's email, because the secondary index moves in this case. The delete of the secondary index then fails, because the delete is targeting the old secondary index key.
Non-Unique Indexes (One-to-Many) Jump to heading #
Non-unique indexes are secondary indexes where a single key can be associated with multiple primary keys, allowing you to query for multiple items based on a shared attribute. For example, when querying users by their favorite color, implement this using a non-unique secondary index. The favorite color is a non-unique attribute since multiple users can have the same favorite color.
To implement a non-unique secondary index for this example, follow these steps:
- 
Define the User interface:
interface User { id : string ; name : string ; favoriteColor : string ; }
- 
Define the insertUser function (store the primary key as the value in the nonunique index; note the composite key includes the user ID to avoid collisions):
async function insertUser ( user : User ) { const primaryKey = [ "users" , user . id ] as const ; const byColorKey = [ "users_by_favorite_color" , user . favoriteColor , user . id , ] as const ; await kv . atomic ( ) . check ( { key : primaryKey , versionstamp : null } ) . set ( primaryKey , user ) // store pointer, not full user . set ( byColorKey , user . id ) . commit ( ) ; }
- 
Define a function to retrieve users by their favorite color. This performs a double lookup per result (index  primary):
async function getUsersByFavoriteColor ( color : string ) : Promise < User [ ] > { const iter = kv . list < string > ( { prefix : [ "users_by_favorite_color" , color ] , } ) ; const ids : string [ ] = [ ] ; for await ( const { value : id } of iter ) { ids . push ( id ) ; } if ( ids . length === 0 ) return [ ] ; const results = await kv . getMany < User > ( ids . map ( ( id ) => [ "users" , id ] as const ) , ) ; return results . map ( ( r ) => r . value ! ) . filter ( Boolean ) ; }
This example demonstrates the use of a non-unique secondary index, users_by_favorite_color , which allows querying users based on their favorite color. The index stores pointers (user IDs) and requires resolving to the primary key to read full values.
The primary difference between unique and nonunique indexes lies in the structure and organization of secondary keys. In unique indexes, each secondary key is associated with exactly one primary key, ensuring that the indexed attribute is unique across all records. In nonunique indexes, a single secondary key can be associated with multiple primary keys, as the indexed attribute may be shared among multiple records. To achieve this, nonunique secondary keys are typically structured with an additional unique identifier (e.g., primary key) as part of the key, allowing multiple records with the same attribute to coexist without conflicts.
When duplicating values may be acceptable Jump to heading #
While pointer indexes are recommended, duplicating the full value in a secondary index can be acceptable when:
- The value is small and reads occur almost exclusively via the secondary index
- You want to avoid a second read and can tolerate the extra storage
- You can reliably keep the primary and secondary in sync via atomic transactions
If duplicating, ensure inserts/updates/deletes modify both keys in the same atomic transaction.
Migration from duplicated-value indexes Jump to heading #
To migrate existing duplicated-value indexes to pointer indexes:
- Backfill: scan primary keys and set secondary index values to the primary key (e.g., user ID).
- Cutover: update write paths to maintain pointer indexes; keep the old index temporarily for reads.
- Cleanup: switch readers to the pointer index, then remove the duplicated index entries.


---

URL: https://docs.deno.com/deploy/kv/transactions/
TITLE: Transactions

Transactions
The Deno KV store utilizes optimistic concurrency control transactions rather than interactive transactions like many SQL systems like PostgreSQL or MySQL. This approach employs versionstamps, which represent the current version of a value for a given key, to manage concurrent access to shared resources without using locks. When a read operation occurs, the system returns a versionstamp for the associated key in addition to the value.
To execute a transaction, one performs an atomic operations that can consist of multiple mutation actions (like set or delete). Along with these actions, key+versionstamp pairs are provided as a condition for the transaction's success. The optimistic concurrency control transaction will only commit if the specified versionstamps match the current version for the values in the database for the corresponding keys. This transaction model ensures data consistency and integrity while allowing concurrent interactions within the Deno KV store.
Because OCC transactions are optimistic, they can fail on commit because the version constraints specified in the atomic operation were violated. This occurs when an agent updates a key used within the transaction between read and commit. When this happens, the agent performing the transaction must retry the transaction.
To illustrate how to use OCC transactions with Deno KV, this example shows how to implement a transferFunds(from: string, to: string, amount: number) function for an account ledger. The account ledger stores the balance for each account in the key-value store. The keys are prefixed by "account" , followed by the account identifier: ["account", "alice"] . The value stored for each key is a number that represents the account balance.
Here's a step-by-step example of implementing this transferFunds function:
async function transferFunds ( sender : string , receiver : string , amount : number ) { if ( amount <= 0 ) throw new Error ( "Amount must be positive" ) ; // Construct the KV keys for the sender and receiver accounts. const senderKey = [ "account" , sender ] ; const receiverKey = [ "account" , receiver ] ; // Retry the transaction until it succeeds. let res = { ok : false } ; while ( ! res . ok ) { // Read the current balance of both accounts. const [ senderRes , receiverRes ] = await kv . getMany ( [ senderKey , receiverKey ] ) ; if ( senderRes . value === null ) { throw new Error ( ` Account ${ sender } not found ` ) ; } if ( receiverRes . value === null ) { throw new Error ( ` Account ${ receiver } not found ` ) ; } const senderBalance = senderRes . value ; const receiverBalance = receiverRes . value ; // Ensure the sender has a sufficient balance to complete the transfer. if ( senderBalance < amount ) { throw new Error ( ` Insufficient funds to transfer ${ amount } from ${ sender } ` , ) ; } // Perform the transfer. const newSenderBalance = senderBalance - amount ; const newReceiverBalance = receiverBalance + amount ; // Attempt to commit the transaction. `res` returns an object with // `ok: false` if the transaction fails to commit due to a check failure // (i.e. the versionstamp for a key has changed) res = await kv . atomic ( ) . check ( senderRes ) // Ensure the sender's balance hasn't changed. . check ( receiverRes ) // Ensure the receiver's balance hasn't changed. . set ( senderKey , newSenderBalance ) // Update the sender's balance. . set ( receiverKey , newReceiverBalance ) // Update the receiver's balance. . commit ( ) ; } }
In this example, the transferFunds function reads the balances and versionstamps of both accounts, calculates the new balances after the transfer, and checks if there are sufficient funds in account A. It then performs an atomic operation, setting the new balances with the versionstamp constraints. If the transaction is successful, the loop exits. If the version constraints are violated, the transaction fails, and the loop retries the transaction until it succeeds.
Limits Jump to heading #
In addition to a max key size of 2 KiB and max value size of 64 KiB, there are certain limits with the Deno KV transaction API:
- Max keys per kv.getMany() : 10
- Max batch size per kv.list() : 1000
- Max checks in an atomic operation : 100
- Max mutations in an atomic operation : 1000
- Max total size of an atomic operation : 800 KiB. This includes all keys and values in checks and mutations, and encoding overhead counts toward this limit as well.
- Max total size of keys : 90 KiB. This includes all keys in checks and mutations, and encoding overhead counts toward this limit as well.
- Max watched keys per kv.watch() 10


---

URL: https://docs.deno.com/deploy/support/
TITLE: Support and Feedback

Support and Feedback
If you have any questions or feedback about Deno Deploy, please reach out to us on the Deno Discord in the #deploy channel or contact us .
We are actively working on improving the platform and would love to hear your thoughts!


---

URL: https://docs.deno.com/deploy/reference/caching/
TITLE: Caching

Caching
Deno Deploy includes a built-in CDN that can cache responses from your application. This improves performance for:
- Static assets (images, CSS, JavaScript files)
- API responses and server-rendered pages that don't change frequently
Caching is enabled by default for all applications, but only responses with appropriate caching headers are actually cached.
Deno Deploy integrates with popular frameworks like Next.js to automatically optimize caching for features such as Incremental Static Regeneration (ISR).
The CDN cache is tied to both the revision and context. When you deploy a new revision, the cache is automatically invalidated, ensuring users always see the latest version of your application. Note that browser caching may still serve older content if the Cache-Control header permits it.
Caching a resource Jump to heading #
To cache a resource, set the Cache-Control header in your response. This standard HTTP header tells browsers and the CDN how to cache your content.
Supported caching directives Jump to heading #
Deno Deploy supports these caching directives:
Directive Description
max-age Maximum time (in seconds) the response is considered fresh by both CDN and browsers. After this time, the response is considered stale and revalidated with the server.
s-maxage Maximum time (in seconds) the response is considered fresh by shared caches (CDNs only, not browsers). After this time, the response is revalidated with the server.
stale-while-revalidate Maximum time (in seconds) a stale response can be served while a fresh one is fetched in the background.
stale-if-error Maximum time (in seconds) a stale response can be served if the server returns an error.
immutable Indicates the response will never change, allowing indefinite caching. Ideal for content-hashed static assets.
no-store Prevents caching of the response. Use for dynamic content that should never be cached.
no-cache Requires revalidation with the server before serving from cache. Use for content that changes frequently but can benefit from conditional requests.
Additional caching headers Jump to heading #
- 
Vary : Specifies which request headers should be included in the cache key, creating separate cached versions based on those headers.
- 
Expires : Sets an absolute expiration date for the response (alternative to max-age ). do not change, such as images or CSS files.
- 
no-store : The response should not be cached. This is useful for dynamic responses that should not be cached, such as API responses or server rendered pages.
- 
no-cache : The response should be revalidated with the server before being served from the cache. This is useful for dynamic responses that may change frequently.
The Vary header can be used to specify which request headers should be part of the cache key for the request.
The Expires header can be used to specify an absolute expiration date for the response. This is an alternative to the max-age directive.


---

URL: https://docs.deno.com/deploy/privacy_policy/
TITLE: Privacy Policy

Privacy Policy
DENO PRIVACY POLICY 09 September 2024
Deno Land Inc. (Deno, we, us, or our) collects and uses personal information in order to provide its products and services to you. This Privacy Policy (the Policy) describes the personal information we collect, the purposes for which we use it, the parties with whom we may share it, and your choices with respect to such information. For purposes of this Privacy Policy, personal information means any information that relates to you as an individual and could reasonably be used to identify you. This Privacy Policy applies to our collection and use of personal information through (i) our website at https://deno.com (the Site); (ii) any websites, applications or other digital properties that link to this Privacy Policy; and (iii) the products and services (the Deno Offerings) we offer to you on our proprietary platform (the Platform) via the following websites:
- Deno Deploy ( https://deno.com/deploy )
- Deno Deploy Classic ( https://deno.com/deploy/classic )
- Deno Subhosting ( https://deno.com/subhosting )
By accessing or using the Site or any other digital property that links to this Privacy Policy, you may learn about Deno and our technology platform, and registered customers may also access the Deno Offerings (collectively, the Services). To the extent permitted by applicable law, your use of Deno products and services constitutes your acknowledgment and/or consent to the practices described in this Policy.
This Privacy Policy incorporates Denos Terms and Conditions (the Terms). Capitalized terms that are not defined in the Privacy Policy have the meaning given to them in the Terms.
I. The Information We Collect, And How We Collect It
We collect the following categories of information, which may include personal information (collectively, the  Information ).
1. Information You Provide To Us
We collect information from and about you directly when you provide it to us. This information may be collected when you contact us, fill out a form, create an account, subscribe to our blog, access or participate on our Sites, respond to surveys, or otherwise interact with us. This information may include:
Contact Information. We collect your contact information when you voluntarily provide it to us. For example, you may disclose contact information to us via the Contact link on our Sites, submit information by mail, telephone, in person or electronically, when signing up for our newsletters and other marketing communications, or when you register to attend an event or program. Contact Information typically includes first name, last name, e-mail address, postal address, organization, telephone number and other information that identifies you or can be used to identify or contact you.
Account Credentials . When you register to create an account with us, we will collect certain additional personal information, including your name, email address, and potentially other information such as your GitHub user name and public GitHub profile.
In addition to Contact Information and Account Credentials, we may collect other kinds of information, such as:
- 
Comments, questions, and requests you may make;
- 
Information about your preferences, such as your preferred methods of communication and the types of information in which you are interested;
- 
Event and service-related information (such as information required for registration, access to premises or online resources, dietary restrictions, and areas of interest);
- 
Audio and visual information, such as photographs, video and voice recordings (e.g., from events you attended with us), or security camera recordings if you visit our premises;
- 
Details of downloads from our Sites;
- 
Records and copies of your correspondence (including email addresses and phone numbers), if you contact us; and
- 
Any other information you voluntarily provide.
2. Information Obtained From Third Parties
We may receive certain information about you from other sources, including publicly available sources (such as public records and social media platforms), as well as our service providers and marketing partners.
When we collect personal information from users and visitors of other sites on which you have interacted with us, we will do so in accordance with the terms of use and privacy policies of those sites and applicable law. We may also receive personal information when you comment on our social media advertisements, post comments about us, or tag us in a public-facing social media post. Personal information may also be collected by the third-party social media sites that host our social media pages. These sites may provide aggregate information and analysis to us about visitors use of our social media pages. This allows us to better understand and analyze our user growth, general demographic information about the users of these pages, and interaction with the content that we post. Overall, this information may be used to help us understand the types of visitors and users of our social media pages and use of the content. This Privacy Policy does not cover personal information collected by such third-party social media sites. For more information on their privacy and security practices please review the privacy policies and terms of use on their respective websites.
3. Information Collected Automatically
We and our service providers may automatically obtain certain information about you, your electronic device, and your interactions with us, including the following:
- 
Device data . We may collect data such as the type of device and its operating system and settings, browser type, mobile device carrier, country, IP address, and unique identifiers.
- 
Internet and other or electronic activity data . This includes information about your interaction with our Sites, emails, and other online content.
- 
Tracking Data . We may collect tracking data using first and third-party cookies, pixels, web server logs, web beacons, and similar data collection and tracking technologies on the Sites, third party websites, apps and online services, and across your devices (such as IP address, browser, type, ISP, platform type, device type). Third parties such as advertising networks and analytics providers may also collect information about your online activities over time and across different websites and devices when you access or use the Sites.
II. How We Use And Share Your Information
Deno uses the Information for the purpose for which it was collected and in a manner that is consistent with this Privacy Policy. These functions include operation, maintenance and improvements to the Sites, providing our products and services, solicitation of your feedback, gaining a better understanding of our customers and visitors of our Sites, responding to your requests and questions, hosting events, and informing you about our organization, products, services, events, and other areas of interest.
Analytics Services . We may use third-party web analytics services, such as Google Analytics, to help us understand and analyze how Site visitors use our services. For more information on how Google Analytics uses data collected through our Sites, visit www.google.com/policies/privacy/partners .
Aggregated Data . We may analyze your personal information in aggregate form which does not identify you personally ( Aggregated Data ). The Aggregated Data may be used to operate, maintain, manage, and improve the Sites, shared with our affiliates, agents, and business partners, and otherwise used and disclosed for lawful business purposes. We do not re-identify de-identified or aggregated information.
Service Providers/Vendors . Like many businesses, we hire other companies to perform certain business-related services. We may disclose personal information to certain types of third party companies but only to the extent needed to enable them to provide such services, for example web hosting, disaster recovery, client survey and marketing, and data storage.
Reorganization . If, in the future, Deno undergoes a corporate, partnership, or business reorganization, we may transfer the Information, including personal information, to the new or surviving entity.
Protection of Rights and Compliance . We may use your Information to protect the rights, privacy or safety of you, us or others; to ensure our compliance with legal and contractual requirements; and to prevent and investigate illegal, unethical, or unauthorized activities (including cyberattacks and identity theft).
If Deno intends on using or disclosing your personal information in any manner that is not consistent with this Privacy Policy, you will be informed of such anticipated use prior to or at the time at which the personal information is collected.
III. How We Protect Your Information
We take commercially reasonable steps to protect your personal information from loss, misuse, and unauthorized access, disclosure, alteration, or destruction. Please understand, however, that no security system is impenetrable. We cannot guarantee the security of our databases, nor can we guarantee that the personal information that you supply will not be intercepted while being transmitted to and from us over the Internet.
IV. Data Retention
Deno determines the retention period for all Information based on the purposes for which we collect and/or receive the Information and/or tax, legal and regulatory requirements. In addition to this, we may consider other factors, such as the nature and sensitivity of the data, and whether we can achieve the purpose for which we collected the data through other means.
V. Your Privacy Choices
1. Your Information
You may request access to, correction of, or deletion of the personal information we maintain about you, and we will endeavor to respond promptly to your request. In order to make such a request, please contact us as indicated below.
2. Marketing Communications
You may opt-out of marketing-related emails by clicking on the unsubscribe link located on the bottom of any marketing email or emailing us at support@deno.com . We will use commercially reasonable efforts to process such requests in a timely manner. Please note that even if you opt-out of marketing-related emails, you will continue to receive service-related and other non-marketing emails.
3. Tracking Technology
You can choose not to permit tracking technologies, such as cookies and web beacons, when you use our services, but blocking some types of these tracking technologies may interfere with your experience.
Browser-Based Opt-Outs . You may be able to disable tracking technologies using your web browser settings. Please review your browsers instructions or visit All About Cookies for general information. Note that your web browser may have settings that allow you to transmit a Do Not Track signal when you use online services. Like many websites, our Sites are not currently designed to respond to Do Not Track signals received from browsers.
Self-Regulatory Program Opt-Outs . Two self-regulatory programs are available to help you control the use of tracking technologies on your browsers  the Digital Advertising Alliance and the Network Advertising Initiative . Both programs help to regulate vendors in the digital advertising space. One function of their self-regulatory programs is to give you the ability to opt out of targeted (or interest-based) advertising, including the use of tracking technologies, from their member companies. You can visit the Digital Advertising Alliances Your Ad Choices website to opt out of targeted advertising for participating vendors. The Network Advertising Initiative similarly assists with opt outs through their Opt Out of Interest-Based Advertising webpage.
Google Analytics Opt-Out. To opt out of Google Analytics cookies, visit Googles My Ad Center and/or download the Google Analytics Opt-Out Browser Add-On.
VI. Children
We do not knowingly collect personal information from children under the age of 18 through the Sites. If you are under 18, please do not give us any personal information. We encourage parents and legal guardians to monitor their childrens Internet usage and to help enforce our Privacy Policy by instructing their children never to provide personal information through the Sites without their permission. If you have reason to believe that a child under the age of 18 has provided personal information to us, please contact us, at support@deno.com and we will endeavor to delete that information from our databases.
VII. External Websites
The Sites may contain links to third-party websites. These third-party sites may collect information about you if you click on a link. We have no control over the privacy practices or the content of these websites. As such, we are not responsible for the content or the privacy policies of those third-party websites. You should check the applicable third-party privacy policy and terms of use when visiting any other websites.
VIII. Important Notice To Non-U.S. Residents
The Sites are hosted in and provided from the United States and other countries. If you are located outside of the United States, please be aware that any information you provide to us may be transferred to the United States or other countries where the privacy laws may not be as protective as those in your country of origin. If you are located outside the United States and choose to use the Sites, you consent to any transfer and processing of your personal information in accordance with this Privacy Policy, and you do so at your own risk.
IX. Notice To California Residents
Pursuant to Section 1798.83 of the California Civil Code, residents of California have the right to obtain certain information about the types of personal information that companies with whom they have an established business relationship (and that are not otherwise exempt) have shared with third parties for direct marketing purposes during the preceding calendar year, including the names and addresses of those third parties, and examples of the types of services or products marketed by those third parties. In order to submit such a request, please contact us using the contact information provided at the end of this document. Please note, however, that we do not share, nor have we shared in the past, personal information with third parties for direct marketing purposes.
X. Notice To Nevada Residents
If you are a resident of Nevada, you have the right to opt-out of the sale of personal information to third parties. You can exercise this right by contacting us at support@deno.com with the subject line Nevada Do Not Sell Request and providing us with your name and the email address. Please note, however, that we do not sell any personal information to third parties.
XI. Changes To This Privacy Policy
This Privacy Policy is effective as of the date stated at the top of this Privacy Policy. We may change this Privacy Policy from time to time. Any such changes will be posted on the Sites. By accessing the Sites after we make any such changes to this Privacy Policy, you are deemed to have accepted such changes. Please be aware that, to the extent permitted by applicable law, our use of the Information is governed by the Privacy Policy in effect at the time we collect the Information. Please refer back to this Privacy Policy on a regular basis.
XII. How To Contact Us
Please reach out to support@deno.com for any questions, complaints, or requests regarding this Privacy Policy, and include in the subject line Privacy Policy", or contact us by mail at:
Deno Land Inc.
1111 6th Ave Ste 550
PMB 702973
San Diego CA, 92101
USA
 2024 Deno Land Inc. All rights reserved.


---

URL: https://docs.deno.com/deploy/reference/tunnel/
TITLE: Tunnel

Tunnel
Deno Deploy's tunnel feature allows you to securely expose your local development server to the internet. This is particularly useful for testing webhooks, sharing your work with collaborators, or accessing your local server from remote locations.
In addition to providing secure access to your local server, Deno Deploy's tunnel can also:
- Pull environment variables in the "Local" context from your Deno Deploy project to your local Deno process.
- Push Open Telemetry traces, metrics, and logs from your local Deno process to your Deno Deploy app, where you can view them in the Deno Deploy dashboard.
- Automatically connect to local development databases assigned to your Deno Deploy app.
Getting started Jump to heading #
To start using the tunnel feature, you'll need to have Deno installed on your local machine. You can then pass the --tunnel flag when running your Deno application locally, either with deno task or deno run . For example:
deno run --tunnel -A main.ts
The first time you run this command, you'll be prompted to authenticate with Deno Deploy and choose which Deno Deploy app you want to connect the tunnel to. Once authenticated, a secure tunnel will be established, and you'll receive a public URL that forwards traffic to your local server.
You can also specify --tunnel for deno task commands defined in your deno.json or package.json file:
{ "tasks" : { "dev" : "astro dev" } }
Then run the task with:
deno task --tunnel dev
Using the tunnel Jump to heading #
Once the tunnel is established, any requests made to the public URL will be forwarded to your local development server. You can use this URL to test webhooks, share your work with others, or access your local server from remote locations.
Stopping the tunnel Jump to heading #
To stop the tunnel, simply terminate the Deno process running your application. This will close the secure connection and stop forwarding traffic to your local server.
View open tunnels Jump to heading #
The "Tunnels" tab of your application dashboard on Deno Deploy shows all active tunnels connected to your application. From this tab, you can view details about each tunnel, including the public URL, the local address it's forwarding to, and the time it was established.
Environment variables Jump to heading #
When using the tunnel feature, the "Local" context environment variables from your Deno Deploy application are made available to your local Deno process. This allows you to use the same configuration locally as you do in your Deno Deploy application.
You can view and manage the environment variables for your Deno Deploy application in the "Environment Variables" tab of your application settings. See the docs on adding, editing, and removing environment variables for more information.
Viewing traces and logs Jump to heading #
When using the tunnel feature, Open Telemetry traces, metrics, and logs from your local Deno process are pushed to your Deno Deploy application. You can view these traces and logs in the "Observability" tab of your application dashboard on Deno Deploy.
You can filter to see only traces and logs from your local process by searching for context:local in the search bar.


---

URL: https://docs.deno.com/deploy/kv/data_modeling_typescript/
TITLE: Data Modeling in TypeScript

Data Modeling in TypeScript
In TypeScript applications, it is usually desirable to create strongly-typed, well-documented objects to contain the data that your application operates on. Using interfaces or classes , you can describe both the shape and behavior of objects in your programs.
If you are using Deno KV, however, there is a bit of extra work required to persist and retrieve objects that are strongly typed. In this guide, we'll cover strategies for working with strongly typed objects going into and back out from Deno KV.
Using interfaces and type assertions Jump to heading #
When storing and retrieving application data in Deno KV, you might want to begin by describing the shape of your data using TypeScript interfaces. Below is an object model which describes some key components of a blogging system:
model.ts
export interface Author { username : string ; fullName : string ; } export interface Post { slug : string ; title : string ; body : string ; author : Author ; createdAt : Date ; updatedAt : Date ; }
This object model describes a blog post and an associated author.
With Deno KV, you can use these TypeScript interfaces like data transfer objects (DTOs) - a strongly typed wrapper around the otherwise untyped objects you might send to or receive from Deno KV.
Without any additional work, you can happily store the contents of one of these DTOs in Deno KV.
import { Author } from "./model.ts" ; const kv = await Deno . openKv ( ) ; const a : Author = { username : "acdoyle" , fullName : "Arthur Conan Doyle" , } ; await kv . set ( [ "authors" , a . username ] , a ) ;
When retrieving this same object from Deno KV, however, it won't by default have type information associated with it. If you know the shape of the object that was stored for the key, however, you can use type assertion to inform the TypeScript compiler about the shape of an object.
import { Author } from "./model.ts" ; const kv = await Deno . openKv ( ) ; const r = await kv . get ( [ "authors" , "acdoyle" ] ) ; const ac = r . value as Author ; console . log ( ac . fullName ) ;
You can also specify an optional type parameter for get :
import { Author } from "./model.ts" ; const kv = await Deno . openKv ( ) ; const r = await kv . get < Author > ( [ "authors" , "acdoyle" ] ) ; console . log ( r . value . fullName ) ;
For simpler data structures, this technique may be sufficient. But often, you will want or need to apply some business logic when creating or accessing your domain objects. When this need arises, you can develop a set of pure functions that can operate on your DTOs.
Encapsulating business logic with a service layer Jump to heading #
When your application's persistence needs become more complex - such as when you need to create secondary indexes to query your data by different keys, or maintain relationships between objects - you will want to create a set of functions to sit on top of your DTOs to ensure that the data being passed around is valid (and not merely typed correctly).
From our business objects above, the Post object is complex enough where it is likely to need a small layer of code to save and retrieve an instance of the object. Below is an example of two functions that wrap the underlying Deno KV APIs, and return strongly typed object instances for the Post interface.
Notably, we need to store an identifier for an Author object, so we can retrieve author information from KV later.
import { Author , Post } from "./model.ts" ; const kv = await Deno . openKv ( ) ; interface RawPost extends Post { authorUsername : string ; } export async function savePost ( p : Post ) : Promise < Post > { const postData : RawPost = Object . assign ( { } , p , { authorUsername : p . author . username , } ) ; await kv . set ( [ "posts" , p . slug ] , postData ) ; return p ; } export async function getPost ( slug : string ) : Promise < Post > { const postResponse = await kv . get ( [ "posts" , slug ] ) ; const rawPost = postResponse . value as RawPost ; const authorResponse = await kv . get ( [ "authors" , rawPost . authorUsername ] ) ; const author = authorResponse . value as Author ; const post = Object . assign ( { } , postResponse . value , { author , } ) as Post ; return post ; }
This thin layer uses a RawPost interface, which extends the actual Post interface, to include some additional data that is used to reference data at another index (the associated Author object).
The savePost and getPost functions take the place of a direct Deno KV get or set operation, so that they can properly serialize and "hydrate" model objects for us with appropriate types and associations.


---

URL: https://docs.deno.com/deploy/kv/backup/
TITLE: Backups

Backups
KV databases hosted on Deno Deploy can be continuously backed up to your own S3-compatible storage buckets. This is in addition to the replication and backups that we internally perform for all data stored in hosted Deno KV databases to ensure high availability and data durability.
This backup happens continuously with very little lag, enabling point-in-time-recovery and live replication. Enabling backup for KV databases unlocks various interesting use-cases:
- Retrieving a consistent snapshot of your data at any point in time in the past
- Running a read-only data replica independent of Deno Deploy
- Pushing data into your favorite data pipeline by piping mutations into streaming platforms and analytical databases like Kafka, BigQuery and ClickHouse
Configuring backup to Amazon S3 Jump to heading #
First you must create a bucket on AWS:
- Go to the AWS S3 console
- Click "Create bucket"
- Enter a bucket name and choose a AWS region, then scroll down and click "Next"
- Install the AWS CLI
- Run aws s3api create-bucket --bucket <bucket-name> --region <region> --create-bucket-configuration LocationConstraint=<region> (replace <bucket-name> and <region> with your own values)
Then, create an IAM policy with PutObject access to the bucket, attach it to an IAM user, and create access keys for that user:
- Go to the AWS IAM console
- Click "Policies" in the left sidebar
- Click on "Create policy"
- Select the "JSON" the policy editor and paste the following policy:
{ "Version" : "2012-10-17" , "Statement" : [ { "Sid" : "KVBackup" , "Effect" : "Allow" , "Action" : "s3:PutObject" , "Resource" : "arn:aws:s3:::<bucket-name>/*" } ] }
Replace <bucket-name> with the name of the bucket you created earlier.
- Click "Review policy"
- Enter a name for the policy and click "Create policy"
- Click "Users" in the left sidebar
- Click "Add user"
- Enter a name for the user and click "Next"
- Click "Attach policies directly"
- Search for the policy you created earlier and click the checkbox next to it
- Click "Next"
- Click "Create user"
- Click on the user you just created
- Click "Security credentials" and then "Create access key"
- Select "Other", then click "Next"
- Enter a description for the access key and click "Create access key"
- Copy the access key ID and secret access key and save them somewhere safe. You will need them later, and you will not be able to retrieve them again.
- Copy the following command to your terminal, and replace <bucket-name> with the name of the bucket you created earlier, then run it:
aws iam create-policy --policy-name --policy-document '{"Version":"2012-10-17","Statement":[{"Sid":"KVBackup","Effect":"Allow","Action":"s3:PutObject","Resource":"arn:aws:s3::: /*"}]}'
- Copy the following command to your terminal, and replace <user-name> with a name for the user you are creating, then run it:
aws iam create-user --user-name
- Copy the following command to your terminal, and replace <policy-arn> with the ARN of the policy you created in step 1, and <user-name> with the name of the user you created in the previous step, then run it:
aws iam attach-user-policy --policy-arn --user-name
- Copy the following command to your terminal, and replace <user-name> with the name of the user you created in step 2, then run it:
aws iam create-access-key --user-name
- Copy the access key ID and secret access key and save them somewhere safe. You will need them later, and you will not be able to retrieve them again.
Now visit the Deno Deploy dashboard , and click on the "KV" tab in your project. Scroll to the "Backup" section, and click on "AWS S3". Enter the bucket name, access key ID, and secret access key you created earlier, and the region the bucket is in. Then click "Save".
The backup will start immediately. Once the data has been backed up, and continuous backup is active, you will see the status change to "Active".
Configuring backup to Google Cloud Storage Jump to heading #
Google Cloud Storage (GCS) is compatible with the S3 protocol, and can also be used as a backup target.
First you must create a bucket on GCP:
- Go to the GCP Cloud Storage console
- Click on "Create" in the top bar
- Enter a bucket name, choose a location, and click "Create"
- Install the gcloud CLI
- Run gcloud storage buckets create <bucket-name> --location <location> (replace <bucket-name> and <location> with your own values)
Then, create a service account with Storage Object Admin access to the bucket, and create an HMAC access key for the service account:
- Go to the GCP IAM console
- Click on "Service accounts" in the left sidebar
- Click on "Create service account"
- Enter a name for the service account and click "Done"
- Copy the email for the service account you just created. You will need it later.
- Go to the GCP Cloud Storage console
- Click on the bucket you created earlier
- Click on "Permissions" in the toolbar
- Click "Grant access"
- Paste the email for the service account you copied earlier into the "New principals" field
- Select "Storage Object Admin" from the "Select a role" dropdown
- Click "Save"
- Click on "Settings" in the left sidebar (still in the Cloud Storage console)
- Click on the "Interoperability" tab
- Click on "Create a key for a service account"
- Select the service account you created earlier
- Click "Create key"
- Copy the access key and secret access key and save them somewhere safe. You will need them later, and you will not be able to retrieve them again.
- Run the following command, replacing <service-account-name> with a name for the service account you are creating:
gcloud iam service-accounts create
- Run the following command, replacing <bucket-name> with the name of the bucket you created earlier, and <service-account-email> with the email of the service account you created in the previous step:
gsutil iam ch serviceAccount: :objectAdmin gs://
- Run the following command, replacing <service-account-email> with the email of the service account you created in the previous step:
gcloud storage hmac create
- Copy the accessId and secret and save them somewhere safe. You will need them later, and you will not be able to retrieve them again.
Now visit the Deno Deploy dashboard , and click on the "KV" tab in your project. Scroll to the "Backup" section, and click on "Google Cloud Storage". Enter the bucket name, access key ID, and secret access key you created earlier, and the region the bucket is in. Then click "Save".
The backup will start immediately. Once the data has been backed up, and continuous backup is active, you will see the status change to "Active".
Using backups Jump to heading #
S3 backups can be used with the denokv tool. Please refer to the documentation for more details.


---

URL: https://docs.deno.com/deploy/kv/operations/
TITLE: Operations

Operations
The Deno KV API provides a set of operations that can be performed on the key space.
There are two operations that read data from the store, and five operations that write data to the store.
Read operations can either be performed in strong or eventual consistency mode. Strong consistency mode guarantees that the read operation will return the most recently written value. Eventual consistency mode may return a stale value, but is faster.
Write operations are always performed in strong consistency mode.
get Jump to heading #
The get operation returns the value and versionstamp associated with a given key. If a value does not exist, get returns a null value and versionstamp.
There are two APIs that can be used to perform a get operation. The Deno.Kv.prototype.get(key, options?) API, which can be used to read a single key, and the Deno.Kv.prototype.getMany(keys, options?) API, which can be used to read multiple keys at once.
Get operations are performed as a "snapshot read" in all consistency modes. This means that when retrieving multiple keys at once, the values returned will be consistent with each other.
const res = await kv . get < string > ( [ "config" ] ) ; console . log ( res ) ; // { key: ["config"], value: "value", versionstamp: "000002fa526aaccb0000" } const res = await kv . get < string > ( [ "config" ] , { consistency : "eventual" } ) ; console . log ( res ) ; // { key: ["config"], value: "value", versionstamp: "000002fa526aaccb0000" } const [ res1 , res2 , res3 ] = await kv . getMany < [ string , string , string ] > ( [ [ "users" , "sam" ] , [ "users" , "taylor" ] , [ "users" , "alex" ] , ] ) ; console . log ( res1 ) ; // { key: ["users", "sam"], value: "sam", versionstamp: "00e0a2a0f0178b270000" } console . log ( res2 ) ; // { key: ["users", "taylor"], value: "taylor", versionstamp: "0059e9035e5e7c5e0000" } console . log ( res3 ) ; // { key: ["users", "alex"], value: "alex", versionstamp: "00a44a3c3e53b9750000" }
list Jump to heading #
The list operation returns a list of keys that match a given selector. The associated values and versionstamps for these keys are also returned. There are 2 different selectors that can be used to filter the keys matched.
The prefix selector matches all keys that start with the given prefix key parts, but not inclusive of an exact match of the key. The prefix selector may optionally be given a start OR end key to limit the range of keys returned. The start key is inclusive, and the end key is exclusive.
The range selector matches all keys that are lexicographically between the given start and end keys. The start key is inclusive, and the end key is exclusive.
Note: In the case of the prefix selector, the prefix key must consist only of full (not partial) key parts. For example, if the key ["foo", "bar"] exists in the store, then the prefix selector ["foo"] will match it, but the prefix selector ["f"] will not.
The list operation may optionally be given a limit to limit the number of keys returned.
List operations can be performed using the Deno.Kv.prototype.list<string>(selector, options?) method. This method returns a Deno.KvListIterator that can be used to iterate over the keys returned. This is an async iterator, and can be used with for await loops.
// Return all users const iter = kv . list < string > ( { prefix : [ "users" ] } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "alex"], value: "alex", versionstamp: "00a44a3c3e53b9750000" } console . log ( users [ 1 ] ) ; // { key: ["users", "sam"], value: "sam", versionstamp: "00e0a2a0f0178b270000" } console . log ( users [ 2 ] ) ; // { key: ["users", "taylor"], value: "taylor", versionstamp: "0059e9035e5e7c5e0000" } // Return the first 2 users const iter = kv . list < string > ( { prefix : [ "users" ] } , { limit : 2 } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "alex"], value: "alex", versionstamp: "00a44a3c3e53b9750000" } console . log ( users [ 1 ] ) ; // { key: ["users", "sam"], value: "sam", versionstamp: "00e0a2a0f0178b270000" } // Return all users lexicographically after "taylor" const iter = kv . list < string > ( { prefix : [ "users" ] , start : [ "users" , "taylor" ] } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "taylor"], value: "taylor", versionstamp: "0059e9035e5e7c5e0000" } // Return all users lexicographically before "taylor" const iter = kv . list < string > ( { prefix : [ "users" ] , end : [ "users" , "taylor" ] } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "alex"], value: "alex", versionstamp: "00a44a3c3e53b9750000" } console . log ( users [ 1 ] ) ; // { key: ["users", "sam"], value: "sam", versionstamp: "00e0a2a0f0178b270000" } // Return all users starting with characters between "a" and "n" const iter = kv . list < string > ( { start : [ "users" , "a" ] , end : [ "users" , "n" ] } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "alex"], value: "alex", versionstamp: "00a44a3c3e53b9750000" }
The list operation reads data from the store in batches. The size of each batch can be controlled using the batchSize option. The default batch size is 500 keys. Data within a batch is read in a single snapshot read, so the values are consistent with each other. Consistency modes apply to each batch of data read. Across batches, data is not consistent. The borders between batches is not visible from the API as the iterator returns individual keys.
The list operation can be performed in reverse order by setting the reverse option to true . This will return the keys in lexicographically descending order. The start and end keys are still inclusive and exclusive respectively, and are still interpreted as lexicographically ascending.
// Return all users in reverse order, ending with "sam" const iter = kv . list < string > ( { prefix : [ "users" ] , start : [ "users" , "sam" ] } , { reverse : true , } ) ; const users = [ ] ; for await ( const res of iter ) users . push ( res ) ; console . log ( users [ 0 ] ) ; // { key: ["users", "taylor"], value: "taylor", versionstamp: "0059e9035e5e7c5e0000" } console . log ( users [ 1 ] ) ; // { key: ["users", "sam"], value: "sam", versionstamp: "00e0a2a0f0178b270000" }
Note: in the above example we set the start key to ["users", "sam"] , even though the first key returned is ["users", "taylor"] . This is because the start and end keys are always evaluated in lexicographically ascending order, even when the list operation is performed in reverse order (which returns the keys in lexicographically descending order).
set Jump to heading #
The set operation sets the value of a key in the store. If the key does not exist, it is created. If the key already exists, its value is overwritten.
The set operation can be performed using the Deno.Kv.prototype.set(key, value) method. This method returns a Promise that resolves to a Deno.KvCommitResult object, which contains the versionstamp of the commit.
Set operations are always performed in strong consistency mode.
const res = await kv . set ( [ "users" , "alex" ] , "alex" ) ; console . log ( res . versionstamp ) ; // "00a44a3c3e53b9750000"
delete Jump to heading #
The delete operation deletes a key from the store. If the key does not exist, the operation is a no-op.
The delete operation can be performed using the Deno.Kv.prototype.delete(key) method.
Delete operations are always performed in strong consistency mode.
await kv . delete ( [ "users" , "alex" ] ) ;
sum Jump to heading #
The sum operation atomically adds a value to a key in the store. If the key does not exist, it is created with the value of the sum. If the key already exists, its value is added to the sum.
The sum operation can only be performed as part of an atomic operation. The Deno.AtomicOperation.prototype.mutate({ type: "sum", value }) method can be used to add a sum mutation to an atomic operation.
The sum operation can only be performed on values of type Deno.KvU64 . Both the operand and the value in the store must be of type Deno.KvU64 .
If the new value of the key is greater than 2^64 - 1 or less than 0 , the sum operation wraps around. For example, if the value in the store is 2^64 - 1 and the operand is 1 , the new value will be 0 .
Sum operations are always performed in strong consistency mode.
await kv . atomic ( ) . mutate ( { type : "sum" , key : [ "accounts" , "alex" ] , value : new Deno . KvU64 ( 100n ) , } ) . commit ( ) ;
min Jump to heading #
The min operation atomically sets a key to the minimum of its current value and a given value. If the key does not exist, it is created with the given value. If the key already exists, its value is set to the minimum of its current value and the given value.
The min operation can only be performed as part of an atomic operation. The Deno.AtomicOperation.prototype.mutate({ type: "min", value }) method can be used to add a min mutation to an atomic operation.
The min operation can only be performed on values of type Deno.KvU64 . Both the operand and the value in the store must be of type Deno.KvU64 .
Min operations are always performed in strong consistency mode.
await kv . atomic ( ) . mutate ( { type : "min" , key : [ "accounts" , "alex" ] , value : new Deno . KvU64 ( 100n ) , } ) . commit ( ) ;
max Jump to heading #
The max operation atomically sets a key to the maximum of its current value and a given value. If the key does not exist, it is created with the given value. If the key already exists, its value is set to the maximum of its current value and the given value.
The max operation can only be performed as part of an atomic operation. The Deno.AtomicOperation.prototype.mutate({ type: "max", value }) method can be used to add a max mutation to an atomic operation.
The max operation can only be performed on values of type Deno.KvU64 . Both the operand and the value in the store must be of type Deno.KvU64 .
Max operations are always performed in strong consistency mode.
await kv . atomic ( ) . mutate ( { type : "max" , key : [ "accounts" , "alex" ] , value : new Deno . KvU64 ( 100n ) , } ) . commit ( ) ;
watch Jump to heading #
The watch operation accepts an array of keys, and returns a ReadableStream , which emits a new value whenever any of the watched keys change their versionstamp . The emitted value is an array of Deno.KvEntryMaybe objects.
Note that the returned stream does not return every single intermediate state of the watched keys, but keeps you up to date with the latest state of keys. This means if a key is modified multiple times quickly, you may not receive a notification for every change, but the latest state of the key.
const db = await Deno . openKv ( ) ; const stream = db . watch ( [ [ "foo" ] , [ "bar" ] ] ) ; for await ( const entries of stream ) { entries [ 0 ] . key ; // ["foo"] entries [ 0 ] . value ; // "bar" entries [ 0 ] . versionstamp ; // "00000000000000010000" entries [ 1 ] . key ; // ["bar"] entries [ 1 ] . value ; // null entries [ 1 ] . versionstamp ; // null }


---

URL: https://docs.deno.com/deploy/reference/playgrounds/
TITLE: Playgrounds

Playgrounds
Playground applications enable you to create, edit, and deploy applications entirely from the Deno Deploy web dashboard, without needing to create a GitHub repository.
Playgrounds contain one or more files (JavaScript, TypeScript, TSX, JSON, etc.) that you can edit directly in the playground editor.
Creating a playground Jump to heading #
You can create playgrounds from the "Applications" page in your organization. Click the "New Playground" button to create a basic "Hello World" playground. Using the dropdown on the "New Playground" button lets you create playgrounds from other templates, such as Next.js or Hono.
Editing a playground Jump to heading #
To edit a playground, open it from the "Applications" page in your organization.
The playground editor consists of five main sections:
- Code editor : The central area where you edit code for the currently selected file. Above the editor is a navbar showing the current file name, which you can click to edit.
- File browser : Located on the left of the code editor, this panel shows all files in the playground. Click any file to open it in the editor. Create new files by clicking the "New" icon at the top of the file browser. Delete files using the delete button next to each file name.
- Top bar : Located above the code editor, this contains action buttons for the playground. The "Deploy" button saves current changes and triggers a build. "Build Config" and "Env Variables" buttons open their respective configuration drawers. The left side of the top bar displays the playground URL (unless the playground hasn't been deployed yet).
- Bottom drawer : Located beneath the code editor, this contains debugging tools including "Build Logs" that show build progress during deployment, and tabs for viewing logs and traces.
- Right drawer : Located to the right of the code editor, this contains tools for inspecting application output. The "Preview" tab displays an iframe showing the deployed application, while "HTTP Explorer" lets you send individual HTTP requests to your deployment.
The playground content automatically saves when you click the "Deploy" button or when the editor loses focus.
Uploading files Jump to heading #
You can upload a zip file containing files and directories to the playground by dragging it into the file browser area. The contents of the zip file will be extracted into the playground, preserving the directory structure.
 The playground editor does not support uploading individual files or directories.
Using the HTTP explorer Jump to heading #
The HTTP Explorer tab in the playground allows you to make arbitrary HTTP requests to any URL served by the playground. This is useful for testing APIs or other services that do not serve a web page.
To use the HTTP Explorer, enter the path and query parameters for the request you want to make, select the HTTP method (GET, POST, etc.), and click on the button labeled with the selected method.
Additional request headers can be added by clicking the "Set Headers" button.
After the response has been made, the HTTP Explorer will display the response status, headers, and body.
To view the trace for the request, click on the "Trace" button in the response section. This will open the request trace for the request in a drawer on top of the playground editor. From there you can also view any console.log output that was captured during the request.
Renaming a playground Jump to heading #
You can rename a playground by editing the playground slug on the playground settings page. This will update the default domain names associated with the playground since they are based on the playground slug. The new slug must be unique within the organization (i.e. must not be in use by another app or playground in the same organization).
Info
Any previous deno.net URLs pointing to the playground will no longer work after renaming.
Custom domains will continue to work, as they are not tied to the playground slug.
Deleting a playground Jump to heading #
Playgrounds can be deleted from the playground settings page. This will remove the playground and all its revisions from the organization. All existing deployments will immediately stop serving traffic, and all custom domain associations will be removed.
The playground and its revisions will no longer be accessible after deletion. Deleted playgrounds cannot be restored through the Deno Deploy UI.
Info
Deleted a playground by mistake? Contact Deno support within 30 days to restore it.
Limitations Jump to heading #
 Playgrounds cannot currently be transferred to another organization.


---

URL: https://docs.deno.com/deploy/kv/node/
TITLE: Using KV in Node.js

Using KV in Node.js
Connecting to a Deno KV database in Node.js is supported via our official client library on npm . You can find usage instructions for this option below.
Installation and usage Jump to heading #
Use your preferred npm client to install the client library for Node.js using one of the commands below.
npm install @deno/kv
pnpm add @deno/kv
yarn add @deno/kv
Once you've added the package to your Node project, you can import the openKv function (supports both ESM import and CJS require -based usage):
import { openKv } from "@deno/kv" ; // Connect to a KV instance const kv = await openKv ( "<KV Connect URL>" ) ; // Write some data await kv . set ( [ "users" , "alice" ] , { name : "Alice" } ) ; // Read it back const result = await kv . get ( [ "users" , "alice" ] ) ; console . log ( result . value ) ; // { name: "Alice" }
By default, the access token used for authentication comes from the DENO_KV_ACCESS_TOKEN environment variable. You can also pass it explicitly:
import { openKv } from "@deno/kv" ; const kv = await openKv ( "<KV Connect URL>" , { accessToken : myToken } ) ;
Once your Deno KV client is initialized, the same API available in Deno may be used in Node as well.
KV Connect URLs Jump to heading #
Connecting to a KV database outside of Deno requires a KV Connect URL. A KV Connect URL for a database hosted on Deno Deploy will be in this format: https://api.deno.com/databases/<database-id>/connect .
The database-id for your project can be found in the Deno Deploy dashboard , under the project's "KV" tab.
More information Jump to heading #
More information about how to use the Deno KV module for Node can be found on the project's README page .


---

URL: https://docs.deno.com/deploy/reference/builds/
TITLE: Builds

Builds
In Deno Deploy, each version of your application code is represented as a revision (or build). When deploying from GitHub, revisions generally map one-to-one to git commits in your repository.
Build triggers Jump to heading #
Builds can be triggered in three ways:
- 
Manually from the UI : Using the "Deploy Default Branch" button on the builds page, which deploys the default git branch (usually main ). The dropdown menu lets you select a different branch.
- 
Manually from the CLI : Using the deno deploy command.
- 
Automatically from GitHub : When a new commit is pushed to a GitHub repository linked to your app.
Build stages Jump to heading #
A revision goes through these stages before becoming available:
- Queuing : The revision waits to be assigned to a builder.
- Preparing : A builder downloads the source code and restores any available build caches.
- Install : The install command executes (if specified), typically downloading dependencies.
- Build : The build command executes (if specified), creating a build artifact that is uploaded to the runtime infrastructure.
- Deploy : The revision is prepared for deployment into each timeline. For each timeline, the following occurs:
- Create database : If the application has an attached database, ensure one exists for this timeline (creating it if necessary).
- Pre-deploy command : Any pre-deploy command configured for the application executes, typically for tasks like database migrations.
- Warmup : Only in the "Preview" timeline, the application is started to ensure it boots correctly.
- Routing : Roll out the new revision to the URLs associated with this timeline.
If any step fails, the build enters a "Failed" state and does not receive traffic.
Build logs are streamed live to the dashboard during the build process and remain available on the build page after completion.
Build caching speeds up builds by reusing files that haven't changed between builds. This happens automatically for framework presets and the DENO_DIR dependency cache.
You can cancel a running build using the "Cancel" button in the top-right corner of the build page. Builds automatically time out based on application configuration. By default, builds time out after 5 minutes, but this can be increased for users on the Pro plan.
App configuration Jump to heading #
App configuration defines how to convert source code into a deployable artifact. You can modify app configuration in three places:
- During app creation by clicking "Edit app config"
- In app settings by clicking "Edit" in the app configuration section
- In the retry drawer on a failed build's page
When creating an app, app configuration may be automatically detected from your repository if you're using a recognized framework or common build setup.
Configuration options Jump to heading #
- 
App directory : The directory within the repository to use as the application root. Useful for monorepos. Defaults to the repository root.
- 
Framework preset : Optimized configuration for supported frameworks like Next.js or Fresh. Learn more about framework integrations .
- 
Install command : Shell command for installing dependencies, such as npm install or deno install .
- 
Build command : Shell command for building the project, often a task from package.json or deno.json , such as deno task build or npm run build .
- 
Pre-deploy command : Shell command that runs after the build is complete but before deployment, typically for tasks like database migrations.
- 
Runtime configuration : Determines how the application serves traffic:
- Dynamic : For applications that respond to requests using a server (API servers, server-rendered websites, etc.)
- Entrypoint : The JavaScript or TypeScript file to execute
- Arguments (optional): Command-line arguments to pass to the application
- Runtime working directory (optional): The working directory for the application at runtime
- Static : For static websites serving pre-rendered content
- Directory : Folder containing static assets (e.g., dist , .output )
- Single page app mode (optional): Serves index.html for paths that don't match static files instead of returning 404 errors
Build environment Jump to heading #
The build environment runs on Linux using either x64 or ARM64 architecture. Available tools include:
- deno (same version as at runtime)
- node
- npm
- npx
- yarn (v1)
- pnpm
- git
- tar
- gzip
Info
All JavaScript inside of the builder is executed using Deno.
The node command is actually a shim that translates Node.js invocations to deno run . Similarly, npm , npx , yarn , and pnpm run through Deno rather than Node.js.
Environment variables configured for the "Build" context are available during builds, but variables from "Production" or "Development" contexts are not. Learn more about environment variables .
Builders have the following resources available during the build process:
- 2 vCPUs
- 3 GB of RAM (can be increased to 4 GB on Pro plan)
- 8 GB of storage


---

URL: https://docs.deno.com/deploy/reference/runtime/
TITLE: Runtime

Runtime
In Deno Deploy, all applications execute using a standard Deno runtime in a secure, isolated Linux environment.
The Deno runtime used in Deno Deploy is the standard Deno runtime , with full support for all features of the Deno CLI, including JSR and NPM dependencies, reading and writing to the file system, making network requests, spawning subprocesses, and loading FFI and node native addons.
The Deno runtime runs using --allow-all permissions.
Custom flags cannot be passed to the Deno runtime.
Runtime environment Jump to heading #
The runtime environment is a Linux-based environment running either x64 or ARM64 architecture. The exact set of tools available in the runtime environment is subject to change and thus cannot be relied upon.
Currently Deno Deploy runs on Deno 2.5.0
Lifecycle Jump to heading #
Deno Deploy runs applications in a serverless environment. This means that an application is not always running and is only started when a request is received. When no incoming traffic is received for a period of time, the application is stopped.
Applications can be started and stopped at any time. They should start quickly to respond to incoming requests without delay.
Multiple instances of the same application can run simultaneously. For example, one instance could be running in the US and another in Europe. Each instance is completely isolated from the others and they do not share CPU, memory, or disk resources. Multiple instances can also start in the same region when needed, such as to handle high traffic or during infrastructure updates.
Startup Jump to heading #
When the system decides to start an application, it provisions a new sandbox environment for the application. This environment is isolated from all other applications.
It then starts the application using the configured entrypoint and waits for the HTTP server to start. If the application crashes before the HTTP server starts, the request that triggered the start will fail with a 502 Bad Gateway error.
Once the application is started, incoming requests are routed to it and responses are sent back to the client.
Shutdown Jump to heading #
The application remains alive until no new incoming requests are received or responses (including response body bytes) are sent for a period of time. The exact timeout is between 5 seconds and 10 minutes. WebSocket connections that actively transmit data (including ping/pong frames) also keep the application alive.
Once the system decides to stop the application, it sends a SIGINT signal to the application as a trigger to shut down. From this point on, the application has 5 seconds to shut down gracefully before it will be forcibly killed with a SIGKILL signal.
Eviction Jump to heading #
Sometimes an isolate may shut down even if the application is actively receiving traffic. Some examples of when this can happen are:
- An application was scaled up to handle load, but the load has decreased enough to be handled by a single instance again.
- The underlying server executing the instance is too resource constrained to continue running this application instance.
- The underlying infrastructure is being updated or has experienced a failure.
When the system decides to evict an application, it attempts to divert traffic away from the instance being evicted as early as possible. Sometimes this means that a request will wait for a new instance to boot up even though an existing instance is already running.
When an application only serves requests that finish quickly, evictions are usually unnoticeable. For applications that serve long-running requests or WebSockets, evictions can be more noticeable because the application may need to be evicted while still processing a request. The system will try to avoid these scenarios, but it is not always possible.
After traffic has been diverted away from the old instance, the system sends a SIGINT signal to trigger a graceful shutdown. The application should finish processing any remaining requests quickly and shut down websockets and other long-running connections. Clients making long-running requests should be prepared to handle these disruptions and reconnect when disconnected.
5 seconds after the SIGINT signal is sent, the old instance will be forcibly killed with a SIGKILL signal if it has not already shut down gracefully.
Cold starts Jump to heading #
Because applications are not always running, they may need to start when a request is received. This is called a cold start. Cold starts in Deno Deploy are highly optimized and complete within 100 milliseconds for hello world applications, and within a few hundred milliseconds for larger applications.
Deno Deploy uses multiple optimizations to enable fast cold starts:
- 
Sandboxes and the Deno runtime are pre-provisioned to ensure they don't need to be created from scratch when starting an application.
- 
Applications start immediately when the client sends the first TCP packet to establish a TLS connection. For fast-starting applications, depending on the network round trip latency, the application may already be running before the client sends the HTTP request.
- 
File system access is optimized for frequently used startup files. Deno Deploy analyzes file access patterns during the build step's warmup phase and optimizes the file system for faster access.
When cold starts are slow, they can negatively impact user experience. To optimize your application for quick startup:
- 
Minimize dependencies used by your application.
- 
Load infrequently accessed code and dependencies lazily using dynamic import() .
- 
Minimize I/O operations during startup, especially top-level await operations and network requests.
If your application starts slowly, please contact Deno support for help investigating the issue.


---

URL: https://docs.deno.com/deploy/kv/key_expiration/
TITLE: Key Expiration (TTL for keys)

Key Expiration (TTL for keys)
Since version 1.36.2, Deno KV supports key expiration, allowing developers to control time to live (TTL) for keys in a KV database. This allows an expiration timestamp to be associated with a key, after which the key will be automatically deleted from the database:
const kv = await Deno . openKv ( ) ; // `expireIn` is the number of milliseconds after which the key will expire. function addSession ( session : Session , expireIn : number ) { await kv . set ( [ "sessions" , session . id ] , session , { expireIn } ) ; }
Key expiration is supported on both Deno CLI and Deno Deploy.
Atomic expiration of multiple keys Jump to heading #
If multiple keys are set in the same atomic operation and have the same expireIn value, the expiration of those keys will be atomic. For example:
const kv = await Deno . openKv ( ) ; function addUnverifiedUser ( user : User , verificationToken : string , expireIn : number , ) { await kv . atomic ( ) . set ( [ "users" , user . id ] , user , { expireIn } ) . set ( [ "verificationTokens" , verificationToken ] , user . id , { expireIn } ) . commit ( ) ; }
Caveats Jump to heading #
The expire timestamp specifies the earliest time after which the key can be deleted from the database. An implementation is allowed to expire a key at any time after the specified timestamp, but not before. If you need to strictly enforce an expiration time (e.g. for security purposes), please also add it as a field of your value and do a check after retrieving the value from the database.


---

URL: https://docs.deno.com/deploy/reference/button/
TITLE: Deploy Button

Deploy Button
The Deploy Button offers a shortcut for users to create and deploy a new application on Deno Deploy based on existing code hosted in a Git repository.
It provides a link directly into the Deno Deploy application creation flow, and populates settings in the creation flow based on provided query parameters or framework detection.
The specified repository will be cloned to the user's GitHub account and set as the source for a new project. By default, the new repository will be public, but can be set to be private if required.
Example Jump to heading #
The deploy button below demonstrates the creation of a new application based on a simple starter project
Create and deploy a new application Jump to heading #
Use the code below to give a button which creates and deploys a new application:
Markdown
[ ! [ Deploy on Deno ] ( https://deno.com/button ) ] ( https://console.deno.com/new?clone = REPOSITORY_URL )
HTML
< a href = "https://console.deno.com/new?clone=REPOSITORY_URL" > < img src = "https://deno.com/button" alt = "Deploy on Deno" / > < /a >
URL
https://console.deno.com/new?clone = REPOSITORY_URL
Parameters Jump to heading #
The following query parameters can be used to configure a Deploy Button:
- clone  (required) The URL of the source repo to clone as a new repo which will then be deployed
- path  (optional) The path within the source repo to clone from. Providing this will create a new repo whose root is this directory from within the source repository.
- app_directory  (optional) The directory within the new repository to use as the application root. This is useful when the repository is structured as a monorepo.
- install  (optional) the command to execute prior to a build in order to install dependencies
- build  (optional) the command to execute to build the application
- predeploy  (optional) the command to execute after a build but before deployment


---

URL: https://docs.deno.com/deploy/acceptable_use_policy/
TITLE: Acceptable use policy

Acceptable use policy
The Deno Deploy service includes resources (CPU time, request counts) that are subject to this Acceptable Use policy. This document can give a rough estimate to what we consider as "Acceptable Use", and what we do not.
Examples of Acceptable Use Jump to heading #
-  Server-side rendered websites
-  Jamstack sites and apps
-  Single page applications
-  APIs that query a DB or external API
-  A personal blog
-  A company website
-  An e-commerce site
-  Reverse proxy
Not Acceptable Use Jump to heading #
-  Crypto mining
-  Highly CPU-intensive load (e.g. machine learning)
-  Media hosting for external sites
-  Scrapers
-  Forward proxy
-  VPN
Guidelines Jump to heading #
We expect most projects to fall well within the usage limits. We will notify you if your projects usage significantly deviates from the norm. We will reach out to you where possible before taking any action to address unreasonable burdens on our infrastructure.


---

URL: https://docs.deno.com/deploy/kv/
TITLE: Deno KV Quick Start

Deno KV Quick Start
Deno KV is a key-value database built directly into the Deno runtime, available in the Deno.Kv namespace . It can be used for many kinds of data storage use cases, but excels at storing simple data structures that benefit from very fast reads and writes. Deno KV is available in the Deno CLI and on Deno Deploy .
Caution
Deno KV is still in development and may change. To use it, you must pass the --unstable-kv flag to Deno.
Let's walk through the key features of Deno KV.
Opening a database Jump to heading #
In your Deno program, you can get a reference to a KV database using Deno.openKv() . You may pass in an optional file system path to where you'd like to store your database, otherwise one will be created for you based on the current working directory of your script.
const kv = await Deno . openKv ( ) ;
Creating, updating, and reading a key-value pair Jump to heading #
Data in Deno KV is stored as key-value pairs, much like properties of a JavaScript object literal or a Map . Keys are represented as an array of JavaScript types, like string , number , bigint , or boolean . Values can be arbitrary JavaScript objects. In this example, we create a key-value pair representing a user's UI preferences, and save it with kv.set() .
const kv = await Deno . openKv ( ) ; const prefs = { username : "ada" , theme : "dark" , language : "en-US" , } ; const result = await kv . set ( [ "preferences" , "ada" ] , prefs ) ;
Once a key-value pair is set, you can read it from the database with kv.get() :
const entry = await kv . get ( [ "preferences" , "ada" ] ) ; console . log ( entry . key ) ; console . log ( entry . value ) ; console . log ( entry . versionstamp ) ;
Both get and list operations return a KvEntry object with the following properties:
- key - the array key you used to set the value
- value - the JavaScript object you set for this key
- versionstamp - a generated value used to determine if a key has been updated.
The set operation is also used to update objects that already exist for a given key. When a key's value is updated, its versionstamp will change to a new generated value.
Listing several key-value pairs Jump to heading #
To get values for a finite number of keys, you may use kv.getMany() . Pass in several keys as arguments, and you'll receive an array of values for each key. Note that values and versionstamps can be null if no value exists for the given key(s).
const kv = await Deno . openKv ( ) ; const result = await kv . getMany ( [ [ "preferences" , "ada" ] , [ "preferences" , "grace" ] , ] ) ; result [ 0 ] . key ; // ["preferences", "ada"] result [ 0 ] . value ; // { ... } result [ 0 ] . versionstamp ; // "00000000000000010000" result [ 1 ] . key ; // ["preferences", "grace"] result [ 1 ] . value ; // null result [ 1 ] . versionstamp ; // null
Often, it is useful to retrieve a list of key-value pairs from all keys that share a given prefix. This type of operation is possible using kv.list() . In this example, we get a list of key-value pairs that share the "preferences" prefix.
const kv = await Deno . openKv ( ) ; const entries = kv . list ( { prefix : [ "preferences" ] } ) ; for await ( const entry of entries ) { console . log ( entry . key ) ; // ["preferences", "ada"] console . log ( entry . value ) ; // { ... } console . log ( entry . versionstamp ) ; // "00000000000000010000" }
Returned keys are ordered lexicographically based on the next component of the key after the prefix. So KV pairs with these keys:
- ["preferences", "ada"]
- ["preferences", "bob"]
- ["preferences", "cassie"]
Will be returned in that order by kv.list() .
Read operations can either be performed in strong or eventual consistency mode . Strong consistency mode guarantees that the read operation will return the most recently written value. Eventual consistency mode may return a stale value, but is faster. By contrast, writes are always performed in strong consistency mode.
Deleting key-value pairs Jump to heading #
You can delete a key from the database using kv.delete() . No action is taken if no value is found for the given key.
const kv = await Deno . openKv ( ) ; await kv . delete ( [ "preferences" , "alan" ] ) ;
Atomic transactions Jump to heading #
Deno KV is capable of executing atomic transactions , which enables you to conditionally execute one or many data manipulation operations at once. In the following example, we create a new preferences object only if it hasn't been created already.
const kv = await Deno . openKv ( ) ; const key = [ "preferences" , "alan" ] ; const value = { username : "alan" , theme : "light" , language : "en-GB" , } ; const res = await kv . atomic ( ) . check ( { key , versionstamp : null } ) // `null` versionstamps mean 'no value' . set ( key , value ) . commit ( ) ; if ( res . ok ) { console . log ( "Preferences did not yet exist. Inserted!" ) ; } else { console . error ( "Preferences already exist." ) ; }
Learn more about transactions in Deno KV here .
Improve querying with secondary indexes Jump to heading #
Secondary indexes store the same data by multiple keys, allowing for simpler queries of the data you need. Let's say that we need to be able to access user preferences by both username AND email. To enable this, you could provide a function that wraps the logic to save the preferences to create two indexes.
const kv = await Deno . openKv ( ) ; async function savePreferences ( prefs ) { const key = [ "preferences" , prefs . username ] ; // Set the primary key const r = await kv . set ( key , prefs ) ; // Set the secondary key's value to be the primary key await kv . set ( [ "preferencesByEmail" , prefs . email ] , key ) ; return r ; } async function getByUsername ( username ) { // Use as before... const r = await kv . get ( [ "preferences" , username ] ) ; return r ; } async function getByEmail ( email ) { // Look up the key by email, then second lookup for actual data const r1 = await kv . get ( [ "preferencesByEmail" , email ] ) ; const r2 = await kv . get ( r1 . value ) ; return r2 ; }
Learn more about secondary indexes in the manual here .
Watching for updates in Deno KV Jump to heading #
You can also listen for updates from Deno KV with kv.watch() , which will emit a new value or values of the key or keys you provide. In the below chat example, we watch for updates on the key ["last_message_id", roomId] . We retrieve messageId , which we then use with kv.list() to grab all the new messages from seen and messageId .
let seen = "" ; for await ( const [ messageId ] of kv . watch ( [ [ "last_message_id" , roomId ] ] ) ) { const newMessages = await Array . fromAsync ( kv . list ( { start : [ "messages" , roomId , seen , "" ] , end : [ "messages" , roomId , messageId , "" ] , } ) ) ; await websocket . write ( JSON . stringify ( newMessages ) ) ; seen = messageId ; }
Learn more about using Deno KV watch here .
Production usage Jump to heading #
Deno KV is available for use in live applications on Deno Deploy . In production, Deno KV is backed by FoundationDB , the open source key-value store created by Apple.
Testing Jump to heading #
By default, Deno.openKv() creates or opens a persistent store based on the path from which the script that invoked it was run. This isn't usually desirable for tests, which need to produce the same behavior when run many times in a row.
To test code that uses Deno KV, you can use the special argument ":memory:" to create an ephemeral Deno KV datastore.
async function setDisplayName ( kv : Deno . Kv , username : string , displayname : string , ) { await kv . set ( [ "preferences" , username , "displayname" ] , displayname ) ; } async function getDisplayName ( kv : Deno . Kv , username : string , ) : Promise < string | null > { return ( await kv . get ( [ "preferences" , username , "displayname" ] ) ) . value as string ; } Deno . test ( "Preferences" , async ( t ) => { const kv = await Deno . openKv ( ":memory:" ) ; await t . step ( "can set displayname" , async ( ) => { const displayName = await getDisplayName ( kv , "example" ) ; assertEquals ( displayName , null ) ; await setDisplayName ( kv , "example" , "Exemplary User" ) ; const displayName = await getDisplayName ( kv , "example" ) ; assertEquals ( displayName , "Exemplary User" ) ; } ) ; } ) ;
This works because Deno KV is backed by SQLite when run for local development. Just like in-memory SQLite databases, multiple ephemeral Deno KV stores can exist at once without interfering with one another. For more information about special database addressing modes, see the SQLite docs on the topic .
Next steps Jump to heading #
At this point, you're just beginning to scratch the surface with Deno KV. Be sure to check out our guide on the Deno KV key space , and a collection of tutorials and example applications here.


---

URL: https://docs.deno.com/deploy/reference/apps/
TITLE: Applications

Applications
Applications are web services that serve traffic within an organization. Each application contains a history of revisions (previous versions), typically corresponding to Git commits when using the GitHub integration.
Applications are identified by a slug, which must be unique within the organization and is used in default domain names.
Creating an application Jump to heading #
To create an application:
- Click the "+ Create App" button on the organization page
- Select the GitHub repository to deploy from
- Configure the app slug (name)
- Set up build configuration
- Add any required environment variables
 Currently, applications must be linked to a GitHub repository during creation.
The build configuration determines how the application is built during the deployment process. Builds are automatically triggered on each push to the linked repository or when manually clicking "Deploy Default Branch". For detailed build configuration information, see the Builds documentation .
You can add environment variables during app creation by clicking "Edit Environment Variables". For more details on environment variables, see the Environment Variables and Contexts documentation.
Renaming an application Jump to heading #
Applications can be renamed by editing the app slug on the app settings page. This will update the default domain names associated with the app since they are based on the app slug. The new slug must be unique within the organization (i.e. must not be in use by another app or playground in the same organization).
Warning
Any previous deno.net URLs pointing to the app will no longer work after renaming.
Custom domains will continue to work, as they are not tied to the app slug.
Deleting an application Jump to heading #
Applications can be deleted from the app settings page. This will remove the app and all its revisions from the organization. All existing deployments will immediately stop serving traffic, and all custom domain associations will be removed.
The app and its revisions will no longer be accessible after deletion, and no traffic will be served from it. Deleted apps cannot be restored through the Deno Deploy UI.
Info
Deleted an app by mistake? Contact Deno support within 30 days to restore it.
Limitations Jump to heading #
 Apps cannot currently be transferred to another organization.
GitHub integration Jump to heading #
The GitHub integration enables automatic deployments of the app from a GitHub repository. Every push to the repository will trigger a new build of the app. Depending on the branch of the commit, the build will be deployed to different timelines .
Apps are linked to a GitHub repository during creation. However, it is possible to unlink the repository after creation, and optionally link it to a new GitHub repository. This can be done from the app settings page.
Only accounts that have been authorized with the Deno Deploy GitHub app will be visible in the GitHub repository dropdown. You can authorize new organizations or repositories by clicking the "+ Add another GitHub account" button in the user or organization dropdown, or the "Configure GitHub app permissions" button in the repository dropdown. This will redirect you to GitHub to authorize the Deno Deploy GitHub app with the selected GitHub account or organization. After authorization, you will be redirected back to the app settings page, where you can select the newly authorized GitHub repository.
GitHub events integration Jump to heading #
Whenever Deno Deploy builds an app from a GitHub repository, it will send a repository_dispatch event to the repository at the start and end of the build. This allows you to trigger GitHub Actions workflows based on the build status.
Deno Deploy will send the following events:
Event Name Description
deno_deploy.build.enqueued Sent when a build is enqueued, i.e. when a push is made to the repository.
deno_deploy.build.cancelled Sent when a build is cancelled, either manually or due to a timeout.
deno_deploy.build.failed Sent when a build fails.
deno_deploy.build.routed Sent when a build completes successfully, and traffic is routed to it.
The payload of the event follows the following TypeScript type definition:
interface DenoDeployBuildEventPayload { app : { /** The UUID of the Deno Deploy app. */ id : string ; /** The slug (name) of the Deno Deploy app. */ slug : string ; } ; organization : { /** The UUID of the Deno Deploy organization containing the app. */ id : string ; /** The slug (name) of the Deno Deploy organization containing the app. */ slug : string ; } ; revision : { /** The ID of the revision being built. */ id : string ; /** A URL to view the revision and build status in the Deno Deploy dashboard. */ html_url : string ; /** The Git commit SHA being built. */ git : { sha : string } ; /** The preview URL the revision is available at, if the build succeeded. */ preview_url : string | null ; } ; }
You can receive these events in a GitHub Actions workflow by adding a repository_dispatch trigger. For example:
on: repository_dispatch: types: [deno_deploy.build.routed] # Listen for successful builds jobs: notify: runs-on: ubuntu-latest steps: - name: Test the preview_url run: | echo "The Deno Deploy app is available at ${{ github.event.client_payload.revision.preview_url }}" curl -I ${{ github.event.client_payload.revision.preview_url }}


---

URL: https://docs.deno.com/deploy/reference/organizations/
TITLE: Organizations

Organizations
Organizations are groups of users that collectively own apps and domains. When signing up for Deno Deploy, each user can either create an organization or join an existing organization through invitation.
All users must belong to an organization to use Deno Deploy, as all resources are owned at the organization level.
Organizations have both a name and a slug. The name is visible only to organization members and appears in the organization dropdown in both Deno Deploy and Deploy Classic. The slug forms part of the default domain for all applications in the organization.
Caution
Organizations cannot be renamed, nor can their slug be changed after creation.
Every organization has a default domain used for production, git branch, and preview URLs for projects in that organization. For example, an organization with the slug acme-inc would have a default domain of acme-inc.deno.net .
Organizations can have multiple members. Currently, all members have owner permissions for the organization, which means they can invite other members, create and delete apps, and manage domains.
Create an organization Jump to heading #
Organizations in Deno Deploy are created when you sign up for a Deno Deploy account.
If you do not yet have a Deno Deploy account, you can create one by visiting the Deno Deploy dashboard and signing in with your GitHub account. You will be prompted to create an organization as part of the sign-up process.
Info
Organization slugs must be unique across all Deno Deploy organizations and cannot match any existing project name in Deno Deploy Classic.
Deleting an organization Jump to heading #
Organizations cannot currently be deleted from the dashboard. Please contact Deno support if you need to delete an organization.
Inviting users to an organization Jump to heading #
To invite a user:
- Go to the organization settings page and click "+ Invite User"
- Enter the user's GitHub account username (e.g., ry )
- Optionally enter an email address to send the invitation to
- Click "Invite"
If you don't specify an email address, we'll attempt to send the invitation to the email in the user's public GitHub profile or another email we may have on record.
After inviting a user, they will receive an email with an invite link (if we have their email address). They must click this link and accept the invitation to join the organization. You can also directly share the personalized invite link displayed in the members table after inviting a user.
You can cancel an invitation before it's accepted by clicking the delete button next to the invited user in the members table and confirming by clicking "Save". This invalidates the previously sent invitation link.
Removing users from an organization Jump to heading #
To remove a member from the organization, find the user in the members table in the organization settings, click the remove button, and confirm by clicking "Delete". "Delete".


---

URL: https://docs.deno.com/deploy/reference/databases/
TITLE: Databases

Databases
Deno Deploy's databases feature enables your applications to easily connect to a variety of databases, enabling seamless state management in your apps. Currently, PostgreSQL and Deno KV are supported.
After creating or linking a database instance, Deno Deploy automatically creates isolated (logical) databases inside of that instance for each deployment environment, including production, Git branches, and preview timelines. Your application code connects to the appropriate database based on the current environment, using automatically injected environment variables. This ensures that your data remains consistent and isolated across different stages of development and deployment.
Deno Deploy currently supports two database engines:
- PostgreSQL  Connect an existing externally hosted PostgreSQL instance, or provision a managed PostgreSQL database through Deno Deploy, hosted by Prisma.
- Deno KV  Provision a fast, globally distributed keyvalue store built for the edge.
Creating a database instance Jump to heading #
There are two ways to add a database instance to your Deno Deploy organization:
- Link Database : Connect an existing external database instance (for example, a PostgreSQL server you run or a managed instance from a cloud provider).
- Provision Database : Create and attach a managed data store from Deno Deploy (Deno KV or Prisma Postgres).
Linking an external database Jump to heading #
To link an existing external database instance you can either:
- go to the "Databases" page in your organization dashboard and click the "Link Database" button,
- go to the "Databases" tab in your app settings, click on "Attach Database", then select "Link Database" in database instance selection dropdown.
From here, enter the connection details for your external database instance. You will need to provide:
- Engine : Select the database engine (currently only PostgreSQL is supported).
- Connection Details : Enter the hostname, port, username, password, and optionally a CA certificate if required by your provider. You can also paste a connection string to automatically populate these fields.
- Slug : Give your database instance a descriptive name to identify it in the dashboard. This name is only used within Deno Deploy and does not affect your actual database server.
Once you've filled out the form, click "Test Connection" to verify your settings. If the connection is successful, click "Save" to add the database instance to your organization.
If the connection fails, double-check your connection details and ensure that your database server is accessible from Deno Deploy's network. We are unable to provide a list of IP addresses for Deno Deploy at this time, so please ensure your database server allows connections from all IPs. If you continue to have trouble, you can contact support for assistance.
Info
Because Deno Deploy creates isolated databases for each environment (production, Git branches, and previews), ensure that the database user you provide has sufficient privileges to create new databases on the server.
TLS/SSL configuration Jump to heading #
When linking an external database, Deno Deploy supports secure SSL/TLS connections. Depending on your database provider, you may need to upload a CA certificate to verify the server's identity.
If your database provider uses a trusted root Certificate Authority (CA), such as Let's Encrypt, no certificate upload is needed and SSL connections work automatically.
For users of AWS RDS, we will automatically detect RDS instances and provide an option to "Use AWS Certificate Bundle" to configure the necessary certificates without manual downloads.
For Google Cloud SQL users, you will need to download the Google Cloud SQL CA certificate from your Google Cloud Console and upload it when linking your database.
For other providers using self-signed certificates or private CAs, you will need to upload the specific CA certificate that was used to sign your database's certificate. You can usually obtain this from your database provider's documentation or console.
Provisioning a managed database Jump to heading #
To create and attach a managed database instance from Deno Deploy, you can either:
- go to the "Databases" page in your organization dashboard and click the "Provision Database" button,
- go to the "Databases" tab in your app settings, click on "Attach Database", then select "Provision Database" in database instance selection dropdown.
From here, select the database engine you want to provision. Available today:
- Deno KV  a fast, globally distributed keyvalue store built for the edge, hosted by Deno on your behalf.
- Prisma Postgres  the world's most advanced open source relational database, hosted by Prisma .
You will have to provide a Slug to identify the database instance in the dashboard. This name is only used within Deno Deploy and does not affect your actual database server.
Depending on the engine you select, there may be additional configuration options, such as choosing a region. Choosing a region close to your application's users can help reduce latency and improve performance of your database queries.
Once you're ready, click "Provision" to create the database instance. Deno Deploy will handle the provisioning process and set up the necessary infrastructure on your behalf.
Linking databases to your apps Jump to heading #
After creating or linking a database instance, you can assign it to your apps. Each database instance can be assigned to multiple apps. Each app gets its own isolated databases within the instance for each deployment environment (production, Git branches, and preview timelines).
Info
It is not currently possible to link multiple database instances to a single app. It is thus not possible to link both a Deno KV and a PostgreSQL database to the same app at this time.
Once assigned, Deno Deploy automatically creates an isolated database for each timeline within that app. The naming scheme for these databases is as follows:
- The single production database uses the format {app-id}-production
- Each Git branch database uses the format {app-id}--{branch-name}
- A singular preview database exists, with the format {app-id}-preview
Info
Currently only one preview database is created per app, shared across all preview deployments. In future releases, each preview deployment will get its own isolated database.
To assign a database to an app you can either:
- Go to the "Databases" page in your organization dashboard, find the database instance you want to assign, click "Assign", then select the app from the dropdown.
- Go to the "Databases" tab in your app settings, click on "Attach Database", then select the database instance from the dropdown.
Once assigned, Deno Deploy will automatically create the necessary isolated databases for each environment within that app.
Connecting to databases from your code Jump to heading #
Once you've assigned a database to your app, connecting to it from your code is simple. Deno Deploy automatically handles connection details, credentials, and environment variables for you.
Deno KV Jump to heading #
For Deno KV, you can use the built-in Deno.openKv() API to connect to your assigned Deno KV instance. No additional configuration is needed - Deno Deploy automatically connects your app to the correct Deno KV instance based on the current environment.
// No arguments needed - Deno Deploy handles this automatically const kv = await Deno . openKv ( ) ; Deno . serve ( async ( ) => { // Use the Deno KV instance await kv . set ( [ "user" , "123" ] , { name : "Alice" , age : 30 } ) ; const user = await kv . get ( [ "user" , "123" ] ) ; return new Response ( JSON . stringify ( user . value ) , { headers : { "content-type" : "application/json" } , } ) ; } ) ;
PostgreSQL Jump to heading #
For PostgreSQL databases (both external and provisioned), Deno Deploy automatically injects standard database environment variables into your app's runtime environment:
- DATABASE_URL : A full connection string for the current environment, in the format postgresql://username:password@hostname:port/database .
- PGHOST : The database server hostname.
- PGPORT : The database server port.
- PGDATABASE : The database name for the current environment.
- PGUSER : The database username.
- PGPASSWORD : The database password.
If your database requires a custom SSL/TLS certificate, Deno Deploy also injects that certificate into the default certificate store, so that all SSL connections work automatically.
You can use your favorite PostgreSQL client library (such as pg from npm) to connect to your database using these environment variables. Most libraries automatically detect and use these standard environment variables without any configuration.
As an example, here's how to connect to PostgreSQL in your Deno Deploy app:
import { Pool } from "npm:pg" ; // No arguments needed - the library reads connection details from environment variables automatically const pool = new Pool ( ) ; Deno . serve ( async ( ) => { // Use the database const result = await pool . query ( "SELECT * FROM users WHERE id = $1" , [ 123 ] ) ; return new Response ( JSON . stringify ( result . rows ) , { headers : { "content-type" : "application/json" } , } ) ; } ) ;
Running migrations and seeding data Jump to heading #
Since each environment has its own isolated database, you will often have many seperate databases to manage in every app. It is not practical to manually run migrations or insert seed data into each database individually every time you deploy a new revision.
To streamline this process, Deno Deploy allows you to configure an automated pre-deploy command that runs every time a revision is rolled out to a timeline, before the deployment starts serving traffic.
This command runs with the same environment variables available to your application, including PGHOST , PGPORT , PGDATABASE , etc., so you can use it to run migrations or seed data using your existing migration tools.
To set up an automated migration command, head to the Settings page of your app, and go to the App Config section. There you can edit the app configuration to set a pre-deploy command in the "Pre-Deploy Command" field (for example, deno task migrate or npm run migrate ).
You can see the detailed logs of the pre-deploy command execution in the revision build logs, in the "Deployment" section.
As an example, you could set up a migration script using node-pg-migrate :
- Add a task to your deno.json :
{ "tasks" : { "migrate" : "deno run --allow-net --allow-env --allow-read --allow-write npm:node-pg-migrate up" } }
- Create a migrations directory and add migration files. For example, migrations/1234567890_create-users-table.js :
exports . up = ( pgm ) => { pgm . createTable ( "users" , { id : "id" , name : { type : "varchar(100)" , notNull : true } , email : { type : "varchar(100)" , notNull : true } , created_at : { type : "timestamp" , notNull : true , default : pgm . func ( "current_timestamp" ) , } , } ) ; } ; exports . down = ( pgm ) => { pgm . dropTable ( "users" ) ; } ;
- Set your pre-deploy command to deno task migrate in the app settings.
Deno Deploy will automatically run this command before each deployment, ensuring all your environment-specific databases stay up to date.
Other migration tools such as Prisma Migrate, Drizzle, or Kysely can also be used in a similar way.
Local Development Jump to heading #
When developing locally, you have two options for your database setup:
- Use a local database instance running on your machine, for example a local PostgreSQL server or the built-in Deno KV backend in Deno.
- Connect to a hosted isolated local development instance provisioned on Deno Deploy, through --tunnel .
Using a local database instance Jump to heading #
Deno KV Jump to heading #
For Deno KV, you can use the built-in Deno.openKv() API to connect to a local Deno KV instance. By default, this uses a local file-based backend stored in your home directory.
const kv = await Deno . openKv ( ) ; // connects to local Deno KV instance Deno . serve ( async ( ) => { // Use the Deno KV instance await kv . set ( [ "user" , "123" ] , { name : "Alice" , age : 30 } ) ; const user = await kv . get ( [ "user" , "123" ] ) ; return new Response ( JSON . stringify ( user . value ) , { headers : { "content-type" : "application/json" } , } ) ; } ) ;
PostgreSQL Jump to heading #
To install PostgreSQL locally, follow the instructions for your operating system on postgresql.org . On macOS, you can use brew install postgresql , while many Linux distributions provide PostgreSQL packages through their package managers.
After installing PostgreSQL, create a new database and user for your local development:
createdb myapp_dev createuser myuser --pwprompt
Set up a .env file in your project root with the connection details for your local PostgreSQL instance:
DATABASE_URL = postgresql://myuser:mypassword@localhost:5432/myapp_dev
You can then use your favorite PostgreSQL client library (such as pg from npm) to connect to your local database using the DATABASE_URL environment variable.
If you are using Deno for local development, you can pass the --env-file flag to automatically load environment variables from your .env file on startup:
deno run -A --env-file main.ts
Using a hosted isolated local development instance Jump to heading #
Deno Deploy allows you to connect to a hosted isolated local development database when using the --tunnel flag in Deno .
To set this up, first provision a database instance in your Deno Deploy organization, and assign it to your app as usual.
Next, start your local development server with the --tunnel flag:
deno task --tunnel dev # or deno run -A --tunnel main.ts
Deno Deploy will automatically inject the appropriate database environment variables into your local environment, allowing your code to connect to the hosted isolated database instance. Please note that this database instance is shared across all developers using the same app, so be cautious when modifying data.
Info
When using the --tunnel flag, Deno Deploy also enables other features such as:
- exporting your logs, traces, and metrics from your local environment to the Deno Deploy dashboard
- pulling environment variables from the "Local" context to your local environment
- exposing your local server to a public URL for easy sharing and testing
If you do not want to use these features, consider using a local database instance instead.
Database management Jump to heading #
Organization level Jump to heading #
You can view and manage your database instances from the "Databases" page in your organization dashboard. Here you will find all your linked and provisioned database instances, along with their status, assigned apps, and connection details (if applicable).
Clicking on a database instance takes you to the database detail page, where you can view more information about the instance, including assigned apps, individual databases created within the instance. For each database provisioned in the instance, you can see its name, status, and associated app and timeline.
If a database fails to create, you'll see an error status with a "Fix" button that you can use to retry the operation and view more detailed error information.
From this page, you can also detach database instances from an app, or delete the instance entirely.
Next to each database in the instance, you can open the database explorer (for PostgreSQL databases), or copy the connection string ( DATABASE_URL ), allowing you to easily connect to your databases from your local machine for debugging.
App level Jump to heading #
You can also manage database instances from the "Databases" tab in your app settings. Here you can see the database instance assigned to your app, along with its status and connection details.
You can see all databases created for your app within the assigned instance, including those for production, Git branches, and preview timelines. For each database, you can see its name and status.
You can open the database explorer (for PostgreSQL databases), and copy the connection string ( DATABASE_URL ) for each database directly from this page, allowing you to easily connect to your databases from your local machine for debugging.
Database explorer Jump to heading #
Deno Deploy provides a built-in database explorer for PostgreSQL databases, allowing you to browse and manage your data directly from the Deno Deploy dashboard.
To access the database explorer, go to the "Databases" tab in your app settings, find the assigned PostgreSQL database instance, and click "Explore Database". This opens the database explorer interface, where you can view tables, run queries, and manage your data.
Info
The database explorer is not available for Deno KV databases at this time.
Prisma Postgres instances and claiming Jump to heading #
When you provision a managed Prisma Postgres database instance through Deno Deploy, the instance is initially "unclaimed". This means that the instance is managed by Deno Deploy on your behalf, but you do not have direct access to the Prisma account that owns the instance. Unclaimed instances have default limits on the number of databases and size based on Deno Deploy's plans.
If you want to have full control over the Prisma Postgres instance, including to upgrade to a higher Prisma plan, you can "claim" the instance into your own Prisma account. To do this, go to the database instance detail page in the Deno Deploy dashboard, and click the "Claim Instance" button. You will be guided through the process of linking the instance to your Prisma account.
Warning
Claiming a Prisma Postgres instance is a permanent action and cannot be undone. Once claimed, the instance is managed through your Prisma account and will make use of your Prisma account limits (based on plan). The Deno Deploy integration to automatically create isolated databases for each app will continue to work as before.
Limits Jump to heading #
There is no limit on the number of linked database instances, or Deno KV instances you can create in your organization.
For managed Prisma Postgres instances, only a couple of instances can be created per organization. In each Prisma Postgres instance, there is a limit to the number of databases that can be created, which directly affects the number of apps that can be assigned to that instance. Once a Prisma Postgres instance reaches its database limit, no more databases can be created in that instance.
If a Prisma Postgres instance is "claimed" through your own Prisma account, the database limit is determined by your Prisma plan. For unclaimed Prisma Postgres instances, a default limit applies.
For managed Prisma Postgres instances, each database has a size limit and operation count limit based on the plan you are on. The usage is shown at the bottom of the database instance detail page. If you are on a claimed Prisma Postgres instance, the limits are determined by your Prisma plan. For unclaimed Prisma Postgres instances, a default limit applies.
Frequently Asked Questions Jump to heading #
Q: Can multiple apps share the same database instance?
Yes! Multiple apps can be assigned to the same database instance. Each app gets its own isolated databases within that instance.
Q: What happens to my data when I remove an app assignment?
The databases remain on your database server. Only the connection between your app and the database instance is removed.
Q: Can I use the same database for multiple environments?
By default, each environment (production, branch, preview) gets its own database to ensure isolation and prevent data conflicts. However, you can customize the database your code connects to using options in your database library.
Q: How do I access my databases directly?
You can connect directly to your database server using the connection details you provided. Use the database names shown in the Deno Deploy dashboard.
Q: Can I change database connection details?
Yes, click "Edit" on any database instance to update connection details. Test the connection before saving to ensure it works.
Q: How do I delete a database instance?
First remove all app assignments, then click "Delete" on the database instance. This only removes the connection from Deno Deploy - your actual database server is not affected.


---

URL: https://docs.deno.com/deploy/reference/oidc/
TITLE: OIDC

OIDC
Deno Deploy is an OIDC provider. Every running application of Deno Deploy can be issued short-lived JWT tokens that are signed by Deno Deploy. These tokens contain information about the application, such as the organization and application ids and slugs, the context in which an application is executing, and the running revision ID.
The tokens can be used to authenticate with third-party services that support OIDC authentication, such as major cloud providers, but also HashiCorp Vault, NPM, and others.
Tip
Do you want to use OIDC tokens to authenticate with AWS or Google Cloud? Use the Cloud Connections feature instead of manually configuring OIDC authentication. Cloud Connections handle the entire configuration for you, including setting up trust relationships and permissions. OIDC is used under the hood.
Issuing Tokens Jump to heading #
To issue a token for the currently running application, use the getIdToken() function from the @deno/oidc module on JSR .
First, install @deno/oidc as a dependency of your application:
deno add jsr:@deno/oidc
Then, import the getIdToken() function and call it with the desired audience:
import { getIdToken } from "jsr:@deno/oidc" ; const token = await getIdToken ( "https://example.com/" ) ; console . log ( token ) ;
The audience parameter is a string that identifies the intended recipient of the token. It is typically a URL or an identifier that represents the service or application that will consume the token. The audience value must match the value configured in the third-party service that you want to authenticate with. It will be placed into the aud claim of the issued JWT token.
The getIdToken() function returns a promise that resolves to a JWT token as a string.
To check whether your current environment supports OIDC (i.e. whether your application is running on Deno Deploy), you can use the supportsIssuingIdTokens namespaced property:
import { supportsIssuingIdTokens } from "jsr:@deno/oidc" ; if ( supportsIssuingIdTokens ) { // OIDC is supported } else { // OIDC is not supported }
Token Structure Jump to heading #
The issued tokens are JWT tokens that are signed using the RS256 algorithm. The tokens contain the following claims:
Claim Name Example Value Description
iss https://oidc.deno.com The issuer of the token, which is always https://oidc.deno.com .
aud https://example.com/ The audience of the token, which is the value passed to the getIdToken() function.
iat 1757924011 The issued-at time of the token, which is a Unix timestamp indicating when the token was issued.
exp 1757924311 The expiration time of the token, which is a Unix timestamp indicating when the token will expire.
nbf 1757923951 The not-before time of the token, which is a Unix timestamp indicating when the token becomes valid.
sub deployment:deno/astro-app/production The subject of the token, which is a string concatenation of deployment:<org>/<app>/<context>
org_id 729adb8f-20d6-4b09-bb14-fac14cb260d1 The unique identifier of the organization that owns the application.
org_slug deno The slug of the organization that owns the application.
app_id 16ad21d8-7aeb-4155-8aa3-9f58df87cd3e The unique identifier of the application.
app_slug astro-app The slug of the application.
context_id 1d685676-92d7-418d-b103-75b46f1a58b4 The unique identifier of the context in which the application is running.
context_name production The context in which the application is running.
revision_id rh2r15rgy802 The unique identifier of the revision of the application that is currently running.
deployment_id A unique hash containing the entire deployment metadata, including the application, revision, and context IDs.
Tokens expire 5 minutes after they are issued. To account for clock skew, the tokens nbf claim is set to 1 minute before the iat claim.
Verifying Tokens Jump to heading #
To verify the tokens issued by Deno Deploy, you need to fetch the public keys from the OIDC provider's JWKS endpoint. The JWKS endpoint for Deno Deploy is:
https://oidc.deno.com/.well-known/jwks.json
Use the kid (key ID) from the JWT token header to select the correct key from the JWKS response.
Deno Deploy also provides a standard OIDC discovery document at:
https://oidc.deno.com/.well-known/openid-configuration
Deno Deploy rotates its signing keys periodically. Therefore, it is important to fetch the JWKS keys dynamically from the JWKS endpoint rather than hardcoding them.
Currently, Deno Deploy signing keys use the ES256 algorithm. This may change in the future, depending on security requirements, best practices, and support in third-party services.
To verify the tokens, you can use a JWT library that supports OIDC and JWKS. In TypeScript, you can use the jose library.


---

URL: https://docs.deno.com/deploy/classic/
TITLE: Deploy Classic

Deploy Classic
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
This section documents the legacy Deno Deploy Classic platform ( dash.deno.com ). We are no longer onboarding new users or organizations to Deploy Classic.
If you already have existing Deploy Classic projects, you may continue to operate them for now, but we strongly encourage you to begin migrating to the new Deno Deploy platform at console.deno.com .
Key differences:
- Deploy Classic is in maintenance mode (no new features, limited updates).
- New capabilities (enhanced Node/NPM support, integrated builds, metrics, tracing, framework presets, static assets, improved infra) are available in Deno Deploy.
Get started with the new platform here: About Deno Deploy Early Access .
Migration guidance is coming soon. In the meantime, you can set up a new Deno Deploy org and redeploy your apps there. Reach out to support if you need assistance.
What is Deno Deploy Classic? Jump to heading #
Deno Deploy Classic is a globally distributed platform for serverless JavaScript applications. Your JavaScript, TypeScript, and WebAssembly code runs on managed servers geographically close to your users, enabling low latency and faster response times. Deploy Classic applications run on fast, light-weight V8 isolates rather than virtual machines, powered by the Deno runtime .
Let's deploy your first application - it should only take a few minutes.
Install Deno and deployctl Jump to heading #
If you haven't already, you can install the Deno runtime using one of the commands below:
curl -fsSL https://deno.land/install.sh | sh
irm https://deno.land/install.ps1 | iex
curl -fsSL https://deno.land/install.sh | sh
After Deno is installed, install the deployctl utility:
deno install -A jsr:@deno/deployctl --global
You can confirm deployctl has been installed correctly by running:
deployctl --help
Now, you're ready to deploy a Deno script from the command line!
Write and test a Deno program Jump to heading #
First, create a directory for the project and create a file called main.ts in it, with the following "Hello World" web server:
main.ts
Deno . serve ( ( ) => new Response ( "Hello, world!" ) ) ;
You can test that it works by running it with the command below:
deno run --allow-net main.ts
Your server should be viewable at localhost:8000 . Now let's run this code on the edge with Deno Deploy!
Deploy your project Jump to heading #
From the directory of the main.ts file you just created, run this command:
deployctl deploy
You will be asked to authorize Deno Deploy in GitHub to sign up to Deno Deploy and/or to provision an access token for deployctl . A few moments after that, your Hello World server will be deployed in Deno Deploy Classic infrastructure all around the world, ready to handle all the traffic you expect.
Next Steps Jump to heading #
Now that you've created your first deployment, you can learn what kinds of apps you can run on Deno Deploy, check out what else you can do with deployctl , or keep reading to find out what other options you have to deploy your code to Deno Deploy. We're so excited to see what you'll ship with Deno Deploy!
Deploy your existing project Jump to heading #
Import a project and run it on the edge with Deno Deploy.
- 
From the Deno Deploy Classic dashboard click the "New Project" button.
- 
Connect to your GitHub account and select the repository you would like to deploy.
- 
Follow the on-screen instructions to deploy your existing application.
If your project requires a build step, use the Project Configuration form to create a GitHub action to deploy your project. Give your project a name and select from the optional framework presets. If you are not using a framework, you can set up your build settings using the form.
- 
Confirm that your build options are correct and click the "Deploy Project" button to kick off your new Github action and deploy your project.
In a few moments, your project will be deployed across ~12 data centers around the world, ready to handle large volumes of traffic.
Once your deployment is successful you can visit your newly deployed project at the url provided on the success page or manage it in your dashboard.
Start with a playground Jump to heading #
A playground is a browser-based editor that enables you to write and run JavaScript or TypeScript code right away This is a great choice for just kicking the tires on Deno and Deno Deploy!
From the Deno Deploy Classic dashboard , click the "New Playground" button to create a playground. We also have a variety of ready built tutorials for you to try out Deno Deploy Classic try them out by clicking on "Learning Playground" or visiting:
Simple HTTP server playground
Using the Deno KV database playground
RESTful API server playground
Realtime app with WebSockets playground
Recurring tasks with Deno.cron playground


---

URL: https://docs.deno.com/deploy/reference/timelines/
TITLE: Timelines

Timelines
A timeline is a representation of the history of one branch of the application. Each timeline has a set of revisions, which are the individual items in the timeline. One of the revisions (usually the most recent one) is the "active" revision, which is the one that is currently serving traffic. The active revision receives traffic on all URLs that are assigned to the timeline.
Each timeline is associated with a context , which determines which environment variables are available to the code running in that timeline.
By default, there are multiple timelines set up for each application:
- 
Production : The production timeline contains all of the revisions from the default git branch. This is the timeline that serves production traffic. This timeline is associated with https://<app-name>.<org-name>.deno.net , and any custom domains that are mapped to the application. It uses the production context.
- 
Git Branch / <branch-name> : Each git branch has its own timeline. This timeline contains all of the revisions from that git branch. This timeline is associated with https://<app-name>--<branch-name>.<org-name>.deno.net . It uses the development context.
There is also one timeline for each revision, that contains only that revision. This is the timeline that backs the preview URL for that revision. This timeline is associated with https://<app-name>-<revision-id>.<org-name>.deno.net . It uses the development context.
Preview timelines are not visible in timeline pages in the UI. You can view the preview URL for a revision on that revision's build page.
You can view the timelines that each revision is associated with on the revision's build page. You can also view the revisions that are associated with a given timeline from the timeline pages.
Active revision Jump to heading #
Each timeline has an active revision. The active revision is the revision that is currently serving traffic for that timeline. You can view the active revision for a timeline on the timeline page.
Usually, the active revision is the most recently built revision on the timeline. However, a different revision can be manually locked to be the active revision. This enables rollback, and timeline locking:
Rollback Jump to heading #
Rollback is the process of reverting the active revision to a previous revision, usually because the newer revision has some sort of bug or issue. By rolling back to a known good revision, you can restore the application to a working state without having to deploy new code via Git, and waiting for a build to complete.
Refer to "changing the active revision" below for more information on how to rollback a timeline.
Timeline locking Jump to heading #
Timeline locking is the process of locking a timeline to a specific revision, to ensure that new builds do not automatically become the active revision. This is useful if you are in a feature freeze situation, for example during a big event, and want to de-risk by not allowing new builds to be deployed. When a timeline is locked to a specific revision you can still create new builds by pushing to Git, but they will not automatically become the active revision on the locked timeline.
Refer to "changing the active revision" below for more information on how to lock a timeline to a specific revision.
Changing the active revision Jump to heading #
On the timelines page, you can lock any revision on that timeline to be the active revision. This will lock the timeline to that revision, and new builds will not automatically become the active revision on this timeline anymore. You can then either unlock the revision from the timeline, reverting back to the default behavior of the latest revision being the active revision, or you can lock a different revision to be the active revision.


---

URL: https://docs.deno.com/deploy/reference/env_vars_and_contexts/
TITLE: Environment Variables and Contexts

Environment Variables and Contexts
Environment variables in Deno Deploy allow you to configure your application with static values such as API keys or database connection strings.
Types of environment variables Jump to heading #
Environment variables can be stored as:
- Plain text : Visible in the UI and suitable for non-sensitive values like feature flags
- Secrets : Never visible in the UI after creation, only readable from application code, suitable for sensitive values like API keys
Variables can be set at:
- Application level : Specific to a single application
- Organization level : Applied to all applications in the organization, but can be overridden by application-level variables
Contexts Jump to heading #
Each environment variable applies to one or more contexts. Contexts represent the logical "environments" in which your code runs, each with its own set of variables and secrets.
By default, there are two contexts:
- Production : Used for the production timeline serving production traffic
- Development : Used for development timelines serving non-production traffic (preview URLs and branch URLs)
Info
Need additional contexts? Please contact support .
Additionally, there is a Build context used during the build process. Environment variables in the Build context are only available during builds and aren't accessible in Production or Development contexts (and vice versa). This separation enables different configuration for build-time vs. runtime.
Within a single application or organization, you cannot have multiple environment variables with the same name in the same context. You can, however, have variables with the same name in different non-overlapping contexts.
Adding, editing and removing environment variables Jump to heading #
You can manage environment variables from several locations:
- On the "New App" page while creating an application
- In the application settings under the "Environment Variables" section
- In the organization settings under the "Environment Variables" section
In each location, click the relevant edit button to open the environment variables drawer. Changes only apply when you click "Save." Clicking "Cancel" discards your changes.
To add a variable:
- Click "Add Environment Variable"
- Enter the name and value
- Specify whether it's a secret
- Select the contexts where it should apply
You can also bulk import variables from a .env file:
- Click "+ Add from .env file"
- Paste the contents of your .env file
- Click "Import variables"
Note that lines starting with # are treated as comments.
To remove a variable, click the "Remove" button next to it.
To edit a variable, click the "Edit" button next to it to modify its name, value, secret status, or applicable contexts.
Using environment variables in your code Jump to heading #
Access environment variables using the Deno.env.get API:
const myEnvVar = Deno . env . get ( "MY_ENV_VAR" ) ;
Limits Jump to heading #
Environment variables have the following limits:
- Environment variable keys can be at most 128 bytes long.
- Environment variable keys can not start with:
- DENO_ , except for DENO_AUTH_TOKENS , DENO_COMPAT , DENO_CONDITIONS , DENO_DEPLOY_ENDPOINT , or DENO_DEPLOY_TOKEN
- LD_
- OTEL_
- Environment variable values can be at most 16 KB (16,384 bytes) long.
- Environment variable keys can not be any of these keys. Instead, use Cloud Connections
- AWS_ROLE_ARN
- AWS_WEB_IDENTITY_TOKEN_FILE
- GCP_WORKLOAD_PROVIDER_ID
- GCP_SERVICE_ACCOUNT_EMAIL
- GCP_PROJECT_ID
- AZURE_CLIENT_ID
- AZURE_TENANT_ID
- AZURE_FEDERATED_TOKEN_FILE
Predefined environment variables Jump to heading #
Deno Deploy provides these predefined environment variables in all contexts:
- 
DENO_DEPLOY=1 : Indicates that the application is running in the Deno Deploy environment.
- 
DENO_DEPLOYMENT_ID : A unique identifier representing the entire configuration set (application ID, revision ID, context, and environment variables). Changes if any of these components change.
- 
DENO_DEPLOY_ORG_ID : The ID of the organization the application belongs to.
- 
DENO_DEPLOY_ORG_SLUG : The slug of the organization the application belongs to.
- 
DENO_DEPLOY_APP_ID : The ID of the application.
- 
DENO_DEPLOY_APP_SLUG : The slug of the application.
- 
DENO_DEPLOY_BUILD_ID : The ID of the currently running revision.
During builds, the environment variable CI=1 is additionally set.


---

URL: https://docs.deno.com/deploy/reference/deno_kv/
TITLE: Deno KV

Deno KV
Deno KV is a Key Value database supported in Deno Deploy as a database engine option in the databases feature. Thanks to the new timelines capability in Deno Deploy Early Access (EA), your apps have full control over the Deno KV databases they use (for example, one for production and one for each Git branch), ensuring data isolation and security across environments.
As with other database engines, your code automatically connects to the correct database for each environmentno timeline detection or manual database naming required.
Getting Started Jump to heading #
Add a KV database Jump to heading #
Navigate to your organization dashboard and click "Databases" in the navigation bar. Click "Provision Database", choose Deno KV as the database engine, provide a memorable name, and save.
Connect an app to a KV database Jump to heading #
Once you have a database instance you can assign it to an app. From the database instances list, click "Assign" next to the database you wish to use and select the app from the dropdown.
Deno Deploy automatically creates a separate database for each timeline. This keeps your production data safe while you develop and test. You can monitor provisioning and watch the status change to "Connected." If any errors occur, click "Fix" to retry.
Using Deno KV in Your Code Jump to heading #
Once you've assigned a database to your app, connecting from code is simple. Deno Deploy sets up the connection to the correct database based on the current environment.
Example Jump to heading #
Here's how to connect to Deno KV in your Deno Deploy app:
const kv = await Deno . openKv ( ) ; Deno . serve ( async ( ) => { const res = await kv . get < number > ( [ "requests" ] ) ; const requests = res . value + 1 ; await kv . set ( [ "requests" ] , requests ) ; return new Response ( JSON . stringify ( requests ) ) ; } ) ;
For detailed information about Deno KV and its features, see the Deno KV documentation .
Un-assigning a KV database Jump to heading #
If you remove a database assignment from an app, the app will no longer be able to access that database. However, the database itself and its data will remain intact and can be reassigned to another app or the same app at a later time. Hover over the name of the assigned app in the databases list and click the 'remove app assignment' icon to un-assign it.
Data Distribution Jump to heading #
Deno KV databases are replicated across at least three data centers in the primary region, Northern Virginia (us-east4). Once a write operation is committed, its mutations are durably stored in a quorum of data centers within the primary region. Cross-region replication is not currently available.
Data storage Jump to heading #
In local development, data is kept in memory. You do not need to create or allocate a database before using the KV APIs locally, and your KV code remains consistent across environments.
Deleting a database instance Jump to heading #
Click "Delete" on the Deno KV entry in the database instances list. Unlike other database engines, this action deletes all existing Deno KV databases and their data. Be sure to back up your data before proceeding.


---

URL: https://docs.deno.com/deploy/reference/frameworks/
TITLE: Frameworks

Frameworks
Deno Deploy supports a number of JavaScript and TypeScript frameworks out of the box. This means that you can use these frameworks without any additional configuration or setup.
Natively supported frameworks are tested to work with Deno Deploy and are automatically detected when you create a new app. Deno Deploy automatically optimizes the build and runtime configuration for these frameworks to be as optimal as possible.
Frameworks not listed here are still likely to work, but may require manually configuring the install and/or build command and the runtime configuration in the build settings.
Feel like a framework is missing? Let us know in the Deno Deploy Discord channel or contact Deno support .
Supported frameworks Jump to heading #
Next.js Jump to heading #
Next.js is a React framework for building full-stack web applications. You use React Components to build user interfaces, and Next.js for additional features and optimizations.
Both pages and app router are supported out of the box. ISR, SSG, SSR, and PPR are supported. Caching is supported out of the box, including using the new "use cache" .
next/image works out of the box.
Next.js on Deno Deploy always builds in standalone mode.
Tracing is supported out of the box, and Next.js automatically emits some spans for incoming requests, routing, rendering, and other operations.
Astro Jump to heading #
Astro is a web framework for building content-driven websites like blogs, marketing, and e-commerce. Astro leverages server rendering over client-side rendering in the browser as much as possible.
For static Astro sites, no additional configuration is needed to use Deno Deploy .
When using SSR in Astro with Deno Deploy , you need to install the @deno/astro-adapter package and configure your astro.config.mjs file to use the adapter:
$ deno add npm:@deno/astro-adapter # or npm install @deno/astro-adapter # or yarn add @deno/astro-adapter # or pnpm add @deno/astro-adapter
astro.config.mjs
import { defineConfig } from 'astro/config'; + import deno from '@deno/astro-adapter'; export default defineConfig({ + output: 'server', + adapter: deno(), });
Sharp image optimization is supported.
The astro:env API is supported.
Nuxt Jump to heading #
Create high-quality web applications with Nuxt, the open source framework that makes full-stack development with Vue.js intuitive.
Nuxt requires no additional setup.
SolidStart Jump to heading #
SolidStart is an open source meta-framework designed to unify components that make up a web application. It is built on top of Solid.
SolidStart requires no additional setup.
SvelteKit Jump to heading #
SvelteKit is a framework for rapidly developing robust, performant web applications using Svelte.
SvelteKit requires no additional setup.
Fresh Jump to heading #
Fresh is a full stack modern web framework for JavaScript and TypeScript developers. Fresh uses Preact as the JSX rendering engine.
Fresh requires no additional setup.
Lume Jump to heading #
Lume is a static site generator for building fast and modern websites using Deno.
Lume requires no additional setup.
Remix Jump to heading #
 Experimental : Remix is not yet fully supported. It is in the process of being integrated into Deno Deploy. Some features may not work as expected. Please report any issues you encounter to the Deno team.


---

URL: https://docs.deno.com/deploy/security/
TITLE: Security and responsible disclosure

Security and responsible disclosure
We consider the security of our systems, and all data controlled by those systems a top priority. No matter how much effort we put into system security, it is still possible that security vulnerabilities are present. We appreciate investigative work into system security carried out by well-intentioned, ethical security researchers. If you discover a vulnerability, however small, we would like to know about it so we can address it with appropriate measures, as quickly as possible. This page outlines the method we use to work with the security research community to address our system security.
Reporting a vulnerability Jump to heading #
Please email you findings to security@deno.com . We strive to resolve all problems as quickly as possible, and are more than happy to play an active role in publication of writeups after the problem is resolved.
Please do the following: Jump to heading #
- Do not take advantage of the vulnerability or problem you have discovered. For example only download data that is necessary to demonstrate the vulnerability - do not download any more. Also do not delete, modify, or view other people's data.
- Do not publish or reveal the problem until it has been resolved.
- Do not use attacks on physical security, social engineering, distributed denial of service, spam or applications of third parties.
- Do provide sufficient information to reproduce the problem, so we will be able to resolve it as quickly as possible. Usually, the IP address or the URL of the affected system and a description of the vulnerability will be sufficient, but complex vulnerabilities may require further explanation.
Our commitment Jump to heading #
- If you act in accordance with this policy, we will not take legal action against you in regard to your report.
- We will handle your report with strict confidentiality, and not pass on your personal details to third parties without your permission.


---

URL: https://docs.deno.com/deploy/tutorials/
TITLE: Deno examples and tutorials

Basics Jump to heading #
- What is Deno?
- Run a script
- Hello World
- Built in TypeScript support
- Your Deno Dev Environment
- Initialize a project
- Executable scripts
- All-in-one tooling
- Tasks and configuration with deno.json
- Top level await
- Update from CommonJS to ESM
- Import and export functions
- Interoperability with Node.js
- Introduction to Deno APIs
- Simple API server
- Simple file server
- Formatting with Deno fmt
- Benchmarking with Deno bench
- Generating documentation with deno doc
Modules and package management Jump to heading #
- Use Node.js built-in modules
- Import modules from npm
- Compatibility with Node & npm
- ECMAScript Modules
- Publishing Modules with JSR
Web standard APIs Jump to heading #
- Browser APIs in Deno
- Better debugging with the console API
- Manipulating & parsing URLs
- Fetch and stream data
- Set timeout and intervals
- Logging with colors
- Web workers
- Web assembly
Web frameworks and libraries Jump to heading #
- TypeScript and JSX
- Build a React App
- Build a Next.js app
- Build a Fresh app
- Build a Svelte app
- Build a Vue app
- Use Express with Deno
- How to use Apollo with Deno
- Build an Astro site with Deno
- Build a Qwik app with Deno
- Build a Nuxt app with Deno
- Build a Typesafe API with tRPC and Deno
- Build an API server with TypeScript
- Build a Vue app
- Build a SolidJS app
- Build a React app
- Build a Tanstack app
- HTTP server file upload
Sandboxes Jump to heading #
- Evaluating JavaScript
- Spawn a subprocess
- Serve a web framework
- Privide SSH access to a sandbox
- Intereactive JavaScript REPL
- Provide a VSCode instance in a sandbox
- Use template literals with variable interpolation
- Error handling
- Error handling with custom error classes
- Command cancellation
- Access string and binary output
- Set and get environment variables
- Stream output to a local file
- Upload files and directories to a sandbox
- Control sandbox lifetime
- Configure sandbox memory
Testing Jump to heading #
- Writing tests
- Basics of testing
- Mocking data in tests
- Stubbing
- Snapshot testing
- Spy functions
- Getting started with Deno test
- Better testing with Deno coverage
- Testing web applications
- BDD testing
OpenTelemetry Jump to heading #
- Basic OpenTelemetry setup
- Export telemetry to Grafana
- Export telemetry to Hyperdx
- Export telemetry to Honeycomb
- Span propagation
- OpenTelemetry with Deno Deploy
Deploying Deno projects Jump to heading #
- Deploy with Deno Deploy
- Deploy with the deploy command
- Migrating a custom domain to Deno Deploy
- AWS Lambda
- Deploy Deno to AWS Lambda
- AWS Lightsail
- Cloudflare workers
- Digital Ocean
- Google Cloud Run
- Kinsta
- Deploying Deno with Docker
Connecting to Databases Jump to heading #
- Connecting to databases
- Use MySQL2 with Deno
- Use PlanetScale with Deno
- Use Redis with Deno
- Use Prisma with Deno
- Use Drizzle with Deno
- Mongoose and MongoDB
- Connect to Redis
- Connect to Postgres
- Connect to Supabase
- Connect to MongoDB
- Connect to SQLite
- Connect to Mongoose and MongoDB
- Connect to Prisma
- Connect to DuckDB
Encoding Jump to heading #
- Hex and base64 encoding
- Parsing and serializing TOML
- Importing JSON
- Byte and text imports
- Image bundling with deno compile
- Parsing and serializing CSV
- Parsing and serializing JSON
- Parsing and serializing YAML
- Manipulating byte arrays
CLI Jump to heading #
- Build a Command Line Utility
- Input prompts
- Permission management
- Command line arguments
- Getting the Deno version
Network Jump to heading #
- HTTP requests
- HTTP Server: Hello world
- HTTP server: Routing
- HTTP server: Serving files
- HTTP server: Streaming
- HTTP server: CRUD with SQLite3
- Hono HTTP server
- HTTP server: WebSockets
- Piping streams
- Outbound WebSockets
- TCP Echo Server
- TCP connector: Ping
- TCP listener: Ping
- TCP/TLS connector: Ping
- TCP/TLS listener: Ping
- Running DNS queries
System Jump to heading #
- Handling OS signals
- Benchmarking
- Create a subprocess
- Subprocess Spawning
- Collecting output from subprocesses
- Reading system metrics
- Process information
- Environment variables
- Subprocesses: Spawning
- Handle OS signals
FileSystem Jump to heading #
- Path operations
- Reading files
- Writing files
- Deleting files
- Checking for file existence
- Moving/Renaming files
- Creating & removing directories
- Watching the filesystem
- Walking directories
- Unix cat
- Creating & resolving symlinks
- Temporary files & directories
- Streaming file operations
- Unzip gzipped file
Cryptography Jump to heading #
- Generating & validating UUIDs
- ULID
- Hashing
- RSASSA-PKCS1-v1_5 Signature and Verification
- HMAC Generation and Verification
- AES Encryption and Decryption
Advanced Jump to heading #
- File system events
- Module Metadata
- File Based Routing
- Build a chat app with WebSockets
- LLM Chat app
- Build a Realtime WebSocket Application
- Build a word finder app
- Connect to OpenAI - Chat completion
- User Data Processing with Deno Collections
- Exponential backoff
Unstable APIs Jump to heading #
- Deno KV watch
- Deno Cron
- Deno queues
- Deno KV: Key/Value database
- UDP listener: Ping
- UDP connector: Ping
- Temporal API
- Importing text
- Importing bytes


---

URL: https://docs.deno.com/deploy/classic/api/runtime-sockets/
TITLE: TCP sockets and TLS

TCP sockets and TLS
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic supports outbound TCP and TLS connections. These APIs allow you to use databases like PostgreSQL, SQLite, MongoDB, etc., with Deploy.
Looking for information on serving TCP? Take a look at the documentation for Deno.serve including its support for TCP options .
Deno.connect Jump to heading #
Make outbound TCP connections.
The function definition is same as Deno with the limitation that transport option can only be tcp and hostname cannot be localhost or empty.
function Deno . connect ( options : ConnectOptions ) : Promise < Conn >
Example Jump to heading #
async function handler ( _req ) { // Make a TCP connection to example.com const connection = await Deno . connect ( { port : 80 , hostname : "example.com" , } ) ; // Send raw HTTP GET request. const request = new TextEncoder ( ) . encode ( "GET / HTTP/1.1\nHost: example.com\r\n\r\n" , ) ; const _bytesWritten = await connection . write ( request ) ; // Read 15 bytes from the connection. const buffer = new Uint8Array ( 15 ) ; await connection . read ( buffer ) ; connection . close ( ) ; // Return the bytes as plain text. return new Response ( buffer , { headers : { "content-type" : "text/plain;charset=utf-8" , } , } ) ; } Deno . serve ( handler ) ;
Deno.connectTls Jump to heading #
Make outbound TLS connections.
The function definition is the same as Deno with the limitation that hostname cannot be localhost or empty.
function Deno . connectTls ( options : ConnectTlsOptions ) : Promise < Conn >
Example Jump to heading #
async function handler ( _req ) { // Make a TLS connection to example.com const connection = await Deno . connectTls ( { port : 443 , hostname : "example.com" , } ) ; // Send raw HTTP GET request. const request = new TextEncoder ( ) . encode ( "GET / HTTP/1.1\nHost: example.com\r\n\r\n" , ) ; const _bytesWritten = await connection . write ( request ) ; // Read 15 bytes from the connection. const buffer = new Uint8Array ( 15 ) ; await connection . read ( buffer ) ; connection . close ( ) ; // Return the bytes as plain text. return new Response ( buffer , { headers : { "content-type" : "text/plain;charset=utf-8" , } , } ) ; } Deno . serve ( handler ) ;


---

URL: https://docs.deno.com/deploy/classic/postgres/
TITLE: Connect to Postgres

Connect to Postgres
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
This tutorial covers how to connect to a Postgres database from an application deployed on Deno Deploy.
Setup Postgres Jump to heading #
This tutorial will focus entirely on connecting to Postgres unencrypted. If you would like to use encryption with a custom CA certificate, use the documentation here .
To get started, we need to create a new Postgres instance for us to connect to. For this tutorial, we will be using Supabase as they provide free, managed Postgres instances. If you like to host your database somewhere else, you can do that too.
- Visit https://app.supabase.io/ and click New project .
- Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
- Click Create new project . Creating the project can take a while, so be patient.
Gather credentials from Postgres Jump to heading #
Once you've set up your Postgres database, gather your connection information from your Postgres instance.
Supabase Jump to heading #
For the Supabase instance above, to get your connection information:
- Navigate to the Database tab on the left.
- Go to the Project Settings >> Database and copy the connection string from the Connection String >> URI field. This is the connection string you will use to connect to your database. Insert the password you saved earlier into this string, and then save the string somewhere - you will need it later.
psql Jump to heading #
If you are using psql, you should generally be able to find your connection information by running:
test=# \conninfo
Your Postgres connection string will take the form:
postgres://user:password@127.0.0.1:5432/deploy?sslmode = disable
Create a project in Deno Deploy Jump to heading #
Next, let's create a project in Deno Deploy Classic and set it up with the requisite environment variables:
- Go to https://dash.deno.com/new (Sign in with GitHub if you didn't already) and click on + Empty Project under Deploy from the command line .
- Now click on the Settings button available on the project page.
- Navigate to Environment Variables Section and add the following secrets.
- DATABASE_URL - The value should be your connection string that you retrieved in the last step.
Write code that connects to Postgres Jump to heading #
To read/write to Postgres, import a suitable Postgres module such as this one from JSR , read the connection string from the environment variables, and create a connection pool.
import { Pool } from "jsr:@bartlomieju/postgres" ; // Get the connection string from the environment variable "DATABASE_URL" const databaseUrl = Deno . env . get ( "DATABASE_URL" ) ! ; // Create a database pool with three connections that are lazily established const pool = new Pool ( databaseUrl , 3 , true ) ; // Connect to the database const connection = await pool . connect ( ) ; try { // Create the table await connection . queryObject ` CREATE TABLE IF NOT EXISTS todos ( id SERIAL PRIMARY KEY, title TEXT NOT NULL ) ` ; } finally { // Release the connection back into the pool connection . release ( ) ; }
Deploy application to Deno Deploy Classic Jump to heading #
Once you have finished writing your application, you can deploy it on Deno Deploy Classic.
To do this, go back to your project page at https://dash.deno.com/projects/<project-name> .
You should see a couple of options to deploy:
- Github integration
- deployctl
deployctl deploy --project = < project-name > < application-file-name >
Unless you want to add a build step, we recommend that you select the Github integration.
For more details on the different ways to deploy on Deno Deploy Classic and the different configuration options, read here .


---

URL: https://docs.deno.com/deploy/classic/how-to-deploy/
TITLE: Deploy with GitHub integration

Deploy with GitHub integration
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The simplest way to deploy more complex projects is via our Github integration. This allows you to link a Deno Deploy Classic project to a GitHub repository. Every time you push to the repository, your changes will be automatically deployed.
Via the Github integration, you can add a Github Action that defines a build step in your deployment process.
See the Github integration page for more details.
Deploy from command line with deployctl Jump to heading #
deployctl is a command line tool for deploying your code to Deno Deploy. You can control more details of your deployment than the above automatic GitHub integration by using deployctl .
See the deployctl page for more details.
Deploy with playground Jump to heading #
The easiest way to deploy some code is via a Deno Deploy Classic playground.
See the playground page for more details.


---

URL: https://docs.deno.com/deploy/classic/ci_github/
TITLE: CI and GitHub Actions

CI and GitHub Actions
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy's Git integration enables deployment of code changes that are pushed
to a GitHub repository. Commits on the production branch will be deployed as a production deployment. Commits on all other branches will be deployed as a preview deployment.
There are two modes of operation for the Git integration:
- Automatic : Deno Deploy Classic will automatically pull code and assets from your repository source every time you push, and deploy it. This mode is very fast, but does not allow for a build step. This is the recommended mode for most users.
- GitHub Actions : In this mode, you push your code and assets to Deno Deploy from a GitHub Actions workflow. This allows you to perform a build step before deploying.
Deno Deploy will select an appropriate mode based on your custom deployment configuration. Below, we go into more detail about the different configurations for Automatic and GitHub Actions mode.
Automatic Jump to heading #
If your project doesn't require any additional build steps, then the system chooses Automatic mode. The entrypoint file is simply the file that Deno Deploy will run.
GitHub Actions Jump to heading #
If you enter a command in Install Step and/or Build Step in the Project Configuration , Deno Deploy Classic will create a necessary GitHub Actions workflow file and push it into your repository. In this workflow file, we leverage the deployctl Github action to deploy your project. You can do whatever you need to do, such as running a build command, before deploying it to Deno Deploy.
To configure preprocessing commands you want to run, click Show advanced options button that appears after choosing your git repository. Then enter values as needed to input boxes.
Tip
For example, if you want to enable ahead-of-time builds for a Fresh project, you will enter deno task build in the Build Step box.
See also the Fresh doc for deploying a Fresh project to Deno Deploy.
The GitHub Actions workflow file that Deno Deploy Classic generates and pushes to your repository looks like as follows.
.github/workflows/deploy.yml
name: Deploy on: push: branches: main pull_request: branches: main jobs: deploy: name: Deploy runs-on: ubuntu-latest permissions: id-token: write # Needed for auth with Deno Deploy contents: read # Needed to clone the repository steps: - name: Clone repository uses: actions/checkout@v4 - name: Install Deno uses: denoland/setup-deno@v2 with: deno-version: v2.x - name: Build step run: "deno task build" - name: Upload to Deno Deploy uses: denoland/deployctl@v1 with: project: " " entrypoint: "main.ts" root: "."
See deployctl README for more details.


---

URL: https://docs.deno.com/deploy/classic/custom-domains/
TITLE: Custom domains

Custom domains
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
By default a project can be reached at its preview URL, which is $PROJECT_ID.deno.dev , e.g. dead-clam-55.deno.dev . You can also add a custom domain by following the instructions below.
Step 1: Add your custom domain in the Deno Deploy Classic dashboard Jump to heading #
- 
Click the "Settings" button on the project page, then select "Domains" from the sidebar.
- 
Enter the domain name you wish to add to the project and press "Add." Note that you must own the domain that you want to add to a project. If you do not own a domain yet, you can register one at a domain registrar like Google Domains, Namecheap, or gandi.net .
- 
The domain is added to the domains list and will have a "setup" badge.
- 
Click on the "setup" badge to visit the domain setup page, which will display the list of DNS records that need to be created/updated for your domain.
Step 2: Update your custom domain's DNS records Jump to heading #
Go to the DNS configuration panel of your domain registrar (or the service you're using to manage DNS) and enter the records as described on the domain setup page.
Step 3: Validate that the DNS records have been updated Jump to heading #
Go back to the Deno Deploy Classic dashboard and click the Validate button on the domain setup page. It will check if the DNS records are correctly set and if so, update the status to "Validated, awaiting certificate provisioning."
Step 4: Provision a certificate for your custom domain Jump to heading #
At this point you have two options. 99% of the time, you should choose the first option.
- 
Let us automatically provision a certificate using Let's Encrypt.
To do this, press the Get automatic certificates button. Provisioning a TLS certificate can take up to a minute. It is possible that the provisioning fails if your domain specifies a CAA record that prevents Let's Encrypt from provisioning certificates. Certificates will be automatically renewed around 30 days before the certificate expires. When you have been issued certificates successfully, you will see a green checkmark.
- 
Manually upload a certificate and private key.
To manually upload a certificate chain and private key, press the Upload your own certificates button. You will be prompted to upload a certificate chain and private key. The certificate chain needs to be complete and valid, and your leaf certificate needs to be at the top of the chain.


---

URL: https://docs.deno.com/deploy/classic/deployctl/
TITLE: Using deployctl on the command line

Using deployctl on the command line
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
deployctl is a command line tool (CLI) that lets you operate the Deno Deploy platform without leaving your terminal. With it you can deploy your code, create and manage your projects and their deployments, and monitor their usage and logs.
Dependencies Jump to heading #
The only dependency for deployctl is the Deno runtime. You can install it by running the following command:
curl -fsSL https://deno.land/install.sh | sh
You don't need to setup a Deno Deploy Classic account beforehand. It will be created along the way when you deploy your first project.
Install deployctl Jump to heading #
With the Deno runtime installed, you can install the deployctl utility with the following command:
deno install -gArf jsr:@deno/deployctl
The -A option in the deno install command grants all permissions to the installed script. You can opt not to use it, in which case you will be prompted to grant the necessary permissions when needed during the execution of the tool.
Deploy Jump to heading #
To perform a new deployment of your code, navigate to the root directory of your project and execute:
deployctl deploy
Project and Entrypoint Jump to heading #
If this is the first deployment of the project, deployctl will guess the project name based on the Git repo or directory it is in. Similarly, it will guess the entrypoint by looking for files with common entrypoint names (main.ts, src/main.ts, etc). After the first deployment, the settings used will be stored in a config file (by default deno.json).
You can specify the project name and/or the entrypoint using the --project and --entrypoint arguments respectively. If the project does not exist, it will be created automatically. By default it is created in the personal organization of the user, but it can also be created in a custom organization by specifying the --org argument. If the organization does not exist yet, it will also be created automatically.
deployctl deploy --project = helloworld --entrypoint = src/entrypoint.ts --org = my-team
Include and Exclude Files Jump to heading #
By default, deployctl deploys all the files in the current directory (recursively, except node_modules directories). You can customize this behavior using the --include and --exclude arguments (also supported in the config file). These arguments accept specific files, whole directories and globs. Here are some examples:
- 
Include only source and static files:
deployctl deploy --include = ./src --include = ./static
- 
Include only Typescript files:
deployctl deploy --include = **/*.ts
- 
Exclude local tooling and artifacts
deployctl deploy --exclude = ./tools --exclude = ./benches
A common pitfall is to not include the source code modules that need to be run (entrypoint and dependencies). The following example will fail because main.ts is not included:
deployctl deploy --include = ./static --entrypoint = ./main.ts
The entrypoint can also be a remote script. A common use case for this is to deploy an static site using std/http/file_server.ts .
deployctl deploy --include = dist --entrypoint = jsr:@std/http/file-server
Environment variables Jump to heading #
You can set env variables using --env (to set individual environment variables) or --env-file (to load one or more environment files). These options can be combined and used multiple times:
deployctl deploy --env-file --env-file = .other-env --env = DEPLOYMENT_TS = $( date +%s )
The deployment will have access to these variables using Deno.env.get() . Be aware that the env variables set with --env and --env-file are specific for the deployment being created and are not added to the list of env variables configured for the project .
Production Deployments Jump to heading #
Each deployment you create have a unique URL. In addition, a project has a "production URL" and custom domains routing trafffic to its "production" deployment. Deployments can be promoted to production at any time, or created directly as production using the --prod flag:
deployctl deploy --prod
Learn more about production deployments in the Deployments docs.
Deployments Jump to heading #
The deployments subcommand groups all the operations around deployments.
List Jump to heading #
You can list the deployments of a project with:
deployctl deployments list
Output:
 Page 1 of the list of deployments of the project 'my-project' is ready   Deployment  Date  Status  Database  Domain  Entrypoint  Branch  Commit    kcbxc4xwe4mc  12/3/2024 13:21:40 CET (2 days)  Preview  Preview  https://my-project-kcbxc4xwe4mc.deno.dev  main.ts  main  4b6c506   c0ph5xa9exb3  12/3/2024 13:21:25 CET (2 days)  Production  Production  https://my-project-c0ph5xa9exb3.deno.dev  main.ts  main  4b6c506   kwkbev9er4h2  12/3/2024 13:21:12 CET (2 days)  Preview  Preview  https://my-project-kwkbev9er4h2.deno.dev  main.ts  main  4b6c506   dxseq0jc8402  6/3/2024 23:16:51 CET (8 days)  Preview  Production  https://my-project-dxseq0jc8402.deno.dev  main.ts  main  099359b   7xr5thz8yjbz  6/3/2024 22:58:32 CET (8 days)  Preview  Preview  https://my-project-7xr5thz8yjbz.deno.dev  main.ts  another  a4d2953   4qr4h5ac3rfn  6/3/2024 22:57:05 CET (8 days)  Failed  Preview  n/a  main.ts  another  56d2c88   25wryhcqmb9q  6/3/2024 22:56:41 CET (8 days)  Preview  Preview  https://my-project-25wryhcqmb9q.deno.dev  main.ts  another  4b6c506   64tbrn8jre9n  6/3/2024 8:21:33 CET (8 days)  Preview  Production  https://my-project-64tbrn8jre9n.deno.dev  main.ts  main  4b6c506   hgqgccnmzg04  6/3/2024 8:17:40 CET (8 days)  Failed  Production  n/a  main.ts  main  8071902   rxkh1w3g74e8  6/3/2024 8:17:28 CET (8 days)  Failed  Production  n/a  main.ts  main  b142a59   wx6cw9aya64c  6/3/2024 8:02:29 CET (8 days)  Preview  Production  https://my-project-wx6cw9aya64c.deno.dev  main.ts  main  b803784   a1qh5fmew2yf  5/3/2024 16:25:29 CET (9 days)  Preview  Production  https://my-project-a1qh5fmew2yf.deno.dev  main.ts  main  4bb1f0f   w6pf4r0rrdkb  5/3/2024 16:07:35 CET (9 days)  Preview  Production  https://my-project-w6pf4r0rrdkb.deno.dev  main.ts  main  6e487fc   nn700gexgdzq  5/3/2024 13:37:11 CET (9 days)  Preview  Production  https://my-project-nn700gexgdzq.deno.dev  main.ts  main  c5b1d1f   98crfqxa6vvf  5/3/2024 13:33:52 CET (9 days)  Preview  Production  https://my-project-98crfqxa6vvf.deno.dev  main.ts  main  090146e   xcdcs014yc5p  5/3/2024 13:30:58 CET (9 days)  Preview  Production  https://my-project-xcdcs014yc5p.deno.dev  main.ts  main  5b78c0f   btw43kx89ws1  5/3/2024 13:27:31 CET (9 days)  Preview  Production  https://my-project-btw43kx89ws1.deno.dev  main.ts  main  663452a   62tg1ketkjx7  5/3/2024 13:27:03 CET (9 days)  Preview  Production  https://my-project-62tg1ketkjx7.deno.dev  main.ts  main  24d1618   07ag6pt6kjex  5/3/2024 13:19:11 CET (9 days)  Preview  Production  https://my-project-07ag6pt6kjex.deno.dev  main.ts  main  4944545   4msyne1rvwj1  5/3/2024 13:17:16 CET (9 days)  Preview  Production  https://my-project-4msyne1rvwj1.deno.dev  main.ts  main  dda85e1   Press enter to fetch the next page [Enter]
This command outputs pages of 20 deployments by default. You can iterate over the pages with the enter key, and use the --page and --limit options to query a specific page and page size.
Like with the rest of commands, you can use the --project option to specify the project of which to list deployments, if you are not in a project directory or want to list deployments from a different project.
Show Jump to heading #
Get all the details of a particular deployment using:
deployctl deployments show
Output:
 The production deployment of the project 'my-project' is 'c0ph5xa9exb3'  The details of the deployment 'c0ph5xa9exb3' are ready: c0ph5xa9exb3 ------------ Status: Production Date: 2 days, 12 hours, 29 minutes, 46 seconds ago (12/3/2024 13:21:25 CET) Project: my-project (e54f23b5-828d-4b7f-af12-706d4591062b) Organization: my-team (d97822ac-ee20-4ce9-b942-5389330b57ee) Domain(s): https://my-project.deno.dev https://my-project-c0ph5xa9exb3.deno.dev Database: Production (0efa985f-3793-48bc-8c05-f740ffab4ca0) Entrypoint: main.ts Env Vars: HOME Git Ref: main [4b6c506] Message: change name Author: John Doe @johndoe [mailto:johndoe@deno.com] Url: https://github.com/arnauorriols/my-project/commit/4b6c50629ceeeb86601347732d01dc7ed63bf34f Crons: another cron [*/10 * * * *] succeeded at 15/3/2024 1:50:00 CET after 2 seconds (next at 15/3/2024 2:00:00 CET) newest cron [*/10 * * * *] n/a yet another cron [*/10 * * * *] failed at 15/3/2024 1:40:00 CET after 2 seconds (next at 15/3/2024 1:51:54 CET)
If no deployment is specified, the command shows the details of the current production deployment of the project. To see the details of the last deployment, use --last , and to see the details of a particular deployment, use --id (or positional argument). You can also use --next or --prev to navigate the deployments chronologically.
For example, to see the details of the second to last deployment, you can do:
deployctl deployments show --last --prev
And to see the details of 2 deployments after a specific deployment:
deployctl deployments show 64tbrn8jre9n --next = 2
Redeploy Jump to heading #
The redeploy command creates a new deployment reusing the build of an existing deployment, for the purpose of changing the resources associated with it. This includes production domains, environment variables and KV databases.
Info
The semantics of selecting the deployment to redeploy are the same as those of the show subcommand , including --last , --id , --next and --prev .
Production Domains Jump to heading #
If you want to change the routing of the production domains of the project to a particular deployment, you can redeploy it with the --prod option:
deployctl deployments redeploy --prod 64tbrn8jre9n
This will create a new deployment with the same code and environment variables as the specified deployment, but with the production domains of the project pointing to it. For those projects with preview/prod databases (ie projects linked to GitHub), this will also set the production database for the new deployment.
Note
This feature is similar to the "promote to production" button found in the Deno Deploy Classic web application with the exception that the "promote to production" button does not create a new deployment. Instead, the "promote to production" button changes the domain routing in-place, however it's restricted to deployments already using the production database.
KV Database Jump to heading #
If this is a GitHub deployment, it will have 2 databases, one for prod deployments and one for preview deployments. You can change the database of a deployment by redeploying it with the --db option:
deployctl deployments redeploy --db = prod --id = 64tbrn8jre9n
Note
When redeploying a deployment to prod, by default it will automatically configure it to use the prod database. You can combine both --prod and --db options to opt out of this behavior. For example, the following command will redeploy the current production deployment (given the lack of positional argument, --id or --last ). The new deployment will become the new production deployment, but it will use the preview database instead of the production database:
deployctl deployments redeploy --prod --db = preview
If your organization has custom databases, you can also set them by UUID:
deployctl deployments redeploy --last --db = 5261e096-f9aa-4b72-8440-1c2b5b553def
Environment Variables Jump to heading #
When a deployment is created, it inherits the environment variables of the project. Given that the deployments are immutable, their environment variables can never be changed. To set new environment variables in a deployment, you need to redeploy it using --env (to set individual variables) and --env-file (to load one or more environment files).
The following command redeploys the current production deployment with the env variables defined in the .env and .other-env files, plus the DEPLOYMENT_TS variable set to the current timestamp. The resulting deployment will be a preview deployment (ie the production domains won't route traffic to it, given the lack of --prod ).
deployctl deployments redeploy --env-file --env-file = .other-env --env = DEPLOYMENT_TS = $( date +%s )
Note
Be aware that when changing env variables, only the env variables set in the redeploy command will be used by the new deployment. The project env variables and the env variables of the deployment being redeployed are ignored. If this does not suit your needs, please report your feedback at https://github.com/denoland/deploy_feedback/issues/
Note
When you change the project environment variables in the Deno Deploy Classic web application, the current production deployment is redeployed with the new environment variables, and the new deployment becomes the new production deployment.
Delete Jump to heading #
You can delete a deployment using the delete subcommand:
deployctl deployments delete 64tbrn8jre9n
Like show and redeploy , delete can also use --last , --next and --prev to select the deployment to delete. Here's an example command that deletes all the deployments of a project except the last (use with caution!):
while deployctl deployments delete --project = my-project --last --prev ; do : ; done
Projects Jump to heading #
The projects subcommand groups all the operations against projects as a whole. this includes list , show , rename , create and delete .
List Jump to heading #
deployctl projects list outputs all the projects your user has access to, grouped by organization:
Personal org: blog url-shortener 'my-team' org: admin-site main-site analytics
You can filter by organization using --org :
deployctl projects list --org = my-team
Show Jump to heading #
To see the details of a particular project, use projects show . If you are inside a project, it will pick up the project id from the config file. You can also specify the project using --project or the positional argument:
deployctl projects show main-site
Output:
main-site --------- Organization: my-team (5261e096-f9aa-4b72-8440-1c2b5b553def) Domain(s): https://my-team.com https://main-site.deno.dev Dash URL: https://dash.deno.com/projects/8422c515-f68f-49b2-89f3-157f4b144611 Repository: https://github.com/my-team/main-site Databases: [main] dd28e63e-f495-416b-909a-183380e3a232 [*] e061c76e-4445-409a-bc36-a1a9040c83b3 Crons: another cron [*/10 * * * *] succeeded at 12/3/2024 14:40:00 CET after 2 seconds (next at 12/3/2024 14:50:00 CET) newest cron [*/10 * * * *] n/a yet another cron [*/10 * * * *] failed at 12/3/2024 14:40:00 CET after 2 seconds (next at 12/3/2024 14:50:00 CET) Deployments: kcbxc4xwe4mc c0ph5xa9exb3* kwkbev9er4h2 dxseq0jc8402 7xr5thz8yjbz 4qr4h5ac3rfn 25wryhcqmb9q 64tbrn8jre9n hgqgccnmzg04 rxkh1w3g74e8 wx6cw9aya64c a1qh5fmew2yf w6pf4r0rrdkb nn700gexgdzq 98crfqxa6vvf xcdcs014yc5p btw43kx89ws1 62tg1ketkjx7 07ag6pt6kjex 4msyne1rvwj1
Rename Jump to heading #
Projects can be renamed easily with the rename subcommand. Similarly to the other commands, if you run the command from within a project's directory, you don't need to specify the current name of the project:
deployctl projects rename my-personal-blog
Output:
 Using config file '/private/tmp/blog/deno.json'  Project 'blog' (8422c515-f68f-49b2-89f3-157f4b144611) found  Project 'blog' renamed to 'my-personal-blog'
Note
Keep in mind that the name of the project is part of the preview domains ( https://my-personal-blog-kcbxc4xwe4mc.deno.dev ) and the default production domain ( https://my-personal-blog.deno.dev ). Therefore, when changing the project name, the URLs with the previous name will no longer route to the project's corresponding deployments.
Create Jump to heading #
You can create an empty project with:
deployctl projects create my-new-project
Delete Jump to heading #
You can delete a project with:
deployctl projects delete my-new-project
Top Jump to heading #
The top subcommand is used to monitor the resource usage of a project in real-time:
deployctl top
Output:
  (idx)  deployment  region  Req/min  CPU%  CPU/req  RSS/5min  Ingress/min  Egress/min  KVr/min  KVw/min  QSenq/min  QSdeq/min    6b80e8  "kcbxc4xwe4mc"  "asia-northeast1"  80  0.61  4.56  165.908  11.657  490.847  0  0  0  0   08312f  "kcbxc4xwe4mc"  "asia-northeast1"  76  3.49  27.58  186.278  19.041  3195.288  0  0  0  0   77c10b  "kcbxc4xwe4mc"  "asia-south1"  28  0.13  2.86  166.806  7.354  111.478  0  0  0  0   15e356  "kcbxc4xwe4mc"  "asia-south1"  66  0.97  8.93  162.288  17.56  4538.371  0  0  0  0   a06817  "kcbxc4xwe4mc"  "asia-southeast1"  126  0.44  2.11  140.087  16.504  968.794  0  0  0  0   d012b6  "kcbxc4xwe4mc"  "asia-southeast1"  119  2.32  11.72  193.704  23.44  8359.829  0  0  0  0   7d9a3d  "kcbxc4xwe4mc"  "australia-southeast1"  8  0.97  75  158.872  10.538  3.027  0  0  0  0   3c21be  "kcbxc4xwe4mc"  "australia-southeast1"  1  0.04  90  105.292  0.08  1.642  0  0  0  0   b75dc7  "kcbxc4xwe4mc"  "europe-west2"  461  5.43  7.08  200.573  63.842  9832.936  0  0  0  0   33607e  "kcbxc4xwe4mc"  "europe-west2"  35  0.21  3.69  141.98  9.438  275.788  0  0  0  0   9be3d2  "kcbxc4xwe4mc"  "europe-west2"  132  0.92  4.19  180.654  15.959  820.513  0  0  0  0   33a859  "kcbxc4xwe4mc"  "europe-west3"  1335  7.57  3.4  172.032  178.064  10967.918  0  0  0  0   3f54ce  "kcbxc4xwe4mc"  "europe-west4"  683  4.76  4.19  187.802  74.696  7565.017  0  0  0  0   cf881c  "kcbxc4xwe4mc"  "europe-west4"  743  3.95  3.19  177.213  86.974  6087.454  0  0  0  0   b4565b  "kcbxc4xwe4mc"  "me-west1"  3  0.21  55  155.46  2.181  0.622  0  0  0  0   b97970  "kcbxc4xwe4mc"  "southamerica-east1"  3  0.08  25  186.049  1.938  0.555  0  0  0  0   fd7a08  "kcbxc4xwe4mc"  "us-east4"  3  0.32  80  201.101  0.975  58.495  0  0  0  0   95d68a  "kcbxc4xwe4mc"  "us-east4"  133  1.05  4.77  166.052  28.107  651.737  0  0  0  0   c473e7  "kcbxc4xwe4mc"  "us-east4"  0  0  0  174.154  0.021  0  0  0  0  0   ebabfb  "kcbxc4xwe4mc"  "us-east4"  19  0.15  4.78  115.732  7.764  67.054  0  0  0  0   eac700  "kcbxc4xwe4mc"  "us-south1"  114  2.37  12.54  183.001  18.401  22417.397  0  0  0  0   cd2194  "kcbxc4xwe4mc"  "us-south1"  35  0.33  5.68  145.871  8.142  91.236  0  0  0  0   140fec  "kcbxc4xwe4mc"  "us-west2"  110  1.43  7.84  115.298  18.093  977.993  0  0  0  0   51689f  "kcbxc4xwe4mc"  "us-west2"  1105  7.66  4.16  187.277  154.876  14648.383  0  0  0  0   c5806e  "kcbxc4xwe4mc"  "us-west2"  620  4.38  4.24  192.291  109.086  9685.688  0  0  0  0    Streaming...
The columns are defined as follows:
Column Description
idx Instance discriminator. Opaque id to discriminate different executions running in the same region.
deployment The id of the deployment running in the executing instance.
Req/min Requests per minute received by the project.
CPU% Percentage of CPU used by the project.
CPU/req CPU time per request, in milliseconds.
RSS/5min Max RSS used by the project during the last 5 minutes, in MB.
Ingress/min Data received by the project per minute, in KB.
Egress/min Data output by the project per minute, in KB.
KVr/min KV reads performed by the project per minute.
KVw/min KV writes performed by the project per minute.
QSenq/min Queues enqueues performed by the project per minute.
QSdeq/min Queues dequeues performed by the project per minute.
You can filter by region using --region , which accepts substrings and can be used multiple times:
deployctl top --region = asia --region = southamerica
Logs Jump to heading #
You can fetch the logs of your deployments with deployctl logs . It supports both live logs where the logs are streamed to the console as they are generated, and query persisted logs where the logs generated in the past are fetched.
To show the live logs of the current production deployment of a project:
deployctl logs
Note
Unlike in the Deno Deploy Classic web application, at the moment the logs subcommand does not automatically switch to the new production deployment when it changes.
To show the live logs of a particular deployment:
deployctl logs --deployment = 1234567890ab
Logs can be filtered by level, region and text using --levels --regions and --grep options:
deployctl logs --levels = error,info --regions = region1,region2 --grep = 'unexpected'
To show the persisted logs, use the --since and/or --until options:
deployctl logs --since = $( date -Iseconds -v-2H ) --until = $( date -Iseconds -v-30M )
deployctl logs --since = $( date -Iseconds --date = '2 hours ago' ) --until = $( date -Iseconds --date = '30 minutes ago' )
API Jump to heading #
If you use the subhosting API , deployctl api will help you interact with the API by handling the authentication and headers for you:
deployctl api /projects/my-personal-blog/deployments
Use --method and --body to specify the HTTP method and the request body:
deployctl api --method = POST --body = '{"name": "main-site"}' organizations/5261e096-f9aa-4b72-8440-1c2b5b553def/projects
Local Development Jump to heading #
For local development you can use the deno CLI. To install deno , follow the instructions in the Deno manual .
After installation, you can run your scripts locally:
$ deno run --allow-net = :8000 ./main.ts Listening on http://localhost:8000
To watch for file changes add the --watch flag:
$ deno run --allow-net = :8000 --watch ./main.ts Listening on http://localhost:8000
For more information about the Deno CLI, and how to configure your development environment and IDE, visit the Deno Manual's Getting Started section.
JSON output Jump to heading #
All the commands that output data have a --format=json option that outputs the data in JSON objects. This output mode is the default when stdout is not a TTY, notably when piping to another command. Together with jq , this mode enables the programmatic use of all the data provided by deployctl :
Get the id of the current production deployment:
deployctl deployments show | jq .build.deploymentId
Get a csv stream of the CPU time per request on each isolate of each region:
deployctl top | jq -r '[.id,.region,.cpuTimePerRequest] | @csv'


---

URL: https://docs.deno.com/deploy/classic/environment-variables/
TITLE: Environment variables

Environment variables
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Environment variables are useful to store values like access tokens of web services. Each deployment has a set of environment variables defined at the moment of creation and accessible from the code via the Deno.env API. There are 2 ways to define the environment variables of a deployment:
Project environment variables Jump to heading #
You can define environment variables at the project level. When you create a deployment, it will get the set of environment variables the project has defined at that particular moment .
For convenience, When you change the environment variables of a project, the current production deployment is redeployed , creating a new production deployment with the new set of environment variables.
Note
Deployments are immutable, including their environment variables. Changing the environment variables of a project does not change the environment variables of existing deployments.
To add an environment variable to your project, click on the Settings button on the project page and then on Environment Variables from the sidebar. Fill in the key/value fields and click on "Add" to add an environment variable to your project.
Updating an existing environment variable works the same way. Click on the "Add Variable" button, enter the same name of the environment variable you wish to update and enter the new value. Click on the "Save" button to complete the update.
Deployment environment variables Jump to heading #
When deploying using deployctl , you can specify environment variables using the --env or --env-file flags , complementing the environment variables already defined for the project. You can also pass multiple --env-file arguments (e.g., --env-file=.env.one --env-file=.env.two ) to include variables from multiple files.
Note
When multiple declarations for the same environment variable exist within a single .env file, the first occurrence is applied. However, if the same variable is defined across multiple .env files (using multiple --env-file arguments), the value from the last file specified takes precedence. This means that the first occurrence found in the last .env file listed will be applied.
These env variables will be specific for the deployment being created.
Default environment variables Jump to heading #
Every deployment has the following environment variables preset, which you can access from your code.
- 
DENO_REGION
It holds the region code of the region in which the deployment is running. You can use this variable to serve region-specific content.
You can refer to the region code from the regions page .
- 
DENO_DEPLOYMENT_ID
It holds the ID of the deployment.


---

URL: https://docs.deno.com/deploy/classic/playgrounds/
TITLE: Playgrounds

Playgrounds
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Playgrounds are an easy way to play around with Deno Deploy, and to create small projects. Using playgrounds you can write code, run it, and see the output fully inside the browser.
Playgrounds have the full power of Deno Deploy: they support all the same features as a normal project, including environment variables, custom domains, and logs.
Playgrounds are also just as performant as all other projects on Deno Deploy: they make full use of our global network to run your code as close to users as possible.
- Creating a playground
- Using the playground editor
- Making a playground public
- Exporting a playground to GitHub
Creating a playground Jump to heading #
To create a new playground press the New Playground button in the top right corner of the project overview page .
This will create a new playground with a randomly generated name. You can change this name in the project settings later.
Using the playground editor Jump to heading #
The playground editor is opened automatically when you create a new playground. You can also open it by navigating to your project's overview page and clicking the Edit button.
The editor consists of two main areas: the editor on the left, and the preview panel on the right. The editor is where you write your code, and the preview panel is where you can see the output of your code through a browser window.
There is also a logs panel underneath the editor panel on the left side. This panel shows the console output of your code, and is useful for debugging your code.
After editing your code, you need to save and deploy it so the preview on the right updates. You can do this by clicking the Save & Deploy button in the top right, by pressing Ctrl + S , or opening the command palette with F1 and selecting Deploy: Save & Deploy .
In the tool bar in the top right of the editor you can see the current deployment status of your project while saving.
The preview panel on the right will refresh automatically every time you save and deploy your code.
The language dropdown in the top right of the editor allows you to switch between JavaScript, JSX, TypeScript, and TSX. The default selected language is TSX which will work for most cases.
Making a playground public Jump to heading #
Playgrounds can be shared with other users by making them public. This means that anyone can view the playground and its preview. Public playgrounds can not be edited by anyone: they can still only be edited by you. Logs are also only shown to you. Users have the option to fork a public playground to make a private copy of it that they can edit.
To make a playground public, press the Share button in the top tool bar in the editor. The URL to your playground will be copied to your clipboard automatically.
You can also change the playground visibility from the playground settings page in the Deno Deploy Classic dashboard. This can be used to change the visibility of a playground from public to private again.
Exporting a playground to GitHub Jump to heading #
Playgrounds can be exported to GitHub. This is useful if your project is starting to outgrow the single file limit of the playground editor.
Doing this will create a new GitHub repository containing the playground code. This project will be automatically turned into a git project that is linked to this new GitHub repository. Environment variables and domains will be retained.
The new GitHub repository will be created in your personal account, and will be set to private. You can change these settings later in the GitHub repository settings.
After exporting a playground, you can no longer use the Deno Deploy Classic playground editor for this project. This is a one-way operation.
To export the playground visit the playground settings page in the Deno Deploy dashboard or select Deploy: Export to GitHub from the command palette (press F1 in the editor).
Here you can enter a name for the new GitHub repository. This name will be used to create the repository on GitHub. The repository must not already exist.
Press Export to export the playground to GitHub.


---

URL: https://docs.deno.com/deploy/classic/api/runtime-response/
TITLE: HTTP Response

HTTP Response
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Response interface is part of the Fetch API and represents a response resource of fetch().
- Constructor
- Parameters
- Properties
- Methods
- Example
Constructor Jump to heading #
The Response() constructor creates a new Response instance.
let response = new Response ( body , init ) ;
Parameters Jump to heading #
name type optional description
body Blob , BufferSource , FormData , ReadableStream , URLSearchParams , or USVString true The body of the response. The default value is null .
init ResponseInit true An optional object that allows setting status and headers of the response.
The return type is a Response instance.
ResponseInit Jump to heading #
name type optional description
status number true The status code of the response.
statusText string true The status message representative of the status code.
headers Headers or string[][] or Record<string, string> false The HTTP headers of the response.
Properties Jump to heading #
name type read only description
body ReadableStream true The getter exposes a ReadableStream of the body contents.
bodyUsed boolean true Indicates whether the body content is read.
url USVString true The URL of the response.
headers Headers true The headers associated with the response.
ok boolean true Indicates if the response is successful (200-299 status).
redirected boolean true Indicates if the response is the result of a redirect.
status number true The status code of the response
statusText string true The status message of the response
type string true The type of the response.
Methods Jump to heading #
name description
arrayBuffer() Reads the body stream to its completion and returns an ArrayBuffer object.
blob() Reads the body stream to its completion and returns a Blob object.
formData() Reads the body stream to its completion and returns a FormData object.
json() Reads the body stream to its completion, parses it as JSON and returns a JavaScript object.
text() Reads the body stream to its completion and returns a USVString object (text).
clone() Clones the response object.
error() Returns a new response object associated with a network error.
redirect(url: string, status?: number) Creates a new response that redirects to the provided URL.
Example Jump to heading #
function handler ( _req ) { // Create a response with html as its body. const response = new Response ( "<html> Hello </html>" , { status : 200 , headers : { "content-type" : "text/html" , } , } ) ; console . log ( response . status ) ; // 200 console . log ( response . headers . get ( "content-type" ) ) ; // text/html return response ; } Deno . serve ( handler ) ;


---

URL: https://docs.deno.com/deploy/classic/api/runtime-fetch/
TITLE: HTTP requests (fetch)

HTTP requests (fetch)
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Fetch API allows you to make outbound HTTP requests in Deno Deploy Classic. It is a web standard and has the following interfaces:
- fetch() - The method that allows you to make outbound HTTP requests
- Request - represents a request resource of fetch()
- Response - represents a response resource of fetch()
- Headers - represents HTTP Headers of requests and responses.
This page shows usage for the fetch() method. You can click above on the other interfaces to learn more about them.
Fetch also supports fetching from file URLs to retrieve static files. For more info on static files, see the filesystem API documentation .
fetch() Jump to heading #
The fetch() method initiates a network request to the provided resource and returns a promise that resolves after the response is available.
function fetch ( resource : Request | string , init ? : RequestInit , ) : Promise < Response > ;
Parameters Jump to heading #
name type optional description
resource Request
USVString false The resource can either be a request object or a URL string.
init RequestInit true The init object lets you apply optional parameters to the request.
The return type of fetch() is a promise that resolves to a Response .
Examples Jump to heading #
The Deno Deploy Classic script below makes a fetch() request to the GitHub API for each incoming request, and then returns that response from the handler function.
async function handler ( req : Request ) : Promise < Response > { const resp = await fetch ( "https://api.github.com/users/denoland" , { // The init object here has an headers object containing a // header that indicates what type of response we accept. // We're not specifying the method field since by default // fetch makes a GET request. headers : { accept : "application/json" , } , } ) ; return new Response ( resp . body , { status : resp . status , headers : { "content-type" : "application/json" , } , } ) ; } Deno . serve ( handler ) ;


---

URL: https://docs.deno.com/deploy/classic/neon-postgres/
TITLE: Connect to Neon Postgres

Connect to Neon Postgres
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
This tutorial covers how to connect to a Neon Postgres database from an application deployed on Deno Deploy.
Setup Postgres Jump to heading #
To get started, we need to create a new Postgres instance for us to connect to. For this tutorial, we will be using Neon Postgres as they provide free, managed Postgres instances. If you like to host your database somewhere else, you can do that too.
- 
Visit https://neon.tech/ and click Sign up to sign up with an email, Github, Google, or partner account. After signing up, you are directed to the Neon Console to create your first project.
- 
Enter a name for your project, select a Postgres version, provide a database name, and select a region. Generally, you'll want to select the region closest to your application. When you're finished, click Create project .
- 
You are presented with the connection string for your new project, which you can use to connect to your database. Save the connection string, which looks something like this:
postgres://alex:AbC123dEf@ep-cool-darkness-123456.us-east-2.aws.neon.tech/dbname?sslmode = require
You will need the connection string in the next step.
Create a project in Deno Deploy Jump to heading #
Next, let's create a project in Deno Deploy Classic and set it up with the requisite environment variables:
- Go to https://dash.deno.com/new (Sign in with GitHub if you didn't already) and click on Create an empty project under Deploy your own code .
- Now click on the Settings button available on the project page.
- Navigate to Environment Variables Section and add the following secret.
- DATABASE_URL - The value should be set to the connection string you saved in the last step.
Write code that connects to Postgres Jump to heading #
To read/write to Postgres using the Neon serverless driver , first install it using the deno add command:
deno add jsr:@neon/serverless
This will create or update your deno.json file with the dependency:
{ "imports" : { "@neon/serverless" : "jsr:@neon/serverless@^0.10.1" } }
Now you can use the driver in your code:
import { neon } from "@neon/serverless" ; // Get the connection string from the environment variable "DATABASE_URL" const databaseUrl = Deno . env . get ( "DATABASE_URL" ) ! ; // Create a SQL query executor const sql = neon ( databaseUrl ) ; try { // Create the table await sql ` CREATE TABLE IF NOT EXISTS todos ( id SERIAL PRIMARY KEY, title TEXT NOT NULL ) ` ; } catch ( error ) { console . error ( error ) ; }
Deploy application to Deno Deploy Classic Jump to heading #
Once you have finished writing your application, you can deploy it on Deno Deploy Classic.
To do this, go back to your project page at https://dash.deno.com/projects/<project-name> .
You should see a couple of options to deploy:
- Github integration
- deployctl
deployctl deploy --project = < project-name > < application-file-name >
Unless you want to add a build step, we recommend that you select the GitHub integration.
For more details on the different ways to deploy on Deno Deploy Classic and the different configuration options, read here .


---

URL: https://docs.deno.com/deploy/classic/api/
TITLE: API Reference

API Reference
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
This is a reference for runtime APIs available on Deno Deploy Classic. This API is very similar to the standard runtime API , but some APIs are not available in the same way, given that Deno Deploy Classic is a serverless environment.
Please use this section of the documentation to explore available APIs on Deno Deploy.
Web APIs Jump to heading #
- console
- atob
- btoa
- Fetch API
- fetch
- Request
- Response
- URL
- File
- Blob
- TextEncoder
- TextDecoder
- TextEncoderStream
- TextDecoderStream
- Performance
- Web Crypto API
- randomUUID()
- getRandomValues()
- SubtleCrypto
- WebSocket API
- Timers ( setTimeout , clearTimeout , and setInterval )
- Streams API
- ReadableStream
- WritableStream
- TransformStream
- URLPattern API
- Import Maps
- Note: import maps are currently only available via deployctl or deployctl GitHub Action workflows.
Deno APIs Jump to heading #
Note: only stable APIs of Deno are made available in Deploy.
- Deno.env - Interact with environment variables (secrets).
- get(key: string): string | undefined - get the value of an environment variable.
- toObject(): { [key: string]: string } - get all environment variables as an object.
- Deno.connect - Connect to TCP sockets.
- Deno.connectTls - Connect to TCP sockets using TLS.
- Deno.startTls - Start TLS handshake from an existing TCP connection.
- Deno.resolveDns - Make DNS queries
- File system API
- Deno.cwd - Get the current working directory
- Deno.readDir - Get directory listings
- Deno.readFile - Read a file into memory
- Deno.readTextFile - Read a text file into memory
- Deno.open - Open a file for streaming reading
- Deno.stat - Get file system entry information
- Deno.lstat - Get file system entry information without following symlinks
- Deno.realPath - Get the real path of a file after resolving symlinks
- Deno.readLink - Get the target path for the given symlink
Future support Jump to heading #
In the future, these APIs will also be added:
- Cache API
- UDP API:
- Deno.connectDatagram for outbound UDP sockets
- Customizable fetch options using Deno.createHttpClient
Limitations Jump to heading #
Just like the Deno CLI, we do not implement the __proto__ object field as specified in ECMA Script Annex B.


---

URL: https://docs.deno.com/deploy/classic/organizations/
TITLE: Organizations

Organizations
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Organizations allow you to collaborate with other users. A project created in an organization is accessible to all members of the organization. Users should first signup for Deno Deploy Classic before they can be added to an organization.
Currently, all organization members have full access to the organization. They can add/remove members, and create/delete/modify all projects in the organization.
Create an organization Jump to heading #
- On your Classic dashboard, click on the organization dropdown in the top left of the screen, in the navigation bar.
- Select Organization + .
- Enter a name for your organization and click on Create .
Add members Jump to heading #
- Select the desired organization in the organization dropdown in the top left of the screen, in the navigation bar.
- Click on the Members icon button.
- Under the Members panel, click on + Invite member .
Note: Users should first signup for Deno Deploy Classic using this link before you invite them.
- Enter the GitHub username of the user and click on Invite .
Deno Deploy Classic will send the user an invite email. They can then can either accept or decline your invite. Once they accept the invite, they're added to your organization and shown in the members panel.
Pending invites are displayed in the Invites panel. You can revoke pending invites by clicking on the delete icon next to the pending invite.
Remove members Jump to heading #
- Select the desired organization in the organization dropdown in the top left of the screen, in the navigation bar.
- Click on the Members icon button.
- In the Members panel, click on the delete button beside the user you want to remove.


---

URL: https://docs.deno.com/deploy/classic/regions/
TITLE: Regions

Regions
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic deploys your code throughout the world. Each new request is served from the closest region to your user. Deno Deploy Classic is presently located in the following regions:
- Singapore ( asia-southeast1 )
- London ( europe-west2 )
- Frankfurt ( europe-west3 )
- So Paulo ( southamerica-east1 )
- North Virginia ( us-east4 )
- California ( us-west2 )
This list will be maintained to reflect the latest summary of our regions.
Code is deployed to all regions and is served from the region closest to the end user to minimize latency. It is not currently possible to restrict the regions in which your code is deployed.


---

URL: https://docs.deno.com/deploy/classic/api/compression/
TITLE: Compressing response bodies

Compressing response bodies
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Compressing the response body to save bandwidth is a common practice. To take some work off your shoulder, we built the capabilities directly into Deploy.
Deno Deploy Classic supports brotli and gzip compression. Compression is applied when the following conditions are met.
- The request to your deployment has Accept-Encoding header set to either br (brotli) or gzip .
- The response from your deployment includes the Content-Type header.
- The provided content type is compressible; we use this database to determine if the content type is compressible.
- The response body size is greater than 20 bytes.
When Deploy compresses the response body, it will set Content-Encoding: gzip or Content-Encoding: br header to the response based on the compression algorithm used.
When is compression skipped? Jump to heading #
Deno Deploy Classic skips the compression if:
- The response has Content-Encoding header.
- The response has Content-Range header.
- The response's Cache-Control header has no-transform value (e.g. cache-control: public, no-transform ).
What happens to my Etag header? Jump to heading #
When you set an Etag header with the response, we convert the header value to a Weak Etag if we apply compression to your response body. If it is already a Weak Etag, we don't touch the header.


---

URL: https://docs.deno.com/deploy/classic/api/runtime-headers/
TITLE: HTTP Headers

HTTP Headers
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Headers interface is part of the Fetch API. It allows you create and manipulate the HTTP headers of request and response resources of fetch().
- Constructor
- Parameters
- Methods
- Example
Constructor Jump to heading #
The Header() constructor creates a new Header instance.
let headers = new Headers ( init ) ;
Parameters Jump to heading #
name type optional description
init Headers / { [key: string]: string } true The init option lets you initialize the headers object with an existing Headers or an object literal.
The return type of the constructor is a Headers instance.
Methods Jump to heading #
name description
append(name: string, value: string) Appends a header (overwrites existing one) to the Headers object.
delete(name: string) Deletes a header from the Headers object.
set(name: string, value: string) Create a new header in the Headers object.
get(name: string) Get the value of the header in the Headers object.
has(name: string) Check if the header exists in the Headers objects.
entries() Get the headers as key-value pair. The result is iterable.
keys() Get all the keys of the Headers object. The result is iterable.
Example Jump to heading #
// Create a new headers object from an object literal. const myHeaders = new Headers ( { accept : "application/json" , } ) ; // Append a header to the headers object. myHeaders . append ( "user-agent" , "Deno Deploy Classic" ) ; // Print the headers of the headers object. for ( const [ key , value ] of myHeaders . entries ( ) ) { console . log ( key , value ) ; } // You can pass the headers instance to Response or Request constructors. const request = new Request ( "https://api.github.com/users/denoland" , { method : "POST" , headers : myHeaders , } ) ;


---

URL: https://docs.deno.com/deploy/classic/api/runtime-node/
TITLE: Node.js built-in APIs

Node.js built-in APIs
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic natively supports importing built-in Node.js modules like fs , path , and http through node: specifiers. This allows running code originally written for Node.js without changes in Deno Deploy Classic.
Here is an example of a Node.js HTTP server running on Deno Deploy Classic:
import { createServer } from "node:http" ; import process from "node:process" ; const server = createServer ( ( req , res ) => { const message = ` Hello from ${ process . env . DENO_REGION } at ${ new Date ( ) } ` ; res . end ( message ) ; } ) ; server . listen ( 8080 ) ;
When using node: specifiers, all other features of Deno Deploy Classic are still available. For example, you can use Deno.env to access environment variables even when using Node.js modules. You can also import other ESM modules from external URLs as usual.
The following Node.js modules are available:
- assert
- assert/strict
- async_hooks
- buffer
- child_process
- cluster
- console
- constants
- crypto
- dgram
- diagnostics_channel
- dns
- dns/promises
- domain
- events
- fs
- fs/promises
- http
- http2
- https
- module
- net
- os
- path
- path/posix
- path/win32
- perf_hooks
- process
- punycode
- querystring
- readline
- stream
- stream/consumers
- stream/promises
- stream/web
- string_decoder
- sys
- timers
- timers/promises
- tls
- tty
- url
- util
- util/types
- v8
- vm
- worker_threads
- zlib
The behavior of these modules should be identical to Node.js in most cases. Due to the sandboxing behaviour of Deno Deploy Classic, some features are not available:
- Executing binaries with child_process
- Spawning workers using worker_threads
- Creating contexts and evaluating code with vm
Note: the emulation of Node.js modules is sufficient for most use cases, but it is not yet perfect. If you encounter any issues, please open an issue .


---

URL: https://docs.deno.com/deploy/classic/use-cases/
TITLE: Deno Deploy Classic Use Cases

Deno Deploy Classic Use Cases
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Some popular use-cases for Deno currently are:
- Middleware
- API servers
- Full websites
Middleware Jump to heading #
Middleware refers to bits of code that execute before and after the request gets to the application server. You'll be writing middleware if you want to execute some JavaScript or any other code very fast, early in the request. By deploying your middleware code at the edge, Deno Deploy Classic ensures the best performance for your app.
Some examples include:
- setting a cookie
- serving different versions of a site depending on geolocation
- path rewriting
- redirecting requests
- dynamically changing the HTML on its way back from the server before it gets to the user.
Deno Deploy Classic is a good alternative to other platforms you might be using to host your middleware right now, for example:
- Cloudflare Workers
- AWS Lambda@Edge
- Traditional load balancers like nginx
- Custom rules
API servers Jump to heading #
Deno is also a great fit for API servers. By deploying these servers "at the edge", closer to clients who are using them, Deno Deploy Classic is able to offer lower latency, improved performance, and reduced bandwidth costs compared to traditional hosting platforms like Heroku or even modern centralized hosting services like DigitalOcean.
Full websites Jump to heading #
We foresee a future where you can actually write your entire website on edge functions. Some examples of sites that are already doing this include:
- blog
- chat
- calendly clone


---

URL: https://docs.deno.com/deploy/classic/api/runtime-fs/
TITLE: File system APIs

File system APIs
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic supports a limited set of the file system APIs available in Deno. These file system APIs can access static files from your deployments. Static files are for example:
- The files in your GitHub repository, if you deploy via the GitHub integration.
- The entrypoint file in a playground deployment.
The APIs that are available are:
- Deno.cwd
- Deno.readDir
- Deno.readFile
- Deno.readTextFile
- Deno.open
- Deno.stat
- Deno.lstat
- Deno.realPath
- Deno.readLink
Deno.cwd Jump to heading #
Deno.cwd() returns the current working directory of your deployment. It is located at the root of your deployment's root directory. For example, if you deployed via the GitHub integration, the current working directory is the root of your GitHub repository.
Deno.readDir Jump to heading #
Deno.readDir() allows you to list the contents of a directory.
The function is fully compatible with Deno .
function Deno . readDir ( path : string | URL ) : AsyncIterable < DirEntry >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example lists the contents of a directory and returns this list as a JSON object in the response body.
async function handler ( _req ) { // List the posts in the `blog` directory located at the root // of the repository. const posts = [ ] ; for await ( const post of Deno . readDir ( ` ./blog ` ) ) { posts . push ( post ) ; } // Return JSON. return new Response ( JSON . stringify ( posts , null , 2 ) , { headers : { "content-type" : "application/json" , } , } ) ; } Deno . serve ( handler ) ;
Deno.readFile Jump to heading #
Deno.readFile() allows you to read a file fully into memory.
The function definition is similar to Deno , but it doesn't support ReadFileOptions for the time being. Support will be added in the future.
function Deno . readFile ( path : string | URL ) : Promise < Uint8Array >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example reads the contents of a file into memory as a byte array, then returns it as the response body.
async function handler ( _req ) { // Let's read the README.md file available at the root // of the repository to explore the available methods. // Relative paths are relative to the root of the repository const readmeRelative = await Deno . readFile ( "./README.md" ) ; // Absolute paths. // The content of the repository is available under at Deno.cwd(). const readmeAbsolute = await Deno . readFile ( ` ${ Deno . cwd ( ) } /README.md ` ) ; // File URLs are also supported. const readmeFileUrl = await Deno . readFile ( new URL ( ` file:// ${ Deno . cwd ( ) } /README.md ` ) , ) ; // Decode the Uint8Array as string. const readme = new TextDecoder ( ) . decode ( readmeRelative ) ; return new Response ( readme ) ; } Deno . serve ( handler ) ;
Note: to use this feature, you must link a GitHub repository to your project.
Deno Deploy Classic supports the Deno.readFile API to read static assets from the file system. This is useful for serving static assets such as images, stylesheets, and JavaScript files. This guide demonstrates how to use this feature.
Imagine the following file structure on a GitHub repository:
 mod.ts  style.css
The contents of mod.ts :
async function handleRequest ( request : Request ) : Promise < Response > { const { pathname } = new URL ( request . url ) ; // This is how the server works: // 1. A request comes in for a specific asset. // 2. We read the asset from the file system. // 3. We send the asset back to the client. // Check if the request is for style.css. if ( pathname . startsWith ( "/style.css" ) ) { // Read the style.css file from the file system. const file = await Deno . readFile ( "./style.css" ) ; // Respond to the request with the style.css file. return new Response ( file , { headers : { "content-type" : "text/css" , } , } ) ; } return new Response ( ` <html> <head> <link rel="stylesheet" href="style.css" /> </head> <body> <h1>Example</h1> </body> </html> ` , { headers : { "content-type" : "text/html; charset=utf-8" , } , } , ) ; } Deno . serve ( handleRequest ) ;
The path provided to the Deno.readFile API is relative to the root of the repository. You can also specify absolute paths, if they are inside Deno.cwd .
Deno.readTextFile Jump to heading #
This function is similar to Deno.readFile except it decodes the file contents as a UTF-8 string.
function Deno . readTextFile ( path : string | URL ) : Promise < string >
Example Jump to heading #
This example reads a text file into memory and returns the contents as the response body.
async function handler ( _req ) { const readme = await Deno . readTextFile ( "./README.md" ) ; return new Response ( readme ) ; } Deno . serve ( handler ) ;
Deno.open Jump to heading #
Deno.open() allows you to open a file, returning a file handle. This file handle can then be used to read the contents of the file. See Deno.File for information on the methods available on the file handle.
The function definition is similar to Deno , but it doesn't support OpenOptions for the time being. Support will be added in the future.
function Deno . open ( path : string | URL ) : Promise < Deno . File >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example opens a file, and then streams the content as the response body.
async function handler ( _req ) { // Open the README.md file available at the root of the repository. const file = await Deno . open ( "./README.md" ) ; // Use the `readable` property, which is a `ReadableStream`. This will // automatically close the file handle when the response is done sending. return new Response ( file . readable ) ; } Deno . serve ( handler ) ;
Note
When you iterate over a file stream as shown below, the file descriptor will be automatically closed at the end of iteration. There is no need to manually close the file descriptor: const iterator = fd.readable[Symbol.asyncIterator]();
Deno.File Jump to heading #
Deno.File is a file handle returned from Deno.open() . It can be used to read chunks of the file using the read() method. The file handle can be closed using the close() method.
The interface is similar to Deno , but it doesn't support writing to the file, or seeking. Support for the latter will be added in the future.
class File { readonly rid : number ; close ( ) : void ; read ( p : Uint8Array ) : Promise < number | null > ; }
The path can be a relative or absolute. It can also be a file: URL.
Deno.File#read() Jump to heading #
The read method is used to read a chunk of the file. It should be passed a buffer to read the data into. It returns the number of bytes read or null if the end of the file has been reached.
function read ( p : Uint8Array ) : Promise < number | null > ;
Deno.File#close() Jump to heading #
The close method is used to close the file handle. Closing the handle will interrupt all ongoing reads.
function close ( ) : void ;
Deno.stat Jump to heading #
Deno.stat() reads a file system entry's metadata. It returns a Deno.FileInfo object. Symlinks are followed.
The function definition is the same as Deno . It does not return modification time, access time, or creation time values.
function Deno . stat ( path : string | URL ) : Promise < Deno . FileInfo >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example gets the size of a file, and returns the result as the response body.
async function handler ( _req ) { // Get file info of the README.md at the root of the repository. const info = await Deno . stat ( "./README.md" ) ; // Get the size of the file in bytes. const size = info . size ; return new Response ( ` README.md is ${ size } bytes large ` ) ; } Deno . serve ( handler ) ;
Deno.lstat Jump to heading #
Deno.lstat() is similar to Deno.stat() , but it does not follow symlinks.
The function definition is the same as Deno . It does not return modification time, access time, or creation time values.
function Deno . lstat ( path : string | URL ) : Promise < Deno . FileInfo >
The path can be a relative or absolute. It can also be a file: URL.
Deno.FileInfo Jump to heading #
The Deno.FileInfo interface is used to represent a file system entry's metadata. It is returned by the Deno.stat() and Deno.lstat() functions. It can represent either a file, a directory, or a symlink.
In Deno Deploy Classic, only the file type, and size properties are available. The size property behaves the same way it does on Linux.
interface FileInfo { isDirectory : boolean ; isFile : boolean ; isSymlink : boolean ; size : number ; }
Deno.realPath Jump to heading #
Deno.realPath() returns the resolved absolute path to a file after following symlinks.
The function definition is the same as Deno .
function Deno . realPath ( path : string | URL ) : Promise < string >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example calls Deno.realPath() to get the absolute path of a file in the root of the repository. The result is returned as the response body.
async function handler ( _req ) { const path = await Deno . realPath ( "./README.md" ) ; return new Response ( ` The fully resolved path for ./README.md is ${ path } ` ) ; } Deno . serve ( handler ) ;
Deno.readLink Jump to heading #
Deno.readLink() returns the target path for a symlink.
The function definition is the same as Deno .
function Deno . readLink ( path : string | URL ) : Promise < string >
The path can be a relative or absolute. It can also be a file: URL.
Example Jump to heading #
This example calls Deno.readLink() to get the absolute path of a file in the root of the repository. The result is returned as the response body.
async function handler ( _req ) { const path = await Deno . readLink ( "./my_symlink" ) ; return new Response ( ` The target path for ./my_symlink is ${ path } ` ) ; } Deno . serve ( handler ) ;


---

URL: https://docs.deno.com/deploy/classic/dynamodb/
TITLE: Connect to DynamoDB

Connect to DynamoDB
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Amazon DynamoDB is a fully managed NoSQL database. To persist data to DynamoDB, follow the steps below:
The tutorial assumes that you have an AWS and Deno Deploy Classic account.
Gather credentials from DynamoDB Jump to heading #
The first step in the process is to generate AWS credentials to programmatically access DynamoDB.
Generate Credentials:
- Go to https://console.aws.amazon.com/iam/ and go to the "Users" section.
- Click on the Add user button, fill the User name field (maybe use denamo ), and select Programmatic access type.
- Click on Next: Permissions , then on Attach existing policies directly , search for AmazonDynamoDBFullAccess and select it.
- Click on Next: Tags , then on Next: Review and finally Create user .
- Click on Download .csv button to download the credentials.
Create a project in Deno Deploy Jump to heading #
Next, let's create a project in Deno Deploy Classic and set it up with the requisite environment variables:
- Go to https://dash.deno.com/new (Sign in with GitHub if you didn't already) and click on + Empty Project under Deploy from the command line .
- Now click on the Settings button available on the project page.
- Navigate to Environment Variables Section and add the following secrets.
- AWS_ACCESS_KEY_ID - Use the value that's available under Access key ID column in the downloaded CSV.
- AWS_SECRET_ACCESS_KEY - Use the value that's available under Secret access key column in the downloaded CSV.
Write code that connects to DynamoDB Jump to heading #
AWS has an official SDK that works with browsers. As most Deno Deploy's APIs are similar to browsers', the same SDK works with Deno Deploy. To use the SDK in Deno, import from a cdn like below and create a client:
import { DynamoDBClient , GetItemCommand , PutItemCommand , } from "https://esm.sh/@aws-sdk/client-dynamodb?dts" ; // Create a client instance by providing your region information. // The credentials are automatically obtained from environment variables which // we set during our project creation step on Deno Deploy, so we don't have to // pass them manually here. const client = new ApiFactory ( ) . makeNew ( DynamoDB ) ; serve ( { "/songs" : handleRequest , } ) ; async function handleRequest ( request ) { // async/await. try { const data = await client . send ( command ) ; // process data. } catch ( error ) { // error handling. } finally { // finally. } }
Deploy application to Deno Deploy Classic Jump to heading #
Once you have finished writing your application, you can deploy it on Deno Deploy Classic.
To do this, go back to your project page at https://dash.deno.com/projects/<project-name> .
You should see a couple of options to deploy:
- Github integration
- deployctl
deployctl deploy --project = < project-name > < application-file-name >
Unless you want to add a build step, we recommend that you select the Github integration.
For more details on the different ways to deploy on Deno Deploy Classic and the different configuration options, read here .


---

URL: https://docs.deno.com/deploy/classic/queues/
TITLE: Using Queues

Using Queues
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Deno runtime includes a queueing API that supports offloading larger workloads for async processing, with guaranteed at-least-once delivery of queued messages. Queues can be used to offload tasks in a web application, or to schedule units of work for a time in the future.
The primary APIs you'll use with queues are in the Deno.Kv namespace as enqueue and listenQueue .
Enqueue a message Jump to heading #
To enqueue a message for processing, use the enqueue method on an instance of Deno.Kv . In the example below, we show what it might look like to enqueue a notification for delivery.
queue_example.ts
// Describe the shape of your message object (optional) interface Notification { forUser : string ; body : string ; } // Get a reference to a KV instance const kv = await Deno . openKv ( ) ; // Create a notification object const message : Notification = { forUser : "alovelace" , body : "You've got mail!" , } ; // Enqueue the message for immediate delivery await kv . enqueue ( message ) ;
You can enqueue a message for later delivery by specifying a delay option in milliseconds.
// Enqueue the message for delivery in 3 days const delay = 1000 * 60 * 60 * 24 * 3 ; await kv . enqueue ( message , { delay } ) ;
You can also specify a key in Deno KV where your message value will be stored if your message isn't delivered for any reason.
// Configure a key where a failed message would be sent const backupKey = [ "failed_notifications" , "alovelace" , Date . now ( ) ] ; await kv . enqueue ( message , { keysIfUndelivered : [ backupKey ] } ) ; // ... disaster strikes ... // Get the unsent message const r = await kv . get < Notification > ( backupKey ) ; // This is the message that didn't get sent: console . log ( "Found failed notification for:" , r . value ?. forUser ) ;
Listening for messages Jump to heading #
You can configure a JavaScript function that will process items added to your queue with the listenQueue method on an instance of Deno.Kv .
listen_example.ts
// Define the shape of the object we expect as a message in the queue interface Notification { forUser : string ; body : string ; } // Create a type guard to check the type of the incoming message function isNotification ( o : unknown ) : o is Notification { return ( ( ( o as Notification ) ?. forUser !== undefined && typeof ( o as Notification ) . forUser === "string" ) && ( ( o as Notification ) ?. body !== undefined && typeof ( o as Notification ) . body === "string" ) ) ; } // Get a reference to a KV database const kv = await Deno . openKv ( ) ; // Register a handler function to listen for values - this example shows // how you might send a notification kv . listenQueue ( ( msg : unknown ) => { // Use type guard - then TypeScript compiler knows msg is a Notification if ( isNotification ( msg ) ) { console . log ( "Sending notification to user:" , msg . forUser ) ; // ... do something to actually send the notification! } else { // If the message is of an unknown type, it might be an error console . error ( "Unknown message received:" , msg ) ; } } ) ;
Queue API with KV atomic transactions Jump to heading #
You can combine the queue API with KV atomic transactions to atomically enqueue messages and modify keys in the same transaction.
kv_transaction_example.ts
const kv = await Deno . openKv ( ) ; kv . listenQueue ( async ( msg : unknown ) => { const nonce = await kv . get ( [ "nonces" , msg . nonce ] ) ; if ( nonce . value === null ) { // This messaged was already processed return ; } const change = msg . change ; const bob = await kv . get ( [ "balance" , "bob" ] ) ; const liz = await kv . get ( [ "balance" , "liz" ] ) ; const success = await kv . atomic ( ) // Ensure this message was not yet processed . check ( { key : nonce . key , versionstamp : nonce . versionstamp } ) . delete ( nonce . key ) . sum ( [ "processed_count" ] , 1n ) . check ( bob , liz ) // balances did not change . set ( [ "balance" , "bob" ] , bob . value - change ) . set ( [ "balance" , "liz" ] , liz . value + change ) . commit ( ) ; } ) ; // Modify keys and enqueue messages in the same KV transaction! const nonce = crypto . randomUUID ( ) ; await kv . atomic ( ) . check ( { key : [ "nonces" , nonce ] , versionstamp : null } ) . enqueue ( { nonce : nonce , change : 10 } ) . set ( [ "nonces" , nonce ] , true ) . sum ( [ "enqueued_count" ] , 1n ) . commit ( ) ;
Queue behavior Jump to heading #
Message delivery guarantees Jump to heading #
The runtime guarantees at-least-once delivery. This means that for majority of enqueued messages, the listenQueue handler will be invoked once for each message. In some failure scenarios, the handler may be invoked multiple times for the same message to ensure delivery. It's important to design your applications such that duplicate messages are handled correctly.
You may use queues in combination with KV atomic transactions primitives to ensure that your queue handler KV updates are performed exactly once per message. See Queue API with KV atomic transactions .
Automatic retries Jump to heading #
listenQueue handler is invoked to process your queued messages when they're ready for delivery. If your handler throws an exception the runtime will automatically retry to call the handler again until it succeeds or until maximum retry attempts are reached. The message is considered to be successfully processed once the listenQueue handler invocation completes successfully. The message will be dropped if the handler consistently fails on retries.
Message delivery order Jump to heading #
The runtime makes best effort to deliver messages in the order they were enqueued. However, there is not strict order guarantee. Occasionally, messages may be delivered out of order to ensure maximum throughput.
Queues on Deno Deploy Jump to heading #
Deno Deploy offers global, serverless, distributed implementation of the queueing API, designed for high availability and throughput. You can use it to build applications that scale to handle large workloads.
Just-in-time isolate spin-up Jump to heading #
When using queues with Deno Deploy, isolates are automatically spun up on demand to invoke your listenQueue handler when a message becomes available for processing. Defining listenQueue handler is the only requirement to enable queue processing in your Deno Deploy application, no additional configuration is needed.
Queue size limit Jump to heading #
The maximum number of undelivered queue messages is limited to 100,000. enqueue method will fail with an error if the queue is full.
Pricing details and limits Jump to heading #
- enqueue is treated just like other Deno.Kv write operations. Enqueued messages consume KV storage and write units.
- Messages delivered through listenQueue consume requests and KV write units.
- See Pricing details for more information.
Use cases Jump to heading #
Queues can be useful in many different scenarios, but there are a few use cases you might see a lot when building web applications.
Offloading async processes Jump to heading #
Sometimes a task that's initiated by a client (like sending a notification or API request), may take long enough where you don't want to make clients wait for that task to be completed before returning a response. Other times, clients don't actually need a response at all, such as when a client is sending your application a webhook request , so there's no need to wait for the underlying task to be completed before returning a response.
In these cases, you can offload work to a queue to keep your web application responsive and send immediate feedback to clients.
Scheduling work for the future Jump to heading #
Another helpful application of queues (and queue APIs like this one), is to schedule work to happen at an appropriate time in the future. Maybe you'd like to send a notification to a new customer a day after they have placed an order to send them a satisfaction survey. You can schedule a queue message to be delivered 24 hours into the future, and set up a listener to send out the notification at that time.


---

URL: https://docs.deno.com/deploy/classic/prisma-postgres/
TITLE: Connect to Prisma Postgres

Connect to Prisma Postgres
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
This tutorial covers how to connect to a Prisma Postgres database from an application deployed on Deno Deploy.
Setup Postgres Jump to heading #
There are several ways to set up a Prisma Postgre database for your Prisma project. This guide covers the most common approaches.
Method 1: Using Prisma CLI Jump to heading #
Run the following command to initialize a new Prisma project with a database:
npx prisma init --db
This will prompt you to select your preferred region and database name. Once completed, you'll find the DATABASE_URL connection string in your .env file.
Method 2: Using npx create-db Jump to heading #
Alternatively, you can use the dedicated database creation tool:
npx create-db@latest
This command will provide you with two connection strings tied to the same database:
Prisma ORM optimized connection string:
prisma+postgres://accelerate.prisma-data.net/?api_key=<api_key>
Standard Prisma Postgres connection string:
postgresql://<username>:<password>@db.prisma.io:5432/postgres
In order to keep the database created with npx create-db , you must follow through with the claim process. That can be done via the claim link provided in the terminal.
The Prisma ORM optimized connection string ( prisma+postgres:// ) only works with the Prisma ORM, while the standard Prisma Postgre connection string can be used with other database tools and libraries.
Create a project in Deno Deploy Jump to heading #
Next, let's create a project in Deno Deploy Classic and set it up with the requisite environment variables:
- Go to https://dash.deno.com/new (Sign in with GitHub if you didn't already) and click on Create an empty project under Deploy your own code .
- Now click on the Settings button available on the project page.
- Navigate to Environment Variables Section and add the following secret.
- DATABASE_URL - The value should be set to the connection string you saved in the last step.
Write code that connects to Postgres Jump to heading #
Now that you have your database set up, let's create a simple application that connects to the Prisma Postgres database using Prisma ORM.
1. Install dependencies Jump to heading #
First, install the required dependencies:
deno install npm:@prisma/client deno install npm:@prisma/extension-accelerate deno install npm:dotenv-cli
Note
The dotenv-cli package is needed because Prisma Client doesn't read .env files by default on Deno.
2. Create the database schema Jump to heading #
With your database connection configured, you can now apply the data model to your database:
deno run -A npm:prisma migrate dev --name init
This command creates a new SQL migration file and runs it against your database.
3. Update your Prisma schema Jump to heading #
Edit your prisma/schema.prisma file to define a Log model and configure it for Deno:
generator client { provider = "prisma-client" output = "../generated/prisma" runtime = "deno" } datasource db { provider = "postgresql" url = env ( "DATABASE_URL" ) } model Log { id Int @ id @ default ( autoincrement ( ) ) level Level message String meta Json } enum Level { Info Warn Error }
4. Create your application Jump to heading #
Create index.ts in your project root with the following content:
import { serve } from "https://deno.land/std@0.140.0/http/server.ts" ; import { withAccelerate } from "npm:@prisma/extension-accelerate" ; import { PrismaClient } from "./generated/prisma/client.ts" ; const prisma = new PrismaClient ( ) . $ extends ( withAccelerate ( ) ) ; async function handler ( request : Request ) { // Ignore /favicon.ico requests: const url = new URL ( request . url ) ; if ( url . pathname === "/favicon.ico" ) { return new Response ( null , { status : 204 } ) ; } const log = await prisma . log . create ( { data : { level : "Info" , message : ` ${ request . method } ${ request . url } ` , meta : { headers : JSON . stringify ( request . headers ) , } , } , } ) ; const body = JSON . stringify ( log , null , 2 ) ; return new Response ( body , { headers : { "content-type" : "application/json; charset=utf-8" } , } ) ; } serve ( handler ) ;
4. Test your application locally Jump to heading #
Start your application locally to test the database connection:
npx dotenv -- deno run -A ./index.ts
Visit http://localhost:8000 in your browser. Each request will create a new log entry in your database and return the log data as JSON.
Deploy application to Deno Deploy Classic Jump to heading #
Once you have finished writing your application, you can deploy it on Deno Deploy Classic.
To do this, go back to your project page at https://dash.deno.com/projects/<project-name> .
You should see a couple of options to deploy:
- Github integration
- deployctl
deployctl deploy --project = < project-name > < application-file-name >
Unless you want to add a build step, we recommend that you select the GitHub integration.
For more details on the different ways to deploy on Deno Deploy Classic and the different configuration options, read here .


---

URL: https://docs.deno.com/deploy/classic/api/dynamic-import/
TITLE: Dynamic import

Dynamic import
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic supports dynamic import but with some limitations. This page outlines these limitations.
Specifiers must be statically determined string literals Jump to heading #
In the usual dynamic import, specifiers don't need to be determined at build time. So all of the following forms are valid:
Valid dynamic imports in Deno CLI
// 1. Statically determined string literal await import ( "jsr:@std/assert" ) ; // 2. Statically determined, but via variable const specifier = "jsr:@std/assert" ; await import ( specifier ) ; // 3. Statically determined, but template literal const stdModuleName = "path" ; await import ( ` jsr:@std/ ${ stdModuleName } ` ) ; // 4. Dynamically determined const rand = Math . random ( ) ; const mod = rand < 0.5 ? "npm:cowsay" : "npm:node-emoji" ; await import ( mod ) ;
In Deno Deploy Classic, however, specifiers must be string literals with no string interpolation. So among the three examples above, only the first one works in Deno Deploy Classic.
Only static string literals work in Deno Deploy Classic
// 1.  Works fine on Deno Deploy Classic await import ( "jsr:@std/assert" ) ; // 2.  Doesn't work on Deno Deploy Classic // because what's passed to `import` is a variable const specifier = "jsr:@std/streams" ; await import ( specifier ) ; // 3.  Doesn't work on Deno Deploy Classic // because this has an interpolation const stdModuleName = "path" ; await import ( ` jsr:@std/ ${ stdModuleName } ` ) ; // 4.  Doesn't work on Deno Deploy Classic // because it's dynamic const rand = Math . random ( ) ; const mod = rand < 0.5 ? "npm:cowsay" : "npm:node-emoji" ; await import ( mod ) ;
One exception - dynamic specifiers work for same project files Jump to heading #
Specifiers that are dynamically determined are supported if target files (modules) are included in the same project.
Dynamic specifiers work for files in the same project
//  Works fine on Deno Deploy Classic await import ( "./my_module1.ts" ) ; //  Works fine on Deno Deploy Classic const rand = Math . random ( ) ; const modPath = rand < 0.5 ? "dir1/moduleA.ts" : "dir2/dir3/moduleB.ts" ; await import ( ` ./ ${ modPath } ` ) ;
Note that template literals starting with ./ tell the module resolver that the target module is in the same project. Conversely, if a specifier does not start with ./ , the possible target modules will not be included the resulting eszip , causing dynamic imports to fail at runtime, even if the final evaluated specifier starts with ./ .
//  Doesn't work because the analyzer can't statically determine if the // specifier starts with `./` or not in this case. // Compare this to the previous example. Only difference is whether to put // `./` in the template literal or in the variable. const rand = Math . random ( ) ; const modPath = rand < 0.5 ? "./dir1/moduleA.ts" : "./dir2/dir3/moduleB.ts" ; await import ( modPath ) ;
We will consider if we can relax this constraint in the future.
What is eszip?
When you do a new deployment on Deno Deploy Classic, the system analyzes your code, constructs the module graph by recursively traversing it, and bundles all the dependencies into a single file. We call this eszip . Since its creation is done completely statically, dynamic import capabilities are limited on Deno Deploy Classic.
Data URLs Jump to heading #
Data URL can be used as a specifier passed to dynamic imports.
Static data URL
//  Works fine on Deno Deploy Classic const { val } = await import ( "data:text/javascript,export const val = 42;" ) ; console . log ( val ) ; // -> 42
For data URLs, fully dynamic data is supported.
Dynamic data URL
function generateDynamicDataUrl ( ) { const moduleStr = ` export const val = ${ Math . random ( ) } ; ` ; return ` data:text/javascript, ${ moduleStr } ` ; } //  Works fine on Deno Deploy Classic const { val } = await import ( generateDynamicDataUrl ( ) ) ; console . log ( val ) ; // -> Random value is printed
Applying this technique to JavaScript code fetched from the web, you can even simulate a true dynamic import:
external.js
export const name = "external.js" ;
Dynamic data URL from fetched source
import { assert } from "jsr:@std/assert/assert" ; const res = await fetch ( "https://gist.githubusercontent.com/magurotuna/1cacb136f9fd6b786eb8bbad92c8e6d6/raw/56a96fd0d246fd3feabbeecea6ea1155bdf5f50d/external.js" , ) ; assert ( res . ok ) ; const src = await res . text ( ) ; const dataUrl = ` data:application/javascript, ${ src } ` ; //  Works fine on Deno Deploy Classic const { name } = await import ( dataUrl ) ; console . log ( ` Hello from ${ name } ` ) ; // -> "Hello from external.js"
However, note that data URL given to import has to be JavaScript; TypeScript, when passed, throws a TypeError at runtime.


---

URL: https://docs.deno.com/deploy/pricing_and_limits/
TITLE: Pricing and limitations

Pricing and limitations
Please see our pricing page for the overview of the available features in all plans. If you have a use case that exceeds any of these limits, please reach out .
No uptime guarantees are provided during the initial public beta for Deno Deploy. Access to the service will be controlled by our acceptable use policy . Any user we deem to be in violation of this policy, runs the risk of having their account terminated.
Maximum size for deployments Jump to heading #
When uploading assets to a deployment, the total size of all files within the deployment (source files and static files) should not exceed 1 gigabyte .
Memory allocation Jump to heading #
Applications have a maximum memory allocation of 512MB
Upload request limits Jump to heading #
We do not set a limit for the number of upload requests your application may handle as long as your application is within our acceptable use policy .
TLS proxying Jump to heading #
TLS termination is required for outgoing connections to port 443 (the port used for HTTPS). Using Deno.connect to connect to these ports is prohibited. If you need to establish a TLS connection to port 443, please use Deno.connectTls instead. fetch is not impacted by this restriction.
This restriction is in place because connecting to port 443 without terminating TLS is frequently used in TLS-over-TLS proxies, which are prohibited on Deno Deploy Classic as per our acceptable use policy .


---

URL: https://docs.deno.com/deploy/classic/cron/
TITLE: Scheduling cron tasks

Scheduling cron tasks
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Deno.cron interface enables you to configure JavaScript or TypeScript code that executes on a configurable schedule using cron syntax . In the example below, we configure a block of JavaScript code that will execute every minute.
Deno . cron ( "Log a message" , "* * * * *" , ( ) => { console . log ( "This will print once a minute." ) ; } ) ;
It's also possible to use JavaScript objects to define the cron schedule. In the example below, we configure a block of JavaScript code that will execute once an hour.
Deno . cron ( "Log a message" , { hour : { every : 1 } } , ( ) => { console . log ( "This will print once an hour." ) ; } ) ;
Deno.cron takes three arguments:
- A human-readable name for the cron task
- A cron schedule string or JavaScript object that defines a schedule on which the cron job will run
- a function to be executed on the given schedule
If you are new to cron syntax, there are a number of third party modules like this one that will help you generate cron schedule strings.
Retrying failed runs Jump to heading #
Failed cron invocations are automatically retried with a default retry policy. If you would like to specify a custom retry policy, you can use the backoffSchedule property to specify an array of wait times (in milliseconds) to wait before retrying the function call again. In the following example, we will attempt to retry failed callbacks three times - after one second, five seconds, and then ten seconds.
Deno . cron ( "Retry example" , "* * * * *" , { backoffSchedule : [ 1000 , 5000 , 10000 ] , } , ( ) => { throw new Error ( "Deno.cron will retry this three times, to no avail!" ) ; } ) ;
Design and limitations Jump to heading #
Below are some design details and limitations to be aware of when using Deno.cron .
Tasks must be defined at the top level module scope Jump to heading #
The Deno.cron interface is designed to support static definition of cron tasks based on pre-defined schedules. All Deno.cron tasks must be defined at the top-level of a module. Any nested Deno.cron definitions (e.g. inside Deno.serve handler) will result in an error or will be ignored.
If you need to schedule tasks dynamically during your Deno program execution, you can use the Deno Queues APIs.
Time zone Jump to heading #
Deno.cron schedules are specified using UTC time zone. This helps avoid issues with time zones which observe daylight saving time.
Overlapping executions Jump to heading #
It's possible for the next scheduled invocation of your cron task to overlap with the previous invocation. If this occurs, Deno.cron will skip the next scheduled invocation in order to avoid overlapping executions.
Day-of-week numeric representation Jump to heading #
Deno.cron does not use 0-based day-of-week numeric representation. Instead, it uses 1-7 (or SUN-SAT) to represent Sunday through Saturday. This may be different compared to other cron engines which use 0-6 representation.
Usage on Deno Deploy Jump to heading #
With Deno Deploy , you can run your background tasks on V8 isolates in the cloud. When doing so, there are a few considerations to keep in mind.
Differences with Deno CLI Jump to heading #
Like other Deno runtime built-ins (like queues and Deno KV), the Deno.cron implementation works slightly differently on Deno Deploy.
How cron works by default Jump to heading #
The implementation of Deno.cron in the Deno runtime keeps execution state in-memory. If you run multiple Deno programs that use Deno.cron , each program will have its own independent set of cron tasks.
How cron works on Deno Deploy Jump to heading #
Deno Deploy provides a serverless implementation of Deno.cron that is designed for high availability and scale. Deno Deploy automatically extracts your Deno.cron definitions at deployment time, and schedules them for execution using on-demand isolates. Your latest production deployment defines the set of active cron tasks that are scheduled for execution. To add, remove, or modify cron tasks, simply modify your code and create a new production deployment.
Deno Deploy guarantees that your cron tasks are executed at least once per each scheduled time interval. This generally means that your cron handler will be invoked once per scheduled time. In some failure scenarios, the handler may be invoked multiple times for the same scheduled time.
Cron dashboard Jump to heading #
When you make a production deployment that includes a cron task, you can view a list of all your cron tasks in the Deploy dashboard under the Cron tab for your project.
Pricing Jump to heading #
Deno.cron invocations are charged at the same rate as inbound HTTP requests to your deployments. Learn more about pricing here .
Deploy-specific limitations Jump to heading #
- Deno.cron is only available for production deployments (not preview deployments)
- The exact invocation time of your Deno.cron handler may vary by up to a minute from the scheduled time
Cron configuration examples Jump to heading #
Here are a few common cron configurations, provided for your convenience.
Run once a minute
Deno . cron ( "Run once a minute" , "* * * * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run every fifteen minutes
Deno . cron ( "Run every fifteen minutes" , "*/15 * * * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run once an hour on the hour
Deno . cron ( "Run once an hour on the hour" , "0 * * * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run every three hours
Deno . cron ( "Run every three hours" , "0 */3 * * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run every day at 1am
Deno . cron ( "Run every day at 1am" , "0 1 * * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run every Wednesday at midnight
Deno . cron ( "Run every Wednesday at midnight" , "0 0 * * WED" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;
Run on the first of the month at midnight
Deno . cron ( "Run on the first of the month at midnight" , "0 0 1 * *" , ( ) => { console . log ( "Hello, cron!" ) ; } ) ;


---

URL: https://docs.deno.com/deploy/classic/api/runtime-broadcast-channel/
TITLE: BroadcastChannel

BroadcastChannel
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
In Deno Deploy Classic, code is run in different data centers around the world in order to reduce latency by servicing requests at the data center nearest to the client. In the browser, the BroadcastChannel API allows different tabs with the same origin to exchange messages. In Deno Deploy, the BroadcastChannel API provides a communication mechanism between the various instances; a simple message bus that connects the various Deploy instances worldwide.
Constructor Jump to heading #
The BroadcastChannel() constructor creates a new BroadcastChannel instance and connects to (or creates) the provided channel.
let channel = new BroadcastChannel ( channelName ) ;
Parameters Jump to heading #
name type description
channelName string The name for the underlying broadcast channel connection.
The return type of the constructor is a BroadcastChannel instance.
Properties Jump to heading #
name type description
name string The name of the underlying broadcast channel.
onmessage function (or null ) The function that's executed when the channel receives a new message ( MessageEvent ).
onmessageerror function (or null ) The function that's executed when the arrived message cannot be deserialized to a JavaScript data structure.
Methods Jump to heading #
name description
close() Close the connection to the underlying channel. After closing, you can no longer post messages to the channel.
postMessage(message) Post a message to the underlying channel. The message can be a string, object literal, a number or any kind of Object .
BroadcastChannel extends EventTarget , which allows you to use methods of EventTarget like addEventListener and removeEventListener on an instance of BroadcastChannel .
Example: Update an in-memory cache across instances Jump to heading #
One use case for a message bus like the one enabled by BroadcastChannel is updating an in-memory cache of data between isolates running in different data centers across the network. In the example below, we show how you can configure a simple server that uses BroadcastChannel to synchornize state across all running instances of the server.
import { Hono } from "jsr:@hono/hono" ; // in-memory cache of messages const messages = [ ] ; // A BroadcastChannel used by all isolates const channel = new BroadcastChannel ( "all_messages" ) ; // When a new message comes in from other instances, add it channel . onmessage = ( event : MessageEvent ) => { messages . push ( event . data ) ; } ; // Create a server to add and retrieve messages const app = new Hono ( ) ; // Add a message to the list app . get ( "/send" , ( c ) => { // New messages can be added by including a "message" query param const message = c . req . query ( "message" ) ; if ( message ) { messages . push ( message ) ; channel . postMessage ( message ) ; } return c . redirect ( "/" ) ; } ) ; // Get a list of messages app . get ( "/" , ( c ) => { // Return the current list of messages return c . json ( messages ) ; } ) ; Deno . serve ( app . fetch ) ;
You can test this example yourself on Deno Deploy Classic using this playground .


---

URL: https://docs.deno.com/deploy/classic/api/runtime-request/
TITLE: HTTP Request

HTTP Request
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Request interface is part of the Fetch API and represents the request of fetch().
- Constructor
- Parameters
- Properties
- Methods
- Example
Constructor Jump to heading #
The Request() constructor creates a new Request instance.
let request = new Request ( resource , init ) ;
Parameters Jump to heading #
name type optional description
resource Request or USVString false The resource can either be a request object or a URL string.
init RequestInit true The init object lets you set optional parameters to apply to the request.
The return type is a Request instance.
RequestInit Jump to heading #
name type default description
method string GET The method of the request.
headers Headers or { [key: string]: string } none Th Headers for the request.
body Blob , BufferSource , FormData , URLSearchParams , USVString , or ReadableStream none The body of the request.
cache string none The cache mode of the request.
credentials string same-origin The credentials mode of the request.
integrity string none The crypotographic hash of the request's body.
mode string cors The request mode you want to use.
redirect string follow The mode of how redirects are handled.
referrer string about:client A USVString specifying no-referrer , client or a URL.
Properties Jump to heading #
name type description
cache string The cache mode indicates how the ( default , no-cache , etc) request should be cached by browser.
credentials string The credentials ( omit , same-origin , etc) indicate whether user agent should send cookies in case of CORs of the request.
destination RequestDestination The string indicates the type of content being requested.
body ReadableStream The getter exposes a ReadableStream of the body contents.
bodyUsed boolean Indicates whether the body content is read.
url USVString The URL of the request.
headers Headers The headers associated with the request.
integrity string The crypotographic hash of the request's body.
method string The request's method ( POST , GET , etc).
mode string Indicates the mode of the request (e.g. cors ).
redirect string The mode of how redirects are handled.
referrer string The referrer of the request.
referrerPolicy string The referrer policy of the request
All the above properties are read only.
Methods Jump to heading #
name description
arrayBuffer() Reads the body stream to its completion and returns an ArrayBuffer object.
blob() Reads the body stream to its completion and returns a Blob object.
formData() Reads the body stream to its completion and returns a FormData object.
json() Reads the body stream to its completion, parses it as JSON and returns a JavaScript object.
text() Reads the body stream to its completion and returns a USVString object (text).
clone() Clones the Request object.
Example Jump to heading #
function handler ( _req ) { // Create a post request const request = new Request ( "https://post.deno.dev" , { method : "POST" , body : JSON . stringify ( { message : "Hello world!" , } ) , headers : { "content-type" : "application/json" , } , } ) ; console . log ( request . method ) ; // POST console . log ( request . headers . get ( "content-type" ) ) ; // application/json return fetch ( request ) ; } Deno . serve ( handler ) ;


---

URL: https://docs.deno.com/deploy/classic/deployments/
TITLE: Deployments

Deployments
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
A deployment is a snapshot of the code and environment variables required to run an application. A new deployment can be created via deployctl or automatically via Deploy's Github integration if configured.
Deployments are immutable after they have been created. To deploy a new version of the code for an application, a new deployment must be created. Once created, deployments remain accessible.
All available deployments are listed on your project page under the Deployments tab, pictured below. Old deployments can be deleted via deployctl and via API .
Custom domains Jump to heading #
There can also be other URLs that can point to a deployment, like custom domains .
Branch domains Jump to heading #
<projectname--branchname>.deno.dev is also supported.
Production vs. preview deployments Jump to heading #
All deployments have a preview URL that can be used to view this specific deployment. Preview URLs have the format {project_name}-{deployment_id}.deno.dev .
A deployment can either be a production or a preview deployment. These deployments do not have any differences in runtime functionality. The only distinguishing factor is that a project's production deployment will receive traffic from the project URL (e.g. myproject.deno.dev ), and from custom domains in addition to traffic to the deployment's preview URL.
Promoting preview deployments to production deployments via Deno Deploy Classic UI Jump to heading #
Preview deployments can be "promoted" to production via the Deno Deploy Classic UI:
- Navigate to the project page.
- Click on the Deployments tab.
- Click on the three dots next to the deployment you want to promote to production and select Promote to Production
Promoting deployments to production is restricted to deployments that already use the production KV database. This is particularly relevant for GitHub deployments that use a different database for preview and production deployments. Deployments (even those that use the preview KV database) can always be redeployed to production using the deployctl deployments redeploy command .
Creating production deployments via deployctl Jump to heading #
If you are deploying your Deno code with deployctl , you can deploy directly to production with the --prod flag:
deployctl deploy --prod --project = helloworld main.ts


---

URL: https://docs.deno.com/deploy/classic/edge-cache/
TITLE: Edge Cache

Edge Cache
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
The Web Cache API is supported on Deno Deploy. The cache is designed to provide microsecond-level read latency, multi-GB/s write throughput and unbounded storage, with the tradeoff of best-effort consistency and durability.
const cache = await caches . open ( "my-cache" ) ; Deno . serve ( async ( req ) => { const cached = await cache . match ( req ) ; if ( cached ) { return cached ; } const res = new Response ( "cached at " + new Date ( ) . toISOString ( ) ) ; await cache . put ( req , res . clone ( ) ) ; return res ; } ) ;
Cached data is stored in the same Deno Deploy Classic region that runs your code. Usually your isolate observes read-after-write (RAW) and write-after-write (WAW) consistency within the same region; however, in rare cases recent writes can be lost, out-of-order or temporarily invisible.
Expiration Jump to heading #
By default, cached data is persisted for an indefinite period of time. While we periodically scan and delete inactive objects, an object is usually kept in cache for at least 30 days.
Edge Cache understands standard HTTP response headers Expires and Cache-Control . You can use them to specify an expiration time for every cached object, for example:
Expires: Thu, 22 Aug 2024 01:22:31 GMT
or:
Cache-Control: max-age=86400
Limitations Jump to heading #
- If a response is not constructed from a Uint8Array or string body, the Content-Length header needs to be manually set.
- Deletion is not yet supported.


---

URL: https://docs.deno.com/deploy/classic/kv_on_deploy/
TITLE: KV on Deno Deploy

KV on Deno Deploy
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Deno Deploy Classic offers a built-in serverless key-value database called Deno KV.
Additionally, Deno KV is available within Deno itself, utilizing SQLite as its backend. This feature has been accessible since Deno v1.32 with the --unstable flag. Learn more about Deno KV .
Consistency Jump to heading #
Deno KV, by default, is a strongly-consistent database. It provides the strictest form of strong consistency called external consistency , which implies:
- Serializability : This is the highest level of isolation for transactions. It ensures that the concurrent execution of multiple transactions results in a system state that would be the same as if the transactions were executed sequentially, one after another. In other words, the end result of serializable transactions is equivalent to some sequential order of these transactions.
- Linearizability : This consistency model guarantees that operations, such as read and write, appear to be instantaneous and occur in real-time. Once a write operation completes, all subsequent read operations will immediately return the updated value. Linearizability ensures a strong real-time ordering of operations, making the system more predictable and easier to reason about.
Meanwhile, you can choose to relax consistency constraints by setting the consistency: "eventual" option on individual read operations. This option allows the system to serve the read from global replicas and caches for minimal latency.
Below are the latency figures observed in our top regions:
Region Latency (Eventual Consistency) Latency (Strong Consistency)
North Virginia (us-east4) 7ms 7ms
Frankfurt (europe-west3) 7ms 94ms
Netherlands (europe-west4) 13ms 95ms
California (us-west2) 72ms 72ms
Hong Kong (asia-east2) 42ms 194ms
Connect to managed databases from outside of Deno Deploy Jump to heading #
You can connect to your Deno Deploy KV database from your Deno application outside of Deno Deploy. To open a managed database, set the DENO_KV_ACCESS_TOKEN environment variable to a Deno Deploy personal access token and provide the URL of the database to Deno.openKv :
const kv = await Deno . openKv ( "https://api.deno.com/databases/<database-id>/connect" , ) ;
Please check the docs for the specification of the protocol for connecting to a remote KV database
Data distribution Jump to heading #
Deno KV databases are replicated across at least 3 data centers in the primary region. Once a write operation is committed, its mutations are persistently stored in a quorum of data centers within the primary region. If cross-region replication is enabled, asynchronous replication typically transfers mutations to the destination region in under 5 seconds.
The system is designed to tolerate most data center-level failures without experiencing downtime or data loss. Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO) help quantify the system's resilience under various failure modes. RPO represents the maximum acceptable amount of data loss measured in time, whereas RTO signifies the maximum acceptable time required to restore the system to normal operations after a failure.
- Loss of one data center in the primary region: RPO=0 (no data loss), RTO<5s (system restoration in under 5 seconds)
- Loss of any number of data centers in a replica region: RPO=0, RTO<5s
- Loss of two or more data centers in the primary region: RPO<60s (under 60 seconds of data loss)


---

URL: https://docs.deno.com/deploy/classic/logs/
TITLE: Application logging

Application logging
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Applications can generate logs at runtime using the console API, with methods such as console.log , console.error , etc. These logs can be viewed in real time by either:
- Navigating to the Logs panel of a project or deployment.
- Using the logs subcommand in deployctl .
Logs will be streamed directly from the application to the log panel or displayed in deployctl logs .
In addition to real-time logs, logs are also retained for a certain duration, which depends on the subscription plan you are on. To view persisted logs, you can:
- If you are using the log panel in your browser, switch from Live to either Recent or Custom in the dropdown menu next to the search box.
- If you prefer the command line, add --since=<DATETIME> and/or --until=<DATETIME> to your deployctl logs command. For more details, consult deployctl logs --help .
Logs older than the retention period are automatically deleted from the system.
Limits Jump to heading #
There are limits on both the size of a log message and the volume of logs produced in a certain amount of time.
Log messages have a maximum size of 2KB. Messages larger than this limit are trimmed to 2KB.
A deployment is allowed to produce up to 1000 log entries per second. If it is exceeded, we may terminate the deployment.


---

URL: https://docs.deno.com/deploy/classic/firebase/
TITLE: Connect to Firebase

Connect to Firebase
Legacy Documentation
You are viewing legacy documentation for Deno Deploy Classic. We recommend migrating to the new Deno Deploy platform.
Firebase is a platform developed by Google for creating mobile and web applications. Its features include authentication primitives for log in and a NoSQL datastore, Firestore, that you can persist data to.
This tutorial covers how to connect to Firebase from an application deployed on Deno Deploy.
Get credentials from Firebase Jump to heading #
This tutorial assumes that you've already created a project in Firebase and added a web application to your project.
- 
Navigate to your project in Firebase and click on Project Settings
- 
Scroll down until you see a card with your app name, and a code sample that includes a firebaseConfig object. It should look something like the below. Keep this handy. We will use it later:
var firebaseConfig = { apiKey : "APIKEY" , authDomain : "example-12345.firebaseapp.com" , projectId : "example-12345" , storageBucket : "example-12345.appspot.com" , messagingSenderId : "1234567890" , appId : "APPID" , } ;
Create a Project in Deno Deploy Jump to heading #
- 
Go to https://dash.deno.com/new (Sign in with GitHub if you didn't already) and click on + Empty Project under Deploy from the command line .
- 
Now click on the Settings button available on the project page.
- 
Navigate to the Environment Variables section and add the following:
FIREBASE_USERNAME The Firebase user (email address) that was added above. FIREBASE_PASSWORD The Firebase user password that was added above. FIREBASE_CONFIG The configuration of the Firebase application as a JSON string.
The configuration needs to be a valid JSON string to be readable by the application. If the code snippet given when setting up looked like this:
var firebaseConfig = { apiKey : "APIKEY" , authDomain : "example-12345.firebaseapp.com" , projectId : "example-12345" , storageBucket : "example-12345.appspot.com" , messagingSenderId : "1234567890" , appId : "APPID" , } ;
You would need to set the value of the string to this (noting that spacing and new lines are not required):
{ "apiKey" : "APIKEY" , "authDomain" : "example-12345.firebaseapp.com" , "projectId" : "example-12345" , "storageBucket" : "example-12345.appspot.com" , "messagingSenderId" : "1234567890" , "appId" : "APPID" }
Write code that connects to Firebase Jump to heading #
The first thing we will do is import the XMLHttpRequest polyfill that Firebase needs to work under Deploy as well as a polyfill for localStorage to allow the Firebase auth to persist logged in users:
import "https://deno.land/x/xhr@0.1.1/mod.ts" ; import { installGlobals } from "https://deno.land/x/virtualstorage@0.1.0/mod.ts" ; installGlobals ( ) ;
 we are using the current version of packages at the time of the writing of this tutorial. They may not be up-to-date and you may want to double check current versions.
Because Deploy has a lot of the web standard APIs, it is best to use the web libraries for Firebase under deploy. Currently v9 is in still in beta for Firebase, so we will use v8:
import firebase from "https://esm.sh/firebase@9.17.0/app" ; import "https://esm.sh/firebase@9.17.0/auth" ; import "https://esm.sh/firebase@9.17.0/firestore" ;
Now we need to setup our Firebase application. We will be getting the configuration from the environment variables we set up previously and get references to the parts of Firebase we are going to use:
const firebaseConfig = JSON . parse ( Deno . env . get ( "FIREBASE_CONFIG" ) ) ; const firebaseApp = firebase . initializeApp ( firebaseConfig , "example" ) ; const auth = firebase . auth ( firebaseApp ) ; const db = firebase . firestore ( firebaseApp ) ;
Ok, we are almost done. We just need to create our middleware application and add the localStorage middleware we imported:
const app = new Application ( ) ; app . use ( virtualStorage ( ) ) ;
And then we need to add middleware to authenticate the user. In this tutorial we are simply grabbing the username and password from the environment variables we will be setting up, but this could easily be adapted to redirect a user to a sign-in page if they are not logged in:
app . use ( async ( ctx , next ) => { const signedInUid = ctx . cookies . get ( "LOGGED_IN_UID" ) ; const signedInUser = signedInUid != null ? users . get ( signedInUid ) : undefined ; if ( ! signedInUid || ! signedInUser || ! auth . currentUser ) { const creds = await auth . signInWithEmailAndPassword ( Deno . env . get ( "FIREBASE_USERNAME" ) , Deno . env . get ( "FIREBASE_PASSWORD" ) , ) ; const { user } = creds ; if ( user ) { users . set ( user . uid , user ) ; ctx . cookies . set ( "LOGGED_IN_UID" , user . uid ) ; } else if ( signedInUser && signedInUid . uid !== auth . currentUser ?. uid ) { await auth . updateCurrentUser ( signedInUser ) ; } } return next ( ) ; } ) ;
Deploy the application to Deno Deploy Jump to heading #
Once you have finished writing your application, you can deploy it on Deno Deploy.
To do this, go back to your project page at https://dash.deno.com/projects/<project-name> .
You should see a couple of options to deploy:
- Github integration
- deployctl
deployctl deploy --project = < project-name > < application-file-name >
Unless you want to add a build step, we recommend that you select the Github integration.
For more details on the different ways to deploy on Deno Deploy Classic and the different configuration options, read here .


---

URL: https://docs.deno.com/deploy/kv/manual/
TITLE: Deno KV Quick Start

Deno KV Quick Start
Deno KV is a key-value database built directly into the Deno runtime, available in the Deno.Kv namespace . It can be used for many kinds of data storage use cases, but excels at storing simple data structures that benefit from very fast reads and writes. Deno KV is available in the Deno CLI and on Deno Deploy .
Caution
Deno KV is still in development and may change. To use it, you must pass the --unstable-kv flag to Deno.
Let's walk through the key features of Deno KV.
Opening a database Jump to heading #
In your Deno program, you can get a reference to a KV database using Deno.openKv() . You may pass in an optional file system path to where you'd like to store your database, otherwise one will be created for you based on the current working directory of your script.
const kv = await Deno . openKv ( ) ;
Creating, updating, and reading a key-value pair Jump to heading #
Data in Deno KV is stored as key-value pairs, much like properties of a JavaScript object literal or a Map . Keys are represented as an array of JavaScript types, like string , number , bigint , or boolean . Values can be arbitrary JavaScript objects. In this example, we create a key-value pair representing a user's UI preferences, and save it with kv.set() .
const kv = await Deno . openKv ( ) ; const prefs = { username : "ada" , theme : "dark" , language : "en-US" , } ; const result = await kv . set ( [ "preferences" , "ada" ] , prefs ) ;
Once a key-value pair is set, you can read it from the database with kv.get() :
const entry = await kv . get ( [ "preferences" , "ada" ] ) ; console . log ( entry . key ) ; console . log ( entry . value ) ; console . log ( entry . versionstamp ) ;
Both get and list operations return a KvEntry object with the following properties:
- key - the array key you used to set the value
- value - the JavaScript object you set for this key
- versionstamp - a generated value used to determine if a key has been updated.
The set operation is also used to update objects that already exist for a given key. When a key's value is updated, its versionstamp will change to a new generated value.
Listing several key-value pairs Jump to heading #
To get values for a finite number of keys, you may use kv.getMany() . Pass in several keys as arguments, and you'll receive an array of values for each key. Note that values and versionstamps can be null if no value exists for the given key(s).
const kv = await Deno . openKv ( ) ; const result = await kv . getMany ( [ [ "preferences" , "ada" ] , [ "preferences" , "grace" ] , ] ) ; result [ 0 ] . key ; // ["preferences", "ada"] result [ 0 ] . value ; // { ... } result [ 0 ] . versionstamp ; // "00000000000000010000" result [ 1 ] . key ; // ["preferences", "grace"] result [ 1 ] . value ; // null result [ 1 ] . versionstamp ; // null
Often, it is useful to retrieve a list of key-value pairs from all keys that share a given prefix. This type of operation is possible using kv.list() . In this example, we get a list of key-value pairs that share the "preferences" prefix.
const kv = await Deno . openKv ( ) ; const entries = kv . list ( { prefix : [ "preferences" ] } ) ; for await ( const entry of entries ) { console . log ( entry . key ) ; // ["preferences", "ada"] console . log ( entry . value ) ; // { ... } console . log ( entry . versionstamp ) ; // "00000000000000010000" }
Returned keys are ordered lexicographically based on the next component of the key after the prefix. So KV pairs with these keys:
- ["preferences", "ada"]
- ["preferences", "bob"]
- ["preferences", "cassie"]
Will be returned in that order by kv.list() .
Read operations can either be performed in strong or eventual consistency mode . Strong consistency mode guarantees that the read operation will return the most recently written value. Eventual consistency mode may return a stale value, but is faster. By contrast, writes are always performed in strong consistency mode.
Deleting key-value pairs Jump to heading #
You can delete a key from the database using kv.delete() . No action is taken if no value is found for the given key.
const kv = await Deno . openKv ( ) ; await kv . delete ( [ "preferences" , "alan" ] ) ;
Atomic transactions Jump to heading #
Deno KV is capable of executing atomic transactions , which enables you to conditionally execute one or many data manipulation operations at once. In the following example, we create a new preferences object only if it hasn't been created already.
const kv = await Deno . openKv ( ) ; const key = [ "preferences" , "alan" ] ; const value = { username : "alan" , theme : "light" , language : "en-GB" , } ; const res = await kv . atomic ( ) . check ( { key , versionstamp : null } ) // `null` versionstamps mean 'no value' . set ( key , value ) . commit ( ) ; if ( res . ok ) { console . log ( "Preferences did not yet exist. Inserted!" ) ; } else { console . error ( "Preferences already exist." ) ; }
Learn more about transactions in Deno KV here .
Improve querying with secondary indexes Jump to heading #
Secondary indexes store the same data by multiple keys, allowing for simpler queries of the data you need. Let's say that we need to be able to access user preferences by both username AND email. To enable this, you could provide a function that wraps the logic to save the preferences to create two indexes.
const kv = await Deno . openKv ( ) ; async function savePreferences ( prefs ) { const key = [ "preferences" , prefs . username ] ; // Set the primary key const r = await kv . set ( key , prefs ) ; // Set the secondary key's value to be the primary key await kv . set ( [ "preferencesByEmail" , prefs . email ] , key ) ; return r ; } async function getByUsername ( username ) { // Use as before... const r = await kv . get ( [ "preferences" , username ] ) ; return r ; } async function getByEmail ( email ) { // Look up the key by email, then second lookup for actual data const r1 = await kv . get ( [ "preferencesByEmail" , email ] ) ; const r2 = await kv . get ( r1 . value ) ; return r2 ; }
Learn more about secondary indexes in the manual here .
Watching for updates in Deno KV Jump to heading #
You can also listen for updates from Deno KV with kv.watch() , which will emit a new value or values of the key or keys you provide. In the below chat example, we watch for updates on the key ["last_message_id", roomId] . We retrieve messageId , which we then use with kv.list() to grab all the new messages from seen and messageId .
let seen = "" ; for await ( const [ messageId ] of kv . watch ( [ [ "last_message_id" , roomId ] ] ) ) { const newMessages = await Array . fromAsync ( kv . list ( { start : [ "messages" , roomId , seen , "" ] , end : [ "messages" , roomId , messageId , "" ] , } ) ) ; await websocket . write ( JSON . stringify ( newMessages ) ) ; seen = messageId ; }
Learn more about using Deno KV watch here .
Production usage Jump to heading #
Deno KV is available for use in live applications on Deno Deploy . In production, Deno KV is backed by FoundationDB , the open source key-value store created by Apple.
Testing Jump to heading #
By default, Deno.openKv() creates or opens a persistent store based on the path from which the script that invoked it was run. This isn't usually desirable for tests, which need to produce the same behavior when run many times in a row.
To test code that uses Deno KV, you can use the special argument ":memory:" to create an ephemeral Deno KV datastore.
async function setDisplayName ( kv : Deno . Kv , username : string , displayname : string , ) { await kv . set ( [ "preferences" , username , "displayname" ] , displayname ) ; } async function getDisplayName ( kv : Deno . Kv , username : string , ) : Promise < string | null > { return ( await kv . get ( [ "preferences" , username , "displayname" ] ) ) . value as string ; } Deno . test ( "Preferences" , async ( t ) => { const kv = await Deno . openKv ( ":memory:" ) ; await t . step ( "can set displayname" , async ( ) => { const displayName = await getDisplayName ( kv , "example" ) ; assertEquals ( displayName , null ) ; await setDisplayName ( kv , "example" , "Exemplary User" ) ; const displayName = await getDisplayName ( kv , "example" ) ; assertEquals ( displayName , "Exemplary User" ) ; } ) ; } ) ;
This works because Deno KV is backed by SQLite when run for local development. Just like in-memory SQLite databases, multiple ephemeral Deno KV stores can exist at once without interfering with one another. For more information about special database addressing modes, see the SQLite docs on the topic .
Next steps Jump to heading #
At this point, you're just beginning to scratch the surface with Deno KV. Be sure to check out our guide on the Deno KV key space , and a collection of tutorials and example applications here.
